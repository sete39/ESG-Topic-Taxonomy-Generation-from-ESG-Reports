{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from ogb.nodeproppred import Evaluator, NodePropPredDataset\n",
    "from tensorflow.keras.layers import BatchNormalization, Dropout, Input, Dense\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from spektral.datasets.ogb import OGB\n",
    "from spektral.layers import GCNConv\n",
    "from spektral.transforms import AdjToSpTensor, GCNFilter\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(action='ignore', category=DeprecationWarning, message='`np.bool` is a deprecated alias')\n",
    "\n",
    "# Load data\n",
    "dataset_name = \"ogbn-arxiv\"\n",
    "ogb_dataset = NodePropPredDataset(dataset_name)\n",
    "dataset = OGB(ogb_dataset, transforms=[GCNFilter(), AdjToSpTensor()])\n",
    "graph = dataset[0]\n",
    "x, adj, y = graph.x, graph.a, graph.y\n",
    "\n",
    "# Parameters\n",
    "channels = 256  # Number of channels for GCN layers\n",
    "dropout = 0.35  # Dropout rate for the features\n",
    "learning_rate = 0.01  # Learning rate\n",
    "epochs = 1000  # Number of training epochs\n",
    "\n",
    "N = dataset.n_nodes  # Number of nodes in the graph\n",
    "F = dataset.n_node_features  # Original size of node features\n",
    "n_out = ogb_dataset.num_classes  # OGB labels are sparse indices\n",
    "\n",
    "# Data splits\n",
    "idx = ogb_dataset.get_idx_split()\n",
    "idx_tr, idx_va, idx_te = idx[\"train\"], idx[\"valid\"], idx[\"test\"]\n",
    "mask_tr = np.zeros(N, dtype=bool)\n",
    "mask_va = np.zeros(N, dtype=bool)\n",
    "mask_te = np.zeros(N, dtype=bool)\n",
    "mask_tr[idx_tr] = True\n",
    "mask_va[idx_va] = True\n",
    "mask_te[idx_te] = True\n",
    "masks = [mask_tr, mask_va, mask_te]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_9 (InputLayer)           [(None, 128)]        0           []                               \n",
      "                                                                                                  \n",
      " input_10 (InputLayer)          [(None, 169343)]     0           []                               \n",
      "                                                                                                  \n",
      " gcn_conv_12 (GCNConv)          (None, 256)          33024       ['input_9[0][0]',                \n",
      "                                                                  'input_10[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 256)         1024        ['gcn_conv_12[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_8 (Dropout)            (None, 256)          0           ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " gcn_conv_13 (GCNConv)          (None, 256)          65792       ['dropout_8[0][0]',              \n",
      "                                                                  'input_10[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 256)         1024        ['gcn_conv_13[0][0]']            \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 256)          0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " gcn_conv_14 (GCNConv)          (None, 40)           10280       ['dropout_9[0][0]',              \n",
      "                                                                  'input_10[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 111,144\n",
      "Trainable params: 110,120\n",
      "Non-trainable params: 1,024\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model definition\n",
    "x_in = Input(shape=(F,))\n",
    "a_in = Input((N,), sparse=True)\n",
    "x_1 = GCNConv(channels, activation=\"relu\")([x_in, a_in])\n",
    "x_1 = BatchNormalization()(x_1)\n",
    "x_1 = Dropout(dropout)(x_1)\n",
    "x_2 = GCNConv(channels, activation=\"relu\")([x_1, a_in])\n",
    "x_2 = BatchNormalization()(x_2)\n",
    "x_2 = Dropout(dropout)(x_2)\n",
    "x_3 = GCNConv(n_out, activation=\"softmax\")([x_2, a_in])\n",
    "# output = Dense(10)(x_3)\n",
    "# Build model\n",
    "model = Model(inputs=[x_in, a_in], outputs=x_3)\n",
    "optimizer = Adam(learning_rate=learning_rate)\n",
    "loss_fn = SparseCategoricalCrossentropy()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "@tf.function\n",
    "def train(inputs, target, mask):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model(inputs, training=True)\n",
    "        loss = loss_fn(target[mask], predictions[mask]) + sum(model.losses)\n",
    "\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "\n",
    "# Evaluation with OGB\n",
    "evaluator = Evaluator(dataset_name)\n",
    "\n",
    "def evaluate(x, a, y, model, masks, evaluator):\n",
    "    p = model([x, a], training=False)\n",
    "    p = p.numpy().argmax(-1)[:, None]\n",
    "    tr_mask, va_mask, te_mask = masks\n",
    "    tr_auc = evaluator.eval({\"y_true\": y[tr_mask], \"y_pred\": p[tr_mask]})[\"acc\"]\n",
    "    va_auc = evaluator.eval({\"y_true\": y[va_mask], \"y_pred\": p[va_mask]})[\"acc\"]\n",
    "    te_auc = evaluator.eval({\"y_true\": y[te_mask], \"y_pred\": p[te_mask]})[\"acc\"]\n",
    "    return tr_auc, va_auc, te_auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep. 1 - Loss: 1.097 - Acc: 0.485 - Val acc: 0.497 - Test acc: 0.527\n",
      "Ep. 2 - Loss: 1.092 - Acc: 0.496 - Val acc: 0.501 - Test acc: 0.531\n",
      "Ep. 3 - Loss: 1.098 - Acc: 0.493 - Val acc: 0.491 - Test acc: 0.524\n",
      "Ep. 4 - Loss: 1.093 - Acc: 0.495 - Val acc: 0.488 - Test acc: 0.522\n",
      "Ep. 5 - Loss: 1.092 - Acc: 0.494 - Val acc: 0.492 - Test acc: 0.523\n",
      "Ep. 6 - Loss: 1.093 - Acc: 0.498 - Val acc: 0.496 - Test acc: 0.525\n",
      "Ep. 7 - Loss: 1.093 - Acc: 0.502 - Val acc: 0.487 - Test acc: 0.520\n",
      "Ep. 8 - Loss: 1.088 - Acc: 0.516 - Val acc: 0.512 - Test acc: 0.541\n",
      "Ep. 9 - Loss: 1.092 - Acc: 0.517 - Val acc: 0.516 - Test acc: 0.545\n",
      "Ep. 10 - Loss: 1.089 - Acc: 0.517 - Val acc: 0.516 - Test acc: 0.546\n",
      "Ep. 11 - Loss: 1.092 - Acc: 0.509 - Val acc: 0.500 - Test acc: 0.534\n",
      "Ep. 12 - Loss: 1.083 - Acc: 0.509 - Val acc: 0.491 - Test acc: 0.525\n",
      "Ep. 13 - Loss: 1.094 - Acc: 0.534 - Val acc: 0.523 - Test acc: 0.549\n",
      "Ep. 14 - Loss: 1.085 - Acc: 0.534 - Val acc: 0.520 - Test acc: 0.544\n",
      "Ep. 15 - Loss: 1.090 - Acc: 0.530 - Val acc: 0.520 - Test acc: 0.549\n",
      "Ep. 16 - Loss: 1.085 - Acc: 0.528 - Val acc: 0.514 - Test acc: 0.545\n",
      "Ep. 17 - Loss: 1.085 - Acc: 0.526 - Val acc: 0.501 - Test acc: 0.533\n",
      "Ep. 18 - Loss: 1.086 - Acc: 0.533 - Val acc: 0.518 - Test acc: 0.546\n",
      "Ep. 19 - Loss: 1.086 - Acc: 0.533 - Val acc: 0.521 - Test acc: 0.548\n",
      "Ep. 20 - Loss: 1.084 - Acc: 0.539 - Val acc: 0.526 - Test acc: 0.552\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m \u001b[39m+\u001b[39m epochs):\n\u001b[1;32m----> 3\u001b[0m     tr_loss \u001b[39m=\u001b[39m train([x, adj], y, mask_tr)\n\u001b[0;32m      4\u001b[0m     tr_acc, va_acc, te_acc \u001b[39m=\u001b[39m evaluate(x, adj, y, model, masks, evaluator)\n\u001b[0;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m      6\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEp. \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m - Loss: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m - Acc: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m - Val acc: \u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m - Test acc: \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m{:.3f}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i, tr_loss, tr_acc, va_acc, te_acc)\n\u001b[0;32m      8\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\saif8\\miniconda3\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\saif8\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:915\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    912\u001b[0m compiler \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxla\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mnonXla\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    914\u001b[0m \u001b[39mwith\u001b[39;00m OptionalXlaContext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 915\u001b[0m   result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    917\u001b[0m new_tracing_count \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    918\u001b[0m without_tracing \u001b[39m=\u001b[39m (tracing_count \u001b[39m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\saif8\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py:954\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock\u001b[39m.\u001b[39mrelease()\n\u001b[0;32m    952\u001b[0m \u001b[39m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[0;32m    953\u001b[0m \u001b[39m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[1;32m--> 954\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_stateful_fn(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    955\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_created_variables \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m ALLOW_DYNAMIC_VARIABLE_CREATION:\n\u001b[0;32m    956\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCreating variables on a non-first call to a function\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    957\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39m decorated with tf.function.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\saif8\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:2496\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2493\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m   2494\u001b[0m   (graph_function,\n\u001b[0;32m   2495\u001b[0m    filtered_flat_args) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[1;32m-> 2496\u001b[0m \u001b[39mreturn\u001b[39;00m graph_function\u001b[39m.\u001b[39;49m_call_flat(\n\u001b[0;32m   2497\u001b[0m     filtered_flat_args, captured_inputs\u001b[39m=\u001b[39;49mgraph_function\u001b[39m.\u001b[39;49mcaptured_inputs)\n",
      "File \u001b[1;32mc:\\Users\\saif8\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:1862\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1858\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1859\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1860\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1861\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1862\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[0;32m   1863\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[0;32m   1864\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1865\u001b[0m     args,\n\u001b[0;32m   1866\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1867\u001b[0m     executing_eagerly)\n\u001b[0;32m   1868\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\saif8\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[0;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[0;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[0;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[0;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[0;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[0;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[0;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[0;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[1;32mc:\\Users\\saif8\\miniconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[0;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[0;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "for i in range(1, 1 + epochs):\n",
    "    tr_loss = train([x, adj], y, mask_tr)\n",
    "    tr_acc, va_acc, te_acc = evaluate(x, adj, y, model, masks, evaluator)\n",
    "    print(\n",
    "        \"Ep. {} - Loss: {:.3f} - Acc: {:.3f} - Val acc: {:.3f} - Test acc: \"\n",
    "        \"{:.3f}\".format(i, tr_loss, tr_acc, va_acc, te_acc)\n",
    "    )\n",
    "# model.compile(optimizer=optimizer, loss='categorical_crossentropy')\n",
    "\n",
    "# from spektral.data import BatchLoader\n",
    "# loader = BatchLoader(dataset, batch_size=32)\n",
    "\n",
    "# model.fit(loader.load(), steps_per_epoch=loader.steps_per_epoch, epochs=1000)\n",
    "\n",
    "# Evaluate model\n",
    "print(\"Evaluating model.\")\n",
    "tr_acc, va_acc, te_acc = evaluate(x, adj, y, model, masks, evaluator)\n",
    "print(\"Done! - Test acc: {:.3f}\".format(te_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(169343, 40), dtype=float32, numpy=\n",
       "array([[0.00462996, 0.00427004, 0.01627544, ..., 0.02665814, 0.00355087,\n",
       "        0.00650889],\n",
       "       [0.00410189, 0.00601544, 0.02579289, ..., 0.0175071 , 0.00256565,\n",
       "        0.01400298],\n",
       "       [0.00686559, 0.0024032 , 0.00238143, ..., 0.00804066, 0.00104982,\n",
       "        0.00649454],\n",
       "       ...,\n",
       "       [0.00218012, 0.00310576, 0.01254589, ..., 0.02632165, 0.00287421,\n",
       "        0.00130697],\n",
       "       [0.00298416, 0.00756151, 0.00810144, ..., 0.01523414, 0.00422839,\n",
       "        0.00280293],\n",
       "       [0.00205035, 0.02974777, 0.01468056, ..., 0.02579349, 0.00984326,\n",
       "        0.00285765]], dtype=float32)>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = model([x, adj], training=False)\n",
    "# p = p.numpy().argmax(-1)[:, None]\n",
    "# tr_mask, va_mask, te_mask = masks\n",
    "# te_auc = evaluator.eval({\"y_true\": y[te_mask], \"y_pred\": p[te_mask]})[\"acc\"]\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.057943, -0.05253 , -0.072603, -0.026555,  0.130435, -0.241386,\n",
       "       -0.449242, -0.018443, -0.087218,  0.11232 , -0.092125, -0.28956 ,\n",
       "       -0.081012,  0.074489, -0.156198, -0.097413,  0.11937 ,  0.645755,\n",
       "        0.077375, -0.09386 , -0.400367,  0.311369, -0.541764,  0.080455,\n",
       "       -0.00695 ,  0.542316, -0.01223 , -0.180773,  0.016466,  0.050778,\n",
       "       -0.208276, -0.08701 ,  0.012363,  0.281671,  0.100448, -0.164255,\n",
       "        0.026892,  0.078199,  0.079534, -0.013387,  0.291491,  0.041601,\n",
       "       -0.141369, -0.134461,  0.016178,  0.280961, -0.091925, -0.240312,\n",
       "        0.461786,  0.187323,  0.15335 ,  0.033118,  0.01076 ,  0.012446,\n",
       "       -0.158857,  0.09798 ,  0.03052 ,  0.016234, -0.095681,  0.05214 ,\n",
       "        0.321836, -0.105675,  0.222873, -0.120619, -0.172259,  0.395426,\n",
       "        0.088274, -0.221882,  0.231014, -0.209604, -0.112524, -0.064443,\n",
       "        0.069746, -0.157444,  0.02228 , -0.418984,  0.134391,  0.26046 ,\n",
       "        0.041681, -0.093468, -0.051622, -0.025531,  0.774428,  0.058099,\n",
       "        0.045217,  0.057072, -0.548202, -0.046379,  0.87283 ,  0.011926,\n",
       "        0.38909 , -0.085909,  0.111557,  0.061788,  0.001474,  0.047552,\n",
       "        0.036312,  0.258624,  0.235854, -0.029015, -0.141505,  0.710568,\n",
       "       -0.057127, -0.117426,  0.305883,  0.167014, -0.199021,  0.127596,\n",
       "        0.027017,  0.545832, -0.19169 , -0.069634, -0.11107 ,  0.114232,\n",
       "        0.116245, -0.015938,  0.115862, -0.062358,  0.21149 , -0.226118,\n",
       "       -0.185603,  0.05323 ,  0.332873,  0.104175,  0.007408,  0.173364,\n",
       "       -0.172796, -0.140059], dtype=float32)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph.x[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e907c9c662b38b6fe9e69d6881d7a36bc6e4ecfbfed88be2a66ef06ac6f6bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
