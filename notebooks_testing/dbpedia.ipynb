{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading df\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../dataset/DBPEDIA_train.csv')\n",
    "document_list = df['text'].to_numpy().astype('str')\n",
    "document_list.dtype\n",
    "print('Finished reading df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding topics to create the adjacency matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "topics = np.concatenate((df['l1'].unique(), df['l2'].unique(), df['l3'].unique()))\n",
    "labelEncoder.fit(topics)\n",
    "\n",
    "def encode_topic(topic):\n",
    "    print(type(topic))\n",
    "    return labelEncoder.transform(topic)\n",
    "\n",
    "df['l1_encoded'] = labelEncoder.transform(df['l1'])\n",
    "df['l2_encoded'] = labelEncoder.transform(df['l2'])\n",
    "df['l3_encoded'] = labelEncoder.transform(df['l3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 17:01:18.810107: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-01 17:01:18.994500: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-01 17:01:19.803632: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cudnn-8.2.4.15-11.4-aa4jklovfx75n2q4jcbyc3dgemu5nqpu/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cuda-11.4.2-sg5bamqhm7bmiz3nf43yzo5thibytq7t/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libxml2-2.9.12-i7nsnmeeeukas6sc2rqa6adykxtky4yj/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/zlib-1.2.11-un5j7szis72ayqdk4yzjlp6vkjrcbtn7/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/xz-5.2.5-wfweai2jkkplgry3aqkfb5fqg44fuxtf/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libiconv-1.16-xa6oobihv7qsbf743iq5g3ioyriyjxwj/lib\n",
      "2023-04-01 17:01:19.803722: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cudnn-8.2.4.15-11.4-aa4jklovfx75n2q4jcbyc3dgemu5nqpu/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cuda-11.4.2-sg5bamqhm7bmiz3nf43yzo5thibytq7t/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libxml2-2.9.12-i7nsnmeeeukas6sc2rqa6adykxtky4yj/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/zlib-1.2.11-un5j7szis72ayqdk4yzjlp6vkjrcbtn7/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/xz-5.2.5-wfweai2jkkplgry3aqkfb5fqg44fuxtf/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libiconv-1.16-xa6oobihv7qsbf743iq5g3ioyriyjxwj/lib\n",
      "2023-04-01 17:01:19.803728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_topic_to_tokenized_dict = {}\n",
    "for topic in topics:\n",
    "    # dbpedia categories are in PascalCase, so this makes them spaced\n",
    "    spaced_words = re.sub( r\"([A-Z])\", r\" \\1\", topic)[1:]\n",
    "    tokenized_sequence = tokenizer.encode_plus(spaced_words, add_special_tokens=True, max_length=max_len, padding='max_length')['input_ids']\n",
    "\n",
    "    encoded_topic_to_tokenized_dict[labelEncoder.transform([topic])[0]] = tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = df['text'].apply(lambda doc: np.array(tokenizer.encode_plus(doc, add_special_tokens=True, max_length=max_len, padding='max_length', truncation=True)['input_ids'])).to_numpy()\n",
    "# documents_labels = labelEncoder.transform(df['l3'].to_numpy())\n",
    "# documents_fixed = np.empty(shape=(len(documents), max_len))\n",
    "# for i, doc in enumerate(documents):\n",
    "#     documents_fixed[i] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"tokenized_dbpedia.pkl\", \"wb\") as f:\n",
    "#     pickle.dump([documents_fixed, documents_labels], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished getting tokenized file\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with (open(\"./tokenized_dbpedia.pkl\", \"rb\")) as f:\n",
    "    documents, documents_labels = pickle.load(f)\n",
    "print('Finished getting tokenized file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    l1 = row['l1_encoded']\n",
    "    l2 = row['l2_encoded']\n",
    "    l3 = row['l3_encoded']\n",
    "\n",
    "    if l1 not in graph_dict:\n",
    "        graph_dict[l1] = {}\n",
    "    if l2 not in graph_dict[l1]:\n",
    "        graph_dict[l1][l2] = {} \n",
    "\n",
    "    graph_dict[l1][l2][l3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating adjacency matrix\n",
    "adj_matrix = np.zeros((len(topics), len(topics)))\n",
    "square_numeric_dict = {'source': [], 'target': []}\n",
    "for i, df_row in df.iterrows():\n",
    "    l1 = df_row['l1_encoded']\n",
    "    l2 = df_row['l2_encoded']\n",
    "    l3 = df_row['l3_encoded']\n",
    "\n",
    "    adj_matrix[l1, l2] = 1\n",
    "    adj_matrix[l2, l3] = 1\n",
    "\n",
    "for i, row in enumerate(adj_matrix):\n",
    "    for j, value in enumerate(row):\n",
    "        if value == 0:\n",
    "            continue\n",
    "        square_numeric_dict['source'].append(i)\n",
    "        square_numeric_dict['target'].append(j)\n",
    "\n",
    "square_numeric_edges = pd.DataFrame(square_numeric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating node features\n",
    "x = np.arange(298)\n",
    "x = labelEncoder.inverse_transform(x)\n",
    "feature_array = x.reshape(298, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished getting GlOVE embedder\n"
     ]
    }
   ],
   "source": [
    "# loading GloVe model to get topic word embeddings\n",
    "# from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "import torchtext\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
    "print('Finished getting GlOVE embedder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['MASK'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating \"ego-graphs\" (each node is seperated into a graph with itself, parent, and siblings)\n",
    "# The base node (so the node itself) will be masked, aka. have a [MASK] embedding\n",
    "# The sibling nodes need to have a negative relationship with the base node (so negative value in adjacency matrix?)\n",
    "from spektral.data import Graph\n",
    "import numpy as np\n",
    "import re\n",
    "def create_ego_graph(l1_topic, l2_topic, l3_topic, graph_dict):\n",
    "    if l3_topic != None:\n",
    "        siblings_list = list(graph_dict[l1_topic][l2_topic].keys())\n",
    "        siblings_list.remove(l3_topic)\n",
    "        base = l3_topic\n",
    "        parent = l2_topic\n",
    "        grandparent = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        all_nodes_list.append(parent)\n",
    "        all_nodes_list.append(grandparent)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "        encoded_parent = node_label_encoder.transform([parent])[0]\n",
    "        encoded_grandparent = node_label_encoder.transform([grandparent])[0]\n",
    "\n",
    "        adj_matrix[encoded_base][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_base] = 1\n",
    "\n",
    "        adj_matrix[encoded_grandparent][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_grandparent] = 1\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "        \n",
    "    elif l2_topic != None:\n",
    "        siblings_list = list(graph_dict[l1_topic].keys())\n",
    "        siblings_list.remove(l2_topic)\n",
    "        base = l2_topic\n",
    "        parent = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        all_nodes_list.append(parent)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "        encoded_parent = node_label_encoder.transform([parent])[0]\n",
    "\n",
    "        adj_matrix[encoded_base][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_base] = 1\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "    \n",
    "    elif l1_topic != None:\n",
    "        siblings_list = list(graph_dict.keys())\n",
    "        siblings_list.remove(l1_topic)\n",
    "        base = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "\n",
    "    ego_features = np.zeros((n_nodes, 50))\n",
    "    encoded_nodes_list = node_label_encoder.transform(all_nodes_list)\n",
    "\n",
    "    for i, node in enumerate(all_nodes_list):\n",
    "        feature = feature_array[node]\n",
    "        split_words_list = re.sub( r\"([A-Z])\", r\" \\1\", feature[0]).split()\n",
    "        n_words = len(split_words_list)\n",
    "        embedding_avg = np.array([glove[word.lower()].numpy() for word in split_words_list]).sum(axis=0)/n_words\n",
    "        \n",
    "        # Masking base node, setting the embedding to all 0's\n",
    "        if (node == base):\n",
    "            embedding_avg = glove['MASK']\n",
    "\n",
    "        ego_features[encoded_nodes_list[i]] = embedding_avg\n",
    "\n",
    "    return Graph(a=adj_matrix, x=ego_features, y=(l1_topic, l2_topic, l3_topic))\n",
    "\n",
    "graph_list = []\n",
    "\n",
    "for l1_topic in graph_dict:\n",
    "    for l2_topic in graph_dict[l1_topic]:\n",
    "        for l3_topic in graph_dict[l1_topic][l2_topic]:\n",
    "            graph_list.append(create_ego_graph(l1_topic, l2_topic, l3_topic, graph_dict))\n",
    "        graph_list.append(create_ego_graph(l1_topic, l2_topic, None, graph_dict))\n",
    "    graph_list.append(create_ego_graph(l1_topic, None, None, graph_dict))\n",
    "\n",
    "graph_list = np.array(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, graph_list: list[Graph], **kwargs):\n",
    "        self.graph_list = graph_list\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        num_l =  np.random.permutation(len(self.graph_list))\n",
    "        return [self.graph_list[i] for i in num_l]\n",
    "    \n",
    "dataset = MyDataset(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.Bilinear import Bilinear\n",
    "from layers.ContextEmbedding import ContextEmbedding\n",
    "from layers.TopicAttentiveEmbedding import TopicAttentiveEmbedding\n",
    "\n",
    "from utils.TopicExpanTrainGen import TopicExpanTrainGen\n",
    "\n",
    "def sequence_to_document_embedding(sequence_embedding: tf.Tensor):\n",
    "    # gets the document representation/embedding from a BERT sequence embedding\n",
    "    # by getting the mean-pooling of the sequence \n",
    "    return tf.math.reduce_mean(sequence_embedding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 17:01:39.490075: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-01 17:01:40.154354: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38220 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:b1:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import int64\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from spektral.layers import GCNConv, GlobalAvgPool\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "import keras_nlp\n",
    "\n",
    "learning_rate = 5e-5  # Learning rate\n",
    "epochs = 4  # Number of training epochs\n",
    "batch_size = 32  # Batch size\n",
    "weight_decay = 5e-6\n",
    "mini_batch_size = 4 # a mini-batch will always have 1 positive triple, and (n-1) negative triples\n",
    "                    # i.e. with a mini_batch_size of 4, we have 1 pos. doc. and 3 neg. docs.\n",
    "batch_ratio = int(batch_size / mini_batch_size)\n",
    "\n",
    "max_len = 512\n",
    "vocab_size = tokenizer.vocab_size\n",
    "infoNCE_temprature = 0.1\n",
    "\n",
    "optimizer = Adam(learning_rate, weight_decay=weight_decay)\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn_binary = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "loss_fn_crossentropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "n_out = dataset.n_labels\n",
    "topic_embedding_dimension = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Custom training loop\n",
    "class ModelWithNCE(Model):\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(self, data):\n",
    "        inputs, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            similarity_prediction, phrase_prediction = self(inputs, training=True)\n",
    "            similarity_prediction_infonce = tf.reshape(similarity_prediction / infoNCE_temprature, shape=(mini_batch_size, -1))\n",
    "\n",
    "            # infoNCE_loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=tf.reshape(target[0], shape=(mini_batch_size, -1)), logits=similarity_prediction_infonce))\n",
    "            # infoNCE_loss = loss_fn_crossentropy(tf.reshape(target[0], shape=(mini_batch_size, -1)), similarity_prediction_infonce)\n",
    "            bce_loss = loss_fn_binary(target[0], similarity_prediction)\n",
    "            phrase_loss = loss_fn(target[1], phrase_prediction)\n",
    "\n",
    "            # tf.print(similarity_prediction[:4], bce_loss, phrase_loss, output_stream=sys.stderr)\n",
    "        gradients = tape.gradient([bce_loss, phrase_loss], self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(target, (similarity_prediction, phrase_prediction))\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_expan_generator = TopicExpanTrainGen(graph_list, documents, documents_labels, batch_size, mini_batch_size, encoded_topic_to_tokenized_dict)\n",
    "#, arr = topic_expan_generator.__getitem__(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# arr[1][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 17:01:42.592622: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['mlm___cls', 'nsp___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/k21148846/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "shared_bilinear = Bilinear(topic_embedding_dimension, 768, 1)\n",
    "\n",
    "# GNNs (Topic Encoder)\n",
    "X_in = Input(shape=(dataset.n_node_features))\n",
    "A_in = Input(shape=(None,), sparse=True)\n",
    "I_in = Input(shape=(), dtype=int64)\n",
    "\n",
    "topic_embedding = GCNConv(topic_embedding_dimension, activation='relu')([X_in, A_in])\n",
    "topic_embedding = GCNConv(topic_embedding_dimension, activation='relu')([topic_embedding, A_in])\n",
    "topic_embedding = GlobalAvgPool(name='topic_embedding')([topic_embedding, I_in])\n",
    "\n",
    "# BERT Embedding (Document Encoder)\n",
    "max_seq_length = max_len\n",
    "encoder = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "embedding = encoder(input_ids)[0]\n",
    "\n",
    "# Transformer Decoders (Phrase Generator)\n",
    "decoder_tokens_input = Input(shape=(max_len,), name=\"decoder_phrase_input\")\n",
    "decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(vocabulary_size=vocab_size, sequence_length=max_len, embedding_dim=768, mask_zero=True)(decoder_tokens_input)\n",
    "\n",
    "# Getting context embedding for decoder\n",
    "topic_attentive_embedding = TopicAttentiveEmbedding()(topic_embedding, embedding, shared_bilinear, training=False)\n",
    "topic_attentive_embedding = tf.keras.layers.Reshape((max_len, 1))(topic_attentive_embedding)\n",
    "context_embedding = ContextEmbedding()([topic_attentive_embedding, embedding])\n",
    "\n",
    "transformer_decoder = keras_nlp.layers.TransformerDecoder(\n",
    "    num_heads=16, \n",
    "    intermediate_dim=max_len,\n",
    "    dropout=0.1\n",
    ")(decoder_embedding, context_embedding)\n",
    "\n",
    "# Transformer Output\n",
    "out2 = Dense(vocab_size)(transformer_decoder)\n",
    "\n",
    "# Output Bilinear layer (Similarity Predictor)\n",
    "document_embedding = Lambda(sequence_to_document_embedding, name='document_embedding')(embedding)\n",
    "out = shared_bilinear([topic_embedding, document_embedding])\n",
    "\n",
    "# Outputs\n",
    "model = ModelWithNCE(inputs=[X_in, A_in, I_in, input_ids, decoder_tokens_input], outputs=[out, out2])\n",
    "\n",
    "# compiling model and adding metrics\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy', perplexity], run_eagerly=True)\n",
    "\n",
    "topic_expan_generator = TopicExpanTrainGen(graph_list, documents[:-20000], documents_labels[:-20000], batch_size, mini_batch_size, encoded_topic_to_tokenized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_with_nce\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " gcn_conv (GCNConv)             (None, 300)          15300       ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gcn_conv_1 (GCNConv)           (None, 300)          90300       ['gcn_conv[0][0]',               \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " topic_embedding (GlobalAvgPool  (None, 300)         0           ['gcn_conv_1[0][0]',             \n",
      " )                                                                'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]']              \n",
      "                                thPoolingAndCrossAt                                               \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " topic_attentive_embedding (Top  (None, 512)         0           ['topic_embedding[0][0]',        \n",
      " icAttentiveEmbedding)                                            'tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " decoder_phrase_input (InputLay  [(None, 512)]       0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 512, 1)       0           ['topic_attentive_embedding[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding (  (None, 512, 768)    23834112    ['decoder_phrase_input[0][0]']   \n",
      " TokenAndPositionEmbedding)                                                                       \n",
      "                                                                                                  \n",
      " context_embedding (ContextEmbe  (None, 512, 768)    0           ['reshape[0][0]',                \n",
      " dding)                                                           'tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " document_embedding (Lambda)    (None, 768)          0           ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, 512, 768)    5517056     ['token_and_position_embedding[0]\n",
      " erDecoder)                                                      [0]',                            \n",
      "                                                                  'context_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " bilinear (Bilinear)            (None, 1)            230401      ['topic_embedding[0][0]',        \n",
      "                                                                  'document_embedding[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512, 30522)   23471418    ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 162,640,827\n",
      "Trainable params: 162,640,827\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4c9ad5f47ce4a4ca764c621e41da92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d193443ec184dadbb2d2e6c7b84fb4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0batch [00:00, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 17:02:03.416426: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x302109d0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-01 17:02:03.416455: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2023-04-01 17:02:03.422131: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-01 17:02:03.591558: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/27618 [..............................] - ETA: 224:05:27 - bilinear_accuracy: 0.7500 - bilinear_perplexity: nan - dense_accuracy: 0.0000e+00 - dense_perplexity: 29681.3750WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "    7/27618 [..............................] - ETA: 13:35:52 - bilinear_accuracy: 0.7411 - bilinear_perplexity: nan - dense_accuracy: 0.1037 - dense_perplexity: 24486.3281 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 15\u001b[0m\n\u001b[1;32m      6\u001b[0m checkpoint_filepath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./checkpoint.h5\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m model_checkpoint_callback \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[1;32m      8\u001b[0m     filepath\u001b[38;5;241m=\u001b[39mcheckpoint_filepath,\n\u001b[1;32m      9\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdense_perplexity\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     12\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     13\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtopic_expan_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mTqdmCallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/keras/utils/traceback_utils.py:65\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/keras/engine/training.py:1650\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1642\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mexperimental\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1644\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1647\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   1648\u001b[0m ):\n\u001b[1;32m   1649\u001b[0m     callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1650\u001b[0m     tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1651\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1652\u001b[0m         context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/keras/engine/training.py:1249\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m   1247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain_function\u001b[39m(iterator):\n\u001b[1;32m   1248\u001b[0m     \u001b[38;5;124;03m\"\"\"Runs a training execution with a single step.\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1249\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstep_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/keras/engine/training.py:1233\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1229\u001b[0m     run_step \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mfunction(\n\u001b[1;32m   1230\u001b[0m         run_step, jit_compile\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reduce_retracing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m     )\n\u001b[1;32m   1232\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(iterator)\n\u001b[0;32m-> 1233\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1234\u001b[0m outputs \u001b[38;5;241m=\u001b[39m reduce_per_replica(\n\u001b[1;32m   1235\u001b[0m     outputs,\n\u001b[1;32m   1236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy,\n\u001b[1;32m   1237\u001b[0m     reduction\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_reduction_method,\n\u001b[1;32m   1238\u001b[0m )\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:1316\u001b[0m, in \u001b[0;36mStrategyBase.run\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscope():\n\u001b[1;32m   1312\u001b[0m   \u001b[38;5;66;03m# tf.distribute supports Eager functions, so AutoGraph should not be\u001b[39;00m\n\u001b[1;32m   1313\u001b[0m   \u001b[38;5;66;03m# applied when the caller is also in Eager mode.\u001b[39;00m\n\u001b[1;32m   1314\u001b[0m   fn \u001b[38;5;241m=\u001b[39m autograph\u001b[38;5;241m.\u001b[39mtf_convert(\n\u001b[1;32m   1315\u001b[0m       fn, autograph_ctx\u001b[38;5;241m.\u001b[39mcontrol_status_ctx(), convert_by_default\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m-> 1316\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extended\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:2895\u001b[0m, in \u001b[0;36mStrategyExtendedV1.call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2893\u001b[0m   kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m   2894\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy()\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2895\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_for_each_replica\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/distribute/distribute_lib.py:3696\u001b[0m, in \u001b[0;36m_DefaultDistributionExtended._call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3694\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call_for_each_replica\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn, args, kwargs):\n\u001b[1;32m   3695\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ReplicaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_container_strategy(), replica_id_in_sync_group\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m-> 3696\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/autograph/impl/api.py:595\u001b[0m, in \u001b[0;36mcall_with_unspecified_conversion_status.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    594\u001b[0m   \u001b[38;5;28;01mwith\u001b[39;00m ag_ctx\u001b[38;5;241m.\u001b[39mControlStatusCtx(status\u001b[38;5;241m=\u001b[39mag_ctx\u001b[38;5;241m.\u001b[39mStatus\u001b[38;5;241m.\u001b[39mUNSPECIFIED):\n\u001b[0;32m--> 595\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/keras/engine/training.py:1222\u001b[0m, in \u001b[0;36mModel.make_train_function.<locals>.step_function.<locals>.run_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   1221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_step\u001b[39m(data):\n\u001b[0;32m-> 1222\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1223\u001b[0m     \u001b[38;5;66;03m# Ensure counter is updated only if `train_step` succeeds.\u001b[39;00m\n\u001b[1;32m   1224\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcontrol_dependencies(_minimum_control_deps(outputs)):\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:880\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    877\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[0;32m--> 880\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    882\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    883\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:912\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    909\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    910\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    911\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 912\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    915\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compiler.py:134\u001b[0m, in \u001b[0;36mTracingCompiler.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    132\u001b[0m   (concrete_function,\n\u001b[1;32m    133\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m--> 134\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconcrete_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:1745\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1741\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1743\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1744\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1745\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1746\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1747\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m     args,\n\u001b[1;32m   1749\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1750\u001b[0m     executing_eagerly)\n\u001b[1;32m   1751\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/eager/polymorphic_function/monomorphic_function.py:378\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    377\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 378\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    380\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    381\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    382\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    383\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    384\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    387\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    391\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "# NOTE: Ignore warning about gradients not existing for BERT's dense layer since \n",
    "# the dense layers are not used and are thus unconnected and do not need training\n",
    "\n",
    "checkpoint_filepath = './checkpoint.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='dense_perplexity',\n",
    "    save_weights_only=True,\n",
    "    save_freq=1000,\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(topic_expan_generator, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[TqdmCallback(verbose=1), model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "from tensorflow import int64\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import tensorflow_ranking\n",
    "\n",
    "from spektral.layers import GCNConv, GlobalAvgPool, GraphMasking\n",
    "\n",
    "# n_out = dataset.n_labels\n",
    "\n",
    "X_in = Input(shape=(50))\n",
    "A_in = Input(shape=(None,), sparse=True)\n",
    "I_in = Input(shape=(), dtype=int64)\n",
    "\n",
    "X = GCNConv(32, activation='relu')([X_in, A_in])\n",
    "X = GCNConv(32, activation='relu')([X, A_in])\n",
    "X = GlobalAvgPool()([X, I_in])\n",
    "\n",
    "shared_bilinear = tensorflow_ranking.keras.layers.Bilinear(32, 32)\n",
    "X_1 = shared_bilinear([X, X])\n",
    "X = shared_bilinear([X, X], training=False)\n",
    "\n",
    "out = Dense(2, activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs=[X_in, A_in, I_in], outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "preds = tf.constant([[[-11.7803297],\n",
    " [-9.34260654],\n",
    " [-14.0992193],\n",
    " [-9.90242]],[[-11.7803297],\n",
    " [-9.34260654],\n",
    " [-14.0992193],\n",
    " [-9.90242]]], dtype=float)\n",
    "\n",
    "target = tf.constant([[[0],\n",
    " [0],\n",
    " [0],\n",
    " [1]],[[0],\n",
    " [0],\n",
    " [0],\n",
    " [1]]], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoided = tf.keras.activations.sigmoid(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=tf.reshape(target, shape=(2, -1)), logits=tf.reshape(sigmoided, shape=(2, -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.activations.softmax(tf.reshape(preds, shape=(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(target, shape=(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e907c9c662b38b6fe9e69d6881d7a36bc6e4ecfbfed88be2a66ef06ac6f6bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
