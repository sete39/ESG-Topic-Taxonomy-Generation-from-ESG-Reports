{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading df\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../dataset/DBPEDIA_train.csv')\n",
    "document_list = df['text'].to_numpy().astype('str')\n",
    "document_list.dtype\n",
    "print('Finished reading df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding topics to create the adjacency matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "topics = np.concatenate((df['l1'].unique(), df['l2'].unique(), df['l3'].unique()))\n",
    "labelEncoder.fit(topics)\n",
    "\n",
    "def encode_topic(topic):\n",
    "    print(type(topic))\n",
    "    return labelEncoder.transform(topic)\n",
    "\n",
    "df['l1_encoded'] = labelEncoder.transform(df['l1'])\n",
    "df['l2_encoded'] = labelEncoder.transform(df['l2'])\n",
    "df['l3_encoded'] = labelEncoder.transform(df['l3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 01:39:45.246961: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-01 01:39:45.371015: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-01 01:39:45.877791: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cudnn-8.2.4.15-11.4-aa4jklovfx75n2q4jcbyc3dgemu5nqpu/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cuda-11.4.2-sg5bamqhm7bmiz3nf43yzo5thibytq7t/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libxml2-2.9.12-i7nsnmeeeukas6sc2rqa6adykxtky4yj/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/zlib-1.2.11-un5j7szis72ayqdk4yzjlp6vkjrcbtn7/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/xz-5.2.5-wfweai2jkkplgry3aqkfb5fqg44fuxtf/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libiconv-1.16-xa6oobihv7qsbf743iq5g3ioyriyjxwj/lib\n",
      "2023-04-01 01:39:45.877879: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cudnn-8.2.4.15-11.4-aa4jklovfx75n2q4jcbyc3dgemu5nqpu/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cuda-11.4.2-sg5bamqhm7bmiz3nf43yzo5thibytq7t/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libxml2-2.9.12-i7nsnmeeeukas6sc2rqa6adykxtky4yj/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/zlib-1.2.11-un5j7szis72ayqdk4yzjlp6vkjrcbtn7/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/xz-5.2.5-wfweai2jkkplgry3aqkfb5fqg44fuxtf/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libiconv-1.16-xa6oobihv7qsbf743iq5g3ioyriyjxwj/lib\n",
      "2023-04-01 01:39:45.877885: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_topic_to_tokenized_dict = {}\n",
    "for topic in topics:\n",
    "    # dbpedia categories are in PascalCase, so this makes them spaced\n",
    "    spaced_words = re.sub( r\"([A-Z])\", r\" \\1\", topic)[1:]\n",
    "    tokenized_sequence = tokenizer.encode_plus(spaced_words, add_special_tokens=True, max_length=max_len, padding='max_length')['input_ids']\n",
    "\n",
    "    encoded_topic_to_tokenized_dict[labelEncoder.transform([topic])[0]] = tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = df['text'].apply(lambda doc: np.array(tokenizer.encode_plus(doc, add_special_tokens=True, max_length=max_len, padding='max_length', truncation=True)['input_ids'])).to_numpy()\n",
    "# documents_labels = labelEncoder.transform(df['l3'].to_numpy())\n",
    "# documents_fixed = np.empty(shape=(len(documents), max_len))\n",
    "# for i, doc in enumerate(documents):\n",
    "#     documents_fixed[i] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"tokenized_dbpedia.pkl\", \"wb\") as f:\n",
    "#     pickle.dump([documents_fixed, documents_labels], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished getting tokenized file\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with (open(\"./tokenized_dbpedia.pkl\", \"rb\")) as f:\n",
    "    documents, documents_labels = pickle.load(f)\n",
    "print('Finished getting tokenized file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    l1 = row['l1_encoded']\n",
    "    l2 = row['l2_encoded']\n",
    "    l3 = row['l3_encoded']\n",
    "\n",
    "    if l1 not in graph_dict:\n",
    "        graph_dict[l1] = {}\n",
    "    if l2 not in graph_dict[l1]:\n",
    "        graph_dict[l1][l2] = {} \n",
    "\n",
    "    graph_dict[l1][l2][l3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating adjacency matrix\n",
    "adj_matrix = np.zeros((len(topics), len(topics)))\n",
    "square_numeric_dict = {'source': [], 'target': []}\n",
    "for i, df_row in df.iterrows():\n",
    "    l1 = df_row['l1_encoded']\n",
    "    l2 = df_row['l2_encoded']\n",
    "    l3 = df_row['l3_encoded']\n",
    "\n",
    "    adj_matrix[l1, l2] = 1\n",
    "    adj_matrix[l2, l3] = 1\n",
    "\n",
    "for i, row in enumerate(adj_matrix):\n",
    "    for j, value in enumerate(row):\n",
    "        if value == 0:\n",
    "            continue\n",
    "        square_numeric_dict['source'].append(i)\n",
    "        square_numeric_dict['target'].append(j)\n",
    "\n",
    "square_numeric_edges = pd.DataFrame(square_numeric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating node features\n",
    "x = np.arange(298)\n",
    "x = labelEncoder.inverse_transform(x)\n",
    "feature_array = x.reshape(298, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished getting GlOVE embedder\n"
     ]
    }
   ],
   "source": [
    "# loading GloVe model to get topic word embeddings\n",
    "# from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "import torchtext\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
    "print('Finished getting GlOVE embedder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['MASK'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating \"ego-graphs\" (each node is seperated into a graph with itself, parent, and siblings)\n",
    "# The base node (so the node itself) will be masked, aka. have a [MASK] embedding\n",
    "# The sibling nodes need to have a negative relationship with the base node (so negative value in adjacency matrix?)\n",
    "from spektral.data import Graph\n",
    "import numpy as np\n",
    "import re\n",
    "def create_ego_graph(l1_topic, l2_topic, l3_topic, graph_dict):\n",
    "    if l3_topic != None:\n",
    "        siblings_list = list(graph_dict[l1_topic][l2_topic].keys())\n",
    "        siblings_list.remove(l3_topic)\n",
    "        base = l3_topic\n",
    "        parent = l2_topic\n",
    "        grandparent = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        all_nodes_list.append(parent)\n",
    "        all_nodes_list.append(grandparent)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "        encoded_parent = node_label_encoder.transform([parent])[0]\n",
    "        encoded_grandparent = node_label_encoder.transform([grandparent])[0]\n",
    "\n",
    "        adj_matrix[encoded_base][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_base] = 1\n",
    "\n",
    "        adj_matrix[encoded_grandparent][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_grandparent] = 1\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "        \n",
    "    elif l2_topic != None:\n",
    "        siblings_list = list(graph_dict[l1_topic].keys())\n",
    "        siblings_list.remove(l2_topic)\n",
    "        base = l2_topic\n",
    "        parent = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        all_nodes_list.append(parent)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "        encoded_parent = node_label_encoder.transform([parent])[0]\n",
    "\n",
    "        adj_matrix[encoded_base][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_base] = 1\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "    \n",
    "    elif l1_topic != None:\n",
    "        siblings_list = list(graph_dict.keys())\n",
    "        siblings_list.remove(l1_topic)\n",
    "        base = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "\n",
    "    ego_features = np.zeros((n_nodes, 50))\n",
    "    encoded_nodes_list = node_label_encoder.transform(all_nodes_list)\n",
    "\n",
    "    for i, node in enumerate(all_nodes_list):\n",
    "        feature = feature_array[node]\n",
    "        split_words_list = re.sub( r\"([A-Z])\", r\" \\1\", feature[0]).split()\n",
    "        n_words = len(split_words_list)\n",
    "        embedding_avg = np.array([glove[word.lower()].numpy() for word in split_words_list]).sum(axis=0)/n_words\n",
    "        \n",
    "        # Masking base node, setting the embedding to all 0's\n",
    "        if (node == base):\n",
    "            embedding_avg = glove['MASK']\n",
    "\n",
    "        ego_features[encoded_nodes_list[i]] = embedding_avg\n",
    "\n",
    "    return Graph(a=adj_matrix, x=ego_features, y=(l1_topic, l2_topic, l3_topic))\n",
    "\n",
    "graph_list = []\n",
    "\n",
    "for l1_topic in graph_dict:\n",
    "    for l2_topic in graph_dict[l1_topic]:\n",
    "        for l3_topic in graph_dict[l1_topic][l2_topic]:\n",
    "            graph_list.append(create_ego_graph(l1_topic, l2_topic, l3_topic, graph_dict))\n",
    "        graph_list.append(create_ego_graph(l1_topic, l2_topic, None, graph_dict))\n",
    "    graph_list.append(create_ego_graph(l1_topic, None, None, graph_dict))\n",
    "\n",
    "graph_list = np.array(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, graph_list: list[Graph], **kwargs):\n",
    "        self.graph_list = graph_list\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        num_l =  np.random.permutation(len(self.graph_list))\n",
    "        return [self.graph_list[i] for i in num_l]\n",
    "    \n",
    "dataset = MyDataset(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from the tensorflow_ranking source code\n",
    "# Source: https://github.com/tensorflow/ranking/blob/v0.5.2/tensorflow_ranking/python/keras/layers.py#L806-L891\n",
    "# Need to modify code since the original implementation only takes two embeddings of the SAME shape\n",
    "# For this case however, the topic embedding and the BERT embedding have two different shapes (50 x 768)\n",
    "import tensorflow as tf\n",
    "\n",
    "class Bilinear(tf.keras.layers.Layer):\n",
    "  \"\"\"A Keras Layer makes bilinear interaction of two vectors.\n",
    "  This Keras Layer implements the bilinear interaction of two vectors of\n",
    "  embedding dimensions. The bilinear, linear and scalar parameters of the\n",
    "  interaction are trainable.\n",
    "  The bilinear interaction are used in the work \"Revisiting two tower models\n",
    "  for unbiased learning to rank\" by Yan et al, see\n",
    "  https://research.google/pubs/pub51296/.\n",
    "  In this work, the bilinear interaction appears to be helpful in model the\n",
    "  complex interaction between position and relevance in unbiased LTR.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, embedding1_dim: int, embedding2_dim: int, output_dim: int, **kwargs: dict[any,\n",
    "                                                                         any]):\n",
    "    \"\"\"Initializer.\n",
    "    Args:\n",
    "      embedding_dim: An integer that indicates the embedding dimension of the\n",
    "        interacting vectors.\n",
    "      output_dim: An integer that indicates the output dimension of the layer.\n",
    "      **kwargs: A dict of keyword arguments for the tf.keras.layers.Layer.\n",
    "    \"\"\"\n",
    "    super().__init__(**kwargs)\n",
    "    self._embedding1_dim = embedding1_dim\n",
    "    self._embedding2_dim = embedding2_dim\n",
    "    self._output_dim = output_dim\n",
    "\n",
    "  def build(self, input_shape: tf.TensorShape):\n",
    "    \"\"\"See tf.keras.layers.Layer.\"\"\"\n",
    "    # Create a trainable weight variable for this layer.\n",
    "    self._bilinear_weight = self.add_weight(\n",
    "        name='bilinear_term',\n",
    "        shape=(self._output_dim, self._embedding1_dim, self._embedding2_dim),  # shape = (output_dim, embedding_1_dim, embedding_2_dim)\n",
    "        initializer=tf.keras.initializers.RandomNormal(stddev=1. / self._embedding1_dim),\n",
    "        trainable=True)\n",
    "    \n",
    "    self._bias = self.add_weight(\n",
    "        name='const_term',\n",
    "        shape=(self._output_dim),\n",
    "        initializer=tf.keras.initializers.Zeros(),\n",
    "        trainable=True)\n",
    "    super().build(input_shape)\n",
    "\n",
    "  def call(self, inputs: tuple[tf.Tensor]) -> tf.Tensor:\n",
    "    \"\"\"Computes bilinear interaction between two vector tensors.\n",
    "    Args:\n",
    "      inputs: A pair of tensors of the same shape [batch_size, embedding_dim].\n",
    "    Returns:\n",
    "      A tensor, of shape [batch_size, output_dim], computed by the bilinear\n",
    "      interaction.\n",
    "    \"\"\"\n",
    "    # Input of the function must be a list of two tensors.\n",
    "    vec_1, vec_2 = inputs\n",
    "    return tf.einsum(\n",
    "        'bk,jkl,bl->bj', vec_1, self._bilinear_weight, vec_2) + self._bias\n",
    "\n",
    "  def compute_output_shape(self, input_shape: tf.TensorShape) -> tuple[int]:\n",
    "    \"\"\"See tf.keras.layers.Layer.\"\"\"\n",
    "    return (input_shape[0], self._output_dim)\n",
    "\n",
    "  def get_config(self) -> dict[str, any]:\n",
    "    \"\"\"See tf.keras.layers.Layer.\"\"\"\n",
    "    config = super().get_config()\n",
    "    config.update({\n",
    "        'embedding1_dim': self._embedding1_dim,\n",
    "        'embedding2_dim': self._embedding2_dim,\n",
    "        'output_dim': self._output_dim\n",
    "    })\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_to_document_embedding(sequence_embedding: tf.Tensor):\n",
    "    # gets the document representation/embedding from a BERT sequence embedding\n",
    "    # by getting the mean-pooling of the sequence \n",
    "    return tf.math.reduce_mean(sequence_embedding, axis=1)\n",
    "\n",
    "class TopicAttentiveEmbedding(tf.keras.layers.Layer):\n",
    "    # Layer for calculating the beta/topic-attentive representation (check TopicExpan paper) of the topic and document embedding\n",
    "    # aka. softmax of the bilinear interaction of the topic embedding with the embedding of each word in the document\n",
    "    # needs to use the same trained weights as the bilinear layer in the Similarity Predictor\n",
    "    def __init__(self,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, topic_embedding: tf.Tensor, sequence_embedding: tf.Tensor, shared_bilinear_layer: tf.keras.layers.Layer):\n",
    "        # topic_embedding shape = (batch_size, topic_embedding_len)\n",
    "        # sequence_embedding shape = (batch_size, max_len, 768)\n",
    "\n",
    "        reshaped_topic_embedding: tf.Tensor = tf.reshape(topic_embedding, (-1, 1, topic_embedding.shape[1]))\n",
    "        reshaped_topic_embedding: tf.Tensor = tf.repeat(reshaped_topic_embedding, sequence_embedding.shape[1], axis=1)\n",
    "\n",
    "        def apply_bilinear(embeddings: tuple[tf.Tensor, tf.Tensor]):\n",
    "            emb1, emb2 = embeddings\n",
    "            bilinear_embedding = tf.reshape(shared_bilinear_layer([emb1, emb2], training=False), shape=[-1])\n",
    "            softmax_bilinear_embedding = tf.nn.softmax(bilinear_embedding)\n",
    "            return softmax_bilinear_embedding\n",
    "        \n",
    "        beta_embedding = tf.map_fn(\n",
    "            apply_bilinear, \n",
    "            elems=(reshaped_topic_embedding, sequence_embedding),\n",
    "            fn_output_signature=tf.TensorSpec(shape=(sequence_embedding.shape[1]))\n",
    "        )\n",
    "        \n",
    "        return beta_embedding\n",
    "    \n",
    "class ContextEmbedding(tf.keras.layers.Layer):\n",
    "    # Layer for calculating the context (Q) representation (check TopicExpan paper) of the topic and document embedding\n",
    "    # aka. the beta representation (which combines the topic and document representations) multiplied by the respective\n",
    "    # word in the document representation\n",
    "    def __init__(self,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs: tuple[tf.Tensor, tf.Tensor]):\n",
    "        topic_attentive_embedding, sequence_embedding = inputs\n",
    "        context_embedding = tf.multiply(sequence_embedding, topic_attentive_embedding)\n",
    "        return context_embedding\n",
    "    \n",
    "class ContextEmbedding(tf.keras.layers.Layer):\n",
    "    # Layer for calculating the context (Q) representation (check TopicExpan paper) of the topic and document embedding\n",
    "    # aka. the beta representation (which combines the topic and document representations) multiplied by the respective\n",
    "    # word in the document representation\n",
    "    def __init__(self,  **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs: tuple[tf.Tensor, tf.Tensor]):\n",
    "        topic_attentive_embedding, sequence_embedding = inputs\n",
    "        context_embedding = tf.multiply(sequence_embedding, topic_attentive_embedding)\n",
    "        return context_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 01:40:04.511977: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-01 01:40:05.111495: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38220 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:b1:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import int64\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from spektral.layers import GCNConv, GlobalAvgPool\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "import keras_nlp\n",
    "\n",
    "learning_rate = 5e-5  # Learning rate\n",
    "epochs = 4  # Number of training epochs\n",
    "batch_size = 32  # Batch size\n",
    "weight_decay = 5e-6\n",
    "mini_batch_size = 4 # a mini-batch will always have 1 positive triple, and (n-1) negative triples\n",
    "                    # i.e. with a mini_batch_size of 4, we have 1 pos. doc. and 3 neg. docs.\n",
    "batch_ratio = int(batch_size / mini_batch_size)\n",
    "\n",
    "max_len = 512\n",
    "vocab_size = tokenizer.vocab_size\n",
    "infoNCE_temprature = 0.1\n",
    "\n",
    "optimizer = Adam(learning_rate, weight_decay=weight_decay)\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn_binary = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "loss_fn_crossentropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "n_out = dataset.n_labels\n",
    "topic_embedding_dimension = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Custom training loop\n",
    "class ModelWithNCE(Model):\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(self, data):\n",
    "        inputs, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            similarity_prediction, phrase_prediction = self(inputs, training=True)\n",
    "            similarity_prediction_infonce = tf.reshape(similarity_prediction / infoNCE_temprature, shape=(mini_batch_size, -1))\n",
    "\n",
    "            # infoNCE_loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=tf.reshape(target[0], shape=(mini_batch_size, -1)), logits=similarity_prediction_infonce))\n",
    "            # infoNCE_loss = loss_fn_crossentropy(tf.reshape(target[0], shape=(mini_batch_size, -1)), similarity_prediction_infonce)\n",
    "            bce_loss = loss_fn_binary(target[0], similarity_prediction)\n",
    "            phrase_loss = loss_fn(target[1], phrase_prediction)\n",
    "\n",
    "            # tf.print(similarity_prediction[:4], bce_loss, phrase_loss, output_stream=sys.stderr)\n",
    "        gradients = tape.gradient([bce_loss, phrase_loss], self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(target, (similarity_prediction, phrase_prediction))\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from spektral.data.utils import to_disjoint\n",
    "from random import shuffle\n",
    "\n",
    "def shift_padded_array_right(arr: np.ndarray):\n",
    "    # replaces last non-zero with a zero (for teacher-forcing)\n",
    "    # return arr\n",
    "    def apply_right_padding(arr: np.ndarray):\n",
    "        # print(arr)\n",
    "        first_zero_index = (arr==0).argmax(axis=0)\n",
    "        shifted = arr.copy()\n",
    "        shifted[first_zero_index-1] = 0\n",
    "        return shifted\n",
    "    \n",
    "    return np.apply_along_axis(\n",
    "        apply_right_padding,\n",
    "        axis=1,\n",
    "        arr=arr\n",
    "    )\n",
    "\n",
    "def shift_padded_array_left(arr: np.ndarray):\n",
    "    # (for teacher-forcing)\n",
    "    shifted_arr = np.roll(arr, -1)\n",
    "    shifted_arr[: :, max_len-1] = 0\n",
    "    return shifted_arr\n",
    "\n",
    "\n",
    "class TopicExpanTrainGen(tf.keras.utils.Sequence):\n",
    "    # Creating a custom Keras generator for model.fit()\n",
    "    # Splits batches into mini-batches of size (mini_batch_size)\n",
    "    # batch size must be a multiple of mini_batch_size\n",
    "\n",
    "    # TODO: Shuffle the dataset on epoch end\n",
    "    def __init__(self, topic_graph_list: list, document_input: np.array, document_topics: np.array, batch_size: int, mini_batch_size: int):\n",
    "        if batch_size % mini_batch_size != 0:\n",
    "            raise Exception('batch_size must be a multiple of mini_batch_size')\n",
    "        if len(document_input) != len(document_topics):\n",
    "            raise Exception('Document list must be equal to the label list in length')\n",
    "        \n",
    "        self.topic_graph_list = topic_graph_list\n",
    "        self.document_input = document_input\n",
    "        self.document_topics = document_topics\n",
    "        self.batch_size = batch_size\n",
    "        self.mini_batch_size = mini_batch_size\n",
    "\n",
    "        # used for creating batches (to get random documents for the negative samples of the batch)\n",
    "        # Stores a list of indexes which match the given topic (key)\n",
    "        self.document_topics_dict = {}\n",
    "        for i, topic in enumerate(document_topics):\n",
    "            if topic not in self.document_topics_dict:\n",
    "                self.document_topics_dict[topic] = [i]\n",
    "            else:\n",
    "                self.document_topics_dict[topic].append(i)\n",
    "\n",
    "        # ego-graph list\n",
    "        self.graph_features = []\n",
    "        # maps a topic to its ego-graph\n",
    "        self._topic_to_graph_dict = {}\n",
    "\n",
    "        for graph in topic_graph_list:\n",
    "            self.graph_features.append(graph)\n",
    "            \n",
    "            label = graph.y[2] if graph.y[2] is not None else graph.y[1] if graph.y[1] is not None else graph.y[0]\n",
    "\n",
    "            self._topic_to_graph_dict[label] = graph\n",
    "\n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.document_topics) / (self.batch_size / self.mini_batch_size))\n",
    "\n",
    "    def __getitem__(self, non_batch_index: int):\n",
    "        # retrieving batch at non_batch_index\n",
    "        batch_to_mini_batch_ratio = int((self.batch_size / self.mini_batch_size))\n",
    "        idx = non_batch_index * batch_to_mini_batch_ratio\n",
    "\n",
    "        batch_graph_list = []\n",
    "        batch_document_list = []\n",
    "        batch_similarity_label = []\n",
    "        batch_phrase_list = []\n",
    "        for mini_batch in range(batch_to_mini_batch_ratio):\n",
    "            # creating mini_batch / getting the positive document\n",
    "            positive_doc_idx = min(idx + mini_batch, len(self.document_topics) - 1)\n",
    "            positive_doc_topic = self.document_topics[positive_doc_idx]\n",
    "            positive_document = [self.document_input[positive_doc_idx]]\n",
    "            positive_graph = self._topic_to_graph_dict[positive_doc_topic]\n",
    "\n",
    "            exclude_list = [positive_doc_topic]\n",
    "            negative_documents = []\n",
    "            mini_batch_phrase_list = [encoded_topic_to_tokenized_dict[positive_doc_topic]] \n",
    "            for _ in range(self.mini_batch_size-1):\n",
    "                # creating mini_batch / getting random negative documents for the rest of the minibatch\n",
    "                random_negative_topic = np.random.choice(list(set([x for x in self.document_topics_dict.keys()]) - set(exclude_list)))\n",
    "                exclude_list.append(random_negative_topic)\n",
    "\n",
    "                random_negative_document_idx = np.random.choice(self.document_topics_dict[random_negative_topic])\n",
    "                random_negative_document = self.document_input[random_negative_document_idx]\n",
    "                negative_documents.append(random_negative_document)\n",
    "\n",
    "                mini_batch_phrase_list.append(encoded_topic_to_tokenized_dict[random_negative_topic])\n",
    "            \n",
    "            # label for the similarirt prediction is 1 for the positive document (first element),\n",
    "            # and zero for all negative documents\n",
    "            mini_batch_similarity_label = [1] + ([0] * (self.mini_batch_size-1))\n",
    "\n",
    "            # shuffling mini batch\n",
    "            to_shuffle_list = list(zip(mini_batch_similarity_label, (positive_document + negative_documents), mini_batch_phrase_list))\n",
    "            shuffle(to_shuffle_list)\n",
    "            mini_batch_similarity_label, mini_batch_document_list, mini_batch_phrase_list = zip(*to_shuffle_list)\n",
    "            # mini_batch_document_list = (positive_document + negative_documents)\n",
    "            batch_graph_list.append(np.array([positive_graph for _ in range(self.mini_batch_size)]))\n",
    "            batch_document_list.append(mini_batch_document_list)\n",
    "            batch_phrase_list.append(mini_batch_phrase_list)\n",
    "            batch_similarity_label.append(mini_batch_similarity_label)\n",
    "        \n",
    "        # GNN batched inputs\n",
    "        batch_graph_list = np.array(batch_graph_list).flatten()\n",
    "        x_in, a_in, i_in = to_disjoint(\n",
    "            x_list=[g.x for g in batch_graph_list],\n",
    "            a_list=[g.a for g in batch_graph_list]\n",
    "        )\n",
    "        a_in_sparse_tensor = tf.sparse.SparseTensor(\n",
    "            indices=np.array([a_in.row, a_in.col]).T,\n",
    "            values=a_in.data,\n",
    "            dense_shape=a_in.shape\n",
    "        )\n",
    "        # document encoder batch inputs\n",
    "        batch_document_list = np.array(batch_document_list).reshape((-1, max_len))\n",
    "        \n",
    "        # similarity predictor batch labels\n",
    "        batch_similarity_label = np.array(batch_similarity_label).reshape((-1, 1))\n",
    "        \n",
    "        # phrase generator phrases/labels\n",
    "        batch_phrase_list = np.array(batch_phrase_list).reshape((-1, max_len))\n",
    "\n",
    "        # combining inputs/features and labels into tuples \n",
    "        model_inputs = (\n",
    "            tf.convert_to_tensor(x_in, dtype=tf.float32), \n",
    "            a_in_sparse_tensor, \n",
    "            tf.convert_to_tensor(i_in, dtype=tf.int32), \n",
    "            tf.convert_to_tensor(batch_document_list),\n",
    "            tf.convert_to_tensor(shift_padded_array_right(batch_phrase_list).reshape((-1, max_len)))\n",
    "        )\n",
    "        model_outputs = (\n",
    "            tf.convert_to_tensor(batch_similarity_label), \n",
    "            tf.convert_to_tensor(shift_padded_array_left(batch_phrase_list))\n",
    "        )\n",
    "        return model_inputs, model_outputs\n",
    "\n",
    "    # def on_epoch_end(self):\n",
    "    #     # shuffling mini batch\n",
    "    #     to_shuffle_list = list(zip(self.document_input, self.document_topics))\n",
    "    #     shuffle(to_shuffle_list)\n",
    "    #     self.document_input, self.document_topics = zip(*to_shuffle_list)\n",
    "    #     return super().on_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_expan_generator = TopicExpanTrainGen(graph_list, documents, documents_labels, batch_size, mini_batch_size)\n",
    "# arr = topic_expan_generator.__getitem__(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# arr[1][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 01:40:07.250721: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/k21148846/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "shared_bilinear = Bilinear(topic_embedding_dimension, 768, 1)\n",
    "\n",
    "# GNNs (Topic Encoder)\n",
    "X_in = Input(shape=(dataset.n_node_features))\n",
    "A_in = Input(shape=(None,), sparse=True)\n",
    "I_in = Input(shape=(), dtype=int64)\n",
    "\n",
    "topic_embedding = GCNConv(topic_embedding_dimension, activation='relu')([X_in, A_in])\n",
    "topic_embedding = GCNConv(topic_embedding_dimension, activation='relu')([topic_embedding, A_in])\n",
    "topic_embedding = GlobalAvgPool(name='topic_embedding')([topic_embedding, I_in])\n",
    "\n",
    "# BERT Embedding (Document Encoder)\n",
    "max_seq_length = max_len\n",
    "encoder = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "embedding = encoder(input_ids)[0]\n",
    "\n",
    "# Transformer Decoders (Phrase Generator)\n",
    "decoder_tokens_input = Input(shape=(max_len,), name=\"decoder_phrase_input\")\n",
    "decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(vocabulary_size=vocab_size, sequence_length=max_len, embedding_dim=768, mask_zero=True)(decoder_tokens_input)\n",
    "\n",
    "# Getting context embedding for decoder\n",
    "topic_attentive_embedding = TopicAttentiveEmbedding()(topic_embedding, embedding, shared_bilinear, training=False)\n",
    "topic_attentive_embedding = tf.keras.layers.Reshape((max_len, 1))(topic_attentive_embedding)\n",
    "context_embedding = ContextEmbedding()([topic_attentive_embedding, embedding])\n",
    "\n",
    "transformer_decoder = keras_nlp.layers.TransformerDecoder(\n",
    "    num_heads=16, \n",
    "    intermediate_dim=max_len,\n",
    "    dropout=0.1\n",
    ")(decoder_embedding, context_embedding)\n",
    "\n",
    "# Transformer Output\n",
    "out2 = Dense(vocab_size)(transformer_decoder)\n",
    "\n",
    "# Output Bilinear layer (Similarity Predictor)\n",
    "document_embedding = Lambda(sequence_to_document_embedding, name='document_embedding')(embedding)\n",
    "out = shared_bilinear([topic_embedding, document_embedding])\n",
    "\n",
    "# Outputs\n",
    "model = ModelWithNCE(inputs=[X_in, A_in, I_in, input_ids, decoder_tokens_input], outputs=[out, out2])\n",
    "\n",
    "# compiling model and adding metrics\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy', perplexity], run_eagerly=True)\n",
    "\n",
    "topic_expan_generator = TopicExpanTrainGen(graph_list, documents[:-20000], documents_labels[:-20000], batch_size, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_with_nce\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " gcn_conv (GCNConv)             (None, 300)          15300       ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gcn_conv_1 (GCNConv)           (None, 300)          90300       ['gcn_conv[0][0]',               \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " topic_embedding (GlobalAvgPool  (None, 300)         0           ['gcn_conv_1[0][0]',             \n",
      " )                                                                'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]']              \n",
      "                                thPoolingAndCrossAt                                               \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " topic_attentive_embedding (Top  (None, 512)         0           ['topic_embedding[0][0]',        \n",
      " icAttentiveEmbedding)                                            'tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " decoder_phrase_input (InputLay  [(None, 512)]       0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 512, 1)       0           ['topic_attentive_embedding[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding (  (None, 512, 768)    23834112    ['decoder_phrase_input[0][0]']   \n",
      " TokenAndPositionEmbedding)                                                                       \n",
      "                                                                                                  \n",
      " context_embedding (ContextEmbe  (None, 512, 768)    0           ['reshape[0][0]',                \n",
      " dding)                                                           'tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " document_embedding (Lambda)    (None, 768)          0           ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, 512, 768)    5517056     ['token_and_position_embedding[0]\n",
      " erDecoder)                                                      [0]',                            \n",
      "                                                                  'context_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " bilinear (Bilinear)            (None, 1)            230401      ['topic_embedding[0][0]',        \n",
      "                                                                  'document_embedding[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512, 30522)   23471418    ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 162,640,827\n",
      "Trainable params: 162,640,827\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96bf0ebe6d4405ebf9d0de62bcb1827",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0epoch [00:00, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea75203c6c5c44c28a38e00f7dd9b198",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0batch [00:00, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-01 01:40:27.930020: I tensorflow/compiler/xla/service/service.cc:173] XLA service 0x6210d970 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2023-04-01 01:40:27.930048: I tensorflow/compiler/xla/service/service.cc:181]   StreamExecutor device (0): NVIDIA A100-SXM4-40GB, Compute Capability 8.0\n",
      "2023-04-01 01:40:27.935159: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2023-04-01 01:40:28.033263: I tensorflow/compiler/jit/xla_compilation_cache.cc:477] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1/27618 [..............................] - ETA: 223:05:49 - bilinear_accuracy: 0.7188 - bilinear_perplexity: nan - dense_accuracy: 0.0000e+00 - dense_perplexity: 30871.3418WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model/bert/pooler/dense/kernel:0', 'tf_bert_model/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
      " 3266/27618 [==>...........................] - ETA: 3:51:02 - bilinear_accuracy: 0.9193 - bilinear_perplexity: nan - dense_accuracy: 0.5969 - dense_perplexity: 8.2344"
     ]
    }
   ],
   "source": [
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "# NOTE: Ignore warning about gradients not existing for BERT's dense layer since \n",
    "# the dense layers are not used and are thus unconnected and do not need training\n",
    "\n",
    "checkpoint_filepath = './checkpoint.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='dense_perplexity',\n",
    "    save_weights_only=True,\n",
    "    save_freq=1000,\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(topic_expan_generator, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[TqdmCallback(verbose=1), model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('test.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "from tensorflow import int64\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import tensorflow_ranking\n",
    "\n",
    "from spektral.layers import GCNConv, GlobalAvgPool, GraphMasking\n",
    "\n",
    "# n_out = dataset.n_labels\n",
    "\n",
    "X_in = Input(shape=(50))\n",
    "A_in = Input(shape=(None,), sparse=True)\n",
    "I_in = Input(shape=(), dtype=int64)\n",
    "\n",
    "X = GCNConv(32, activation='relu')([X_in, A_in])\n",
    "X = GCNConv(32, activation='relu')([X, A_in])\n",
    "X = GlobalAvgPool()([X, I_in])\n",
    "\n",
    "shared_bilinear = tensorflow_ranking.keras.layers.Bilinear(32, 32)\n",
    "X_1 = shared_bilinear([X, X])\n",
    "X = shared_bilinear([X, X], training=False)\n",
    "\n",
    "out = Dense(2, activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs=[X_in, A_in, I_in], outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "preds = tf.constant([[[-11.7803297],\n",
    " [-9.34260654],\n",
    " [-14.0992193],\n",
    " [-9.90242]],[[-11.7803297],\n",
    " [-9.34260654],\n",
    " [-14.0992193],\n",
    " [-9.90242]]], dtype=float)\n",
    "\n",
    "target = tf.constant([[[0],\n",
    " [0],\n",
    " [0],\n",
    " [1]],[[0],\n",
    " [0],\n",
    " [0],\n",
    " [1]]], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoided = tf.keras.activations.sigmoid(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=tf.reshape(target, shape=(2, -1)), logits=tf.reshape(sigmoided, shape=(2, -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.activations.softmax(tf.reshape(preds, shape=(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(target, shape=(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e907c9c662b38b6fe9e69d6881d7a36bc6e4ecfbfed88be2a66ef06ac6f6bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
