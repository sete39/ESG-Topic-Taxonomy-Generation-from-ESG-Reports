{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading df\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./DBpedia_train_terms_augmented.csv')\n",
    "document_list = df['text'].to_numpy().astype('str')\n",
    "document_list.dtype\n",
    "\n",
    "print('Finished reading df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "240942"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l3</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>William Alexander Massey (October 7, 1856 – Ma...</td>\n",
       "      <td>Agent</td>\n",
       "      <td>Politician</td>\n",
       "      <td>Senator</td>\n",
       "      <td>massey</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Lions is the sixth studio album by American ro...</td>\n",
       "      <td>Work</td>\n",
       "      <td>MusicalWork</td>\n",
       "      <td>Album</td>\n",
       "      <td>album</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Pirqa (Aymara and Quechua for wall, hispaniciz...</td>\n",
       "      <td>Place</td>\n",
       "      <td>NaturalPlace</td>\n",
       "      <td>Mountain</td>\n",
       "      <td>pirqa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Cancer Prevention Research is a biweekly peer-...</td>\n",
       "      <td>Work</td>\n",
       "      <td>PeriodicalLiterature</td>\n",
       "      <td>AcademicJournal</td>\n",
       "      <td>oncology</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>The Princeton University Chapel is located on ...</td>\n",
       "      <td>Place</td>\n",
       "      <td>Building</td>\n",
       "      <td>HistoricBuilding</td>\n",
       "      <td>chapel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text     l1  \\\n",
       "0           0  William Alexander Massey (October 7, 1856 – Ma...  Agent   \n",
       "1           1  Lions is the sixth studio album by American ro...   Work   \n",
       "2           2  Pirqa (Aymara and Quechua for wall, hispaniciz...  Place   \n",
       "3           3  Cancer Prevention Research is a biweekly peer-...   Work   \n",
       "4           4  The Princeton University Chapel is located on ...  Place   \n",
       "\n",
       "                     l2                l3     terms  \n",
       "0            Politician           Senator    massey  \n",
       "1           MusicalWork             Album     album  \n",
       "2          NaturalPlace          Mountain     pirqa  \n",
       "3  PeriodicalLiterature   AcademicJournal  oncology  \n",
       "4              Building  HistoricBuilding    chapel  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding topics to create the adjacency matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "VIRTUAL_NODE_NAME = 'ZZ_VIRTUAL'\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "topics = np.concatenate((df['l1'].unique(), df['l2'].unique(), df['l3'].unique()))\n",
    "# For virtual node when expanding topic taxonomy\n",
    "topics = np.append(topics, VIRTUAL_NODE_NAME)\n",
    "labelEncoder.fit(topics)\n",
    "\n",
    "def encode_topic(topic):\n",
    "    print(type(topic))\n",
    "    return labelEncoder.transform(topic)\n",
    "\n",
    "df['l1_encoded'] = labelEncoder.transform(df['l1'])\n",
    "df['l2_encoded'] = labelEncoder.transform(df['l2'])\n",
    "df['l3_encoded'] = labelEncoder.transform(df['l3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-30 17:29:32.483716: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-30 17:29:33.060611: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-05-30 17:29:36.341097: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cudnn-8.2.4.15-11.4-aa4jklovfx75n2q4jcbyc3dgemu5nqpu/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cuda-11.4.2-sg5bamqhm7bmiz3nf43yzo5thibytq7t/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libxml2-2.9.12-i7nsnmeeeukas6sc2rqa6adykxtky4yj/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/zlib-1.2.11-un5j7szis72ayqdk4yzjlp6vkjrcbtn7/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/xz-5.2.5-wfweai2jkkplgry3aqkfb5fqg44fuxtf/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libiconv-1.16-xa6oobihv7qsbf743iq5g3ioyriyjxwj/lib::/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/anaconda3-2021.05-5d7m6vbj62rh6onwyyz6mdqatpag2b3b/lib:/lib\n",
      "2023-05-30 17:29:36.341486: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cudnn-8.2.4.15-11.4-aa4jklovfx75n2q4jcbyc3dgemu5nqpu/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cuda-11.4.2-sg5bamqhm7bmiz3nf43yzo5thibytq7t/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libxml2-2.9.12-i7nsnmeeeukas6sc2rqa6adykxtky4yj/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/zlib-1.2.11-un5j7szis72ayqdk4yzjlp6vkjrcbtn7/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/xz-5.2.5-wfweai2jkkplgry3aqkfb5fqg44fuxtf/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libiconv-1.16-xa6oobihv7qsbf743iq5g3ioyriyjxwj/lib::/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/anaconda3-2021.05-5d7m6vbj62rh6onwyyz6mdqatpag2b3b/lib:/lib\n",
      "2023-05-30 17:29:36.341492: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_topic_to_tokenized_dict = {}\n",
    "for topic in topics:\n",
    "    # dbpedia categories are in PascalCase, so this makes them spaced\n",
    "    spaced_words = re.sub( r\"([A-Z])\", r\" \\1\", topic)[1:]\n",
    "    tokenized_sequence = tokenizer.encode_plus(spaced_words, add_special_tokens=True, max_length=max_len, padding='max_length')['input_ids']\n",
    "\n",
    "    encoded_topic_to_tokenized_dict[labelEncoder.transform([topic])[0]] = tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = df['text'].apply(lambda doc: np.array(tokenizer.encode_plus(doc, add_special_tokens=True, max_length=max_len, padding='max_length', truncation=True)['input_ids'])).to_numpy()\n",
    "# documents_labels = labelEncoder.transform(df['l3'].to_numpy())\n",
    "# documents_fixed = np.empty(shape=(len(documents), max_len))\n",
    "# for i, doc in enumerate(documents):\n",
    "#     documents_fixed[i] = doc\n",
    "# terms = df['terms'].apply(lambda doc: np.array(tokenizer.encode_plus(doc, add_special_tokens=True, max_length=max_len, padding='max_length', truncation=True)['input_ids'])).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"tokenized_dbpedia.pkl\", \"wb\") as f:\n",
    "#     pickle.dump([documents_fixed, documents_labels], f)\n",
    "\n",
    "# import pickle\n",
    "# with open(\"tokenized_dbpedia_terms.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(terms, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished getting tokenized files\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with (open(\"./tokenized_dbpedia.pkl\", \"rb\")) as f:\n",
    "    documents, documents_labels = pickle.load(f)\n",
    "\n",
    "with (open(\"./tokenized_dbpedia_terms.pkl\", \"rb\")) as f:\n",
    "    terms = pickle.load(f)\n",
    "    \n",
    "print('Finished getting tokenized files')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    l1 = row['l1_encoded']\n",
    "    l2 = row['l2_encoded']\n",
    "    l3 = row['l3_encoded']\n",
    "\n",
    "    if l1 not in graph_dict:\n",
    "        graph_dict[l1] = {}\n",
    "    if l2 not in graph_dict[l1]:\n",
    "        graph_dict[l1][l2] = {} \n",
    "\n",
    "    graph_dict[l1][l2][l3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating node features\n",
    "x = np.arange(298)\n",
    "x = labelEncoder.inverse_transform(x)\n",
    "feature_array = x.reshape(298, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished getting GlOVE embedder\n"
     ]
    }
   ],
   "source": [
    "# loading GloVe model to get topic word embeddings\n",
    "# from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "import torchtext\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
    "print('Finished getting GlOVE embedder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import Graph\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def create_ego_graph(l1_topic, l2_topic, l3_topic, graph_dict, feature_array, is_virtual=False):\n",
    "    # Creating \"ego-graphs\" (each node is seperated into a graph with itself, parent, and siblings)\n",
    "    # The base node (so the node itself) will be masked, aka. have a [MASK] embedding\n",
    "    # The sibling nodes need to have a negative relationship with the base node (so negative value in adjacency matrix?)\n",
    "    if l3_topic != None:\n",
    "        siblings_list = list(graph_dict[l1_topic][l2_topic].keys())\n",
    "        siblings_list.remove(l3_topic)\n",
    "        base = l3_topic\n",
    "        parent = l2_topic\n",
    "        grandparent = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        all_nodes_list.append(parent)\n",
    "        all_nodes_list.append(grandparent)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "        encoded_parent = node_label_encoder.transform([parent])[0]\n",
    "        encoded_grandparent = node_label_encoder.transform([grandparent])[0]\n",
    "\n",
    "        adj_matrix[encoded_base][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_base] = 1\n",
    "\n",
    "        adj_matrix[encoded_grandparent][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_grandparent] = 1\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "        \n",
    "    elif l2_topic != None:\n",
    "        siblings_list = list(graph_dict[l1_topic].keys())\n",
    "        siblings_list.remove(l2_topic)\n",
    "        base = l2_topic\n",
    "        parent = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        all_nodes_list.append(parent)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "        encoded_parent = node_label_encoder.transform([parent])[0]\n",
    "\n",
    "        adj_matrix[encoded_base][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_base] = 1\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "    \n",
    "    elif l1_topic != None:\n",
    "        siblings_list = list(graph_dict.keys())\n",
    "        siblings_list.remove(l1_topic)\n",
    "        base = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "\n",
    "    ego_features = np.zeros((n_nodes, 50))\n",
    "    encoded_nodes_list = node_label_encoder.transform(all_nodes_list)\n",
    "\n",
    "    for i, node in enumerate(all_nodes_list):\n",
    "        # Masking base node, setting the embedding to all 0's\n",
    "        if (node == base):\n",
    "            embedding_avg = glove['MASK']\n",
    "        else:\n",
    "            feature = feature_array[node]\n",
    "            split_words_list = re.sub( r\"([A-Z])\", r\" \\1\", feature[0]).split()\n",
    "            n_words = len(split_words_list)\n",
    "            embedding_avg = np.array([glove[word.lower()].numpy() for word in split_words_list]).sum(axis=0)/n_words\n",
    "\n",
    "        ego_features[encoded_nodes_list[i]] = embedding_avg\n",
    "\n",
    "    return Graph(a=adj_matrix, x=ego_features, y=(l1_topic, l2_topic, l3_topic))\n",
    "\n",
    "graph_list = []\n",
    "\n",
    "for l1_topic in graph_dict:\n",
    "    for l2_topic in graph_dict[l1_topic]:\n",
    "        for l3_topic in graph_dict[l1_topic][l2_topic]:\n",
    "            graph_list.append(create_ego_graph(l1_topic, l2_topic, l3_topic, graph_dict, feature_array))\n",
    "        graph_list.append(create_ego_graph(l1_topic, l2_topic, None, graph_dict, feature_array))\n",
    "    graph_list.append(create_ego_graph(l1_topic, None, None, graph_dict, feature_array))\n",
    "\n",
    "graph_list = np.array(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, graph_list: list[Graph], **kwargs):\n",
    "        self.graph_list = graph_list\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        num_l =  np.random.permutation(len(self.graph_list))\n",
    "        return [self.graph_list[i] for i in num_l]\n",
    "    \n",
    "dataset = MyDataset(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.Bilinear import Bilinear\n",
    "from layers.ContextEmbedding import ContextEmbedding\n",
    "from layers.TopicAttentiveEmbedding import TopicAttentiveEmbedding\n",
    "\n",
    "from utils.TopicExpanTrainGen import TopicExpanTrainGen\n",
    "\n",
    "def sequence_to_document_embedding(sequence_embedding: tf.Tensor):\n",
    "    # gets the document representation/embedding from a BERT sequence embedding\n",
    "    # by getting the mean-pooling of the sequence \n",
    "    return tf.math.reduce_mean(sequence_embedding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replacing half the leaf nodes with parent node for evaluating the expansion, used as \"ground-truth\"\n",
    "leaf_nodes = df['l3_encoded'].unique()\n",
    "percent_to_remove = 0.3333\n",
    "leaves_to_remove = set(np.random.choice(leaf_nodes, int(len(leaf_nodes)*percent_to_remove), replace=False))\n",
    "replaced_document_labels = np.copy(documents_labels)\n",
    "\n",
    "for i, l3_topic in enumerate(replaced_document_labels):\n",
    "    if l3_topic not in leaves_to_remove:\n",
    "        continue\n",
    "    parent_l2_topic = df[df['l3_encoded'] == l3_topic].iloc[0]['l2_encoded']\n",
    "    replaced_document_labels[i] = parent_l2_topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "133 in leaves_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([204,   6, 167,   0, 129, 223, 160, 144,  59, 188])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replaced_document_labels[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import int64\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from spektral.layers import GCNConv, GlobalAvgPool\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "import keras_nlp\n",
    "\n",
    "learning_rate = 5e-5  # Learning rate\n",
    "epochs = 2  # Number of training epochs\n",
    "batch_size = 32  # Batch size\n",
    "weight_decay = 5e-6\n",
    "mini_batch_size = 2 # a mini-batch will always have 1 positive triple, and (n-1) negative triples\n",
    "                    # i.e. with a mini_batch_size of 4, we have 1 pos. doc. and 3 neg. docs.\n",
    "batch_ratio = int(batch_size / mini_batch_size)\n",
    "\n",
    "# empty phrase for training with negative documents\n",
    "# negative_document_phrase = tokenizer.encode_plus('', add_special_tokens=True, max_length=max_len, padding='max_length')['input_ids']\n",
    "\n",
    "max_len = 512\n",
    "vocab_size = tokenizer.vocab_size\n",
    "infoNCE_temprature = 0.1\n",
    "\n",
    "optimizer = Adam(learning_rate, weight_decay=weight_decay)\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn_binary = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "loss_fn_crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "def infoNCE(labels, logits, infoNCE_temprature, mini_batch_size: int):\n",
    "    with_temprature = tf.reshape(logits / infoNCE_temprature, shape=(mini_batch_size, -1))\n",
    "    reshaped_labels = tf.reshape(labels, shape=(mini_batch_size, -1))\n",
    "    \n",
    "    softmaxed = tf.nn.softmax(with_temprature)\n",
    "    with_log = -tf.math.log(softmaxed)\n",
    "    multiplied = tf.math.multiply(reshaped_labels, with_log)\n",
    "    \n",
    "    return tf.reduce_sum(multiplied)\n",
    "    \n",
    "n_out = dataset.n_labels\n",
    "topic_embedding_dimension = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_sample = tf.convert_to_tensor(np.array([1, 0, 0 ,0, 0, 0, 1, 0, 0, 0, 0, 1]), dtype=tf.float64)\n",
    "logits_sample = tf.convert_to_tensor(np.array([1.5, 0.4, 1 ,0.3, 0.4, 0.7, 0.6, 0.8, 0.2, 0.1, 0.9, 0.5]))\n",
    "infoNCE(labels_sample, logits_sample, 0.1, mini_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Custom training loop\n",
    "class ModelWithNCE(Model):\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(self, data):\n",
    "        inputs, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            similarity_prediction, phrase_prediction = self(inputs, training=True)\n",
    "            # similarity_prediction_infonce = tf.reshape(similarity_prediction / infoNCE_temprature, shape=(mini_batch_size, -1))\n",
    "\n",
    "            # infoNCE_loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=tf.reshape(target[0], shape=(mini_batch_size, -1)), logits=similarity_prediction_infonce))\n",
    "            # infoNCE_loss = infoNCE(tf.cast(target[0], tf.float32), similarity_prediction, infoNCE_temprature, mini_batch_size)\n",
    "            # infoNCE_loss = loss_fn_crossentropy(tf.reshape(target[0], shape=(mini_batch_size, -1)), similarity_prediction_infonce)\n",
    "            bce_loss = loss_fn_binary(target[0], similarity_prediction)\n",
    "            \n",
    "            # TODO: calculate phrase loss only for POSITIVE documents (i.e. ignore negative documents)\n",
    "            # removing negative documents from phrase loss\n",
    "            phrase_mask = tf.cast(tf.reshape(target[0], shape=(-1, )), dtype=tf.bool)\n",
    "            phrase_loss = loss_fn(tf.boolean_mask(target[1], phrase_mask), tf.boolean_mask(phrase_prediction, phrase_mask))\n",
    "\n",
    "            # tf.print(phrase_mask[:4], bce_loss, tf.boolean_mask(phrase_prediction, phrase_mask).shape, tf.boolean_mask(target[1], phrase_mask).shape, output_stream=sys.stderr)\n",
    "        gradients = tape.gradient([bce_loss, phrase_loss], self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state((target[0], tf.boolean_mask(target[1], phrase_mask)), (similarity_prediction, tf.boolean_mask(phrase_prediction, phrase_mask)))\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import TFEncoderDecoderModel as EncoderDecoderModel\n",
    "\n",
    "EMBEDDING_SIZE=384\n",
    "\n",
    "shared_bilinear = Bilinear(topic_embedding_dimension, EMBEDDING_SIZE, 1)\n",
    "\n",
    "# GNNs (Topic Encoder)\n",
    "X_in = Input(shape=(dataset.n_node_features))\n",
    "A_in = Input(shape=(None,), sparse=True)\n",
    "I_in = Input(shape=(), dtype=int64)\n",
    "\n",
    "topic_embedding = GCNConv(topic_embedding_dimension, activation='relu')([X_in, A_in])\n",
    "topic_embedding = GCNConv(topic_embedding_dimension, activation='relu')([topic_embedding, A_in])\n",
    "topic_embedding = GlobalAvgPool(name='topic_embedding')([topic_embedding, I_in])\n",
    "\n",
    "# BERT Embedding (Document Encoder)\n",
    "max_seq_length = max_len\n",
    "bert2bert = EncoderDecoderModel.from_encoder_decoder_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "embedding = bert2bert.encoder(input_ids)[0]\n",
    "\n",
    "# Transformer Decoders (Phrase Generator)\n",
    "# try a pre-trained decoder\n",
    "decoder_tokens_input = Input(shape=(max_len,), dtype=tf.int32, name=\"decoder_phrase_input\")\n",
    "# decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(vocabulary_size=vocab_size, sequence_length=max_len, embedding_dim=EMBEDDING_SIZE, mask_zero=True)(decoder_tokens_input)\n",
    "\n",
    "# Getting context embedding for decoder\n",
    "topic_attentive_embedding = TopicAttentiveEmbedding()(topic_embedding, embedding, shared_bilinear, training=False)\n",
    "topic_attentive_embedding = tf.keras.layers.Reshape((max_len, 1))(topic_attentive_embedding)\n",
    "context_embedding = ContextEmbedding()([topic_attentive_embedding, embedding])\n",
    "\n",
    "# transformer_decoder = keras_nlp.layers.TransformerDecoder(\n",
    "#     num_heads=16, \n",
    "#     intermediate_dim=max_len,\n",
    "#     dropout=0.2\n",
    "# )(decoder_embedding, context_embedding)\n",
    "\n",
    "out2 = bert2bert.decoder(decoder_tokens_input, encoder_hidden_states=context_embedding)[0]\n",
    "\n",
    "# Transformer Output\n",
    "# out2 = Dense(vocab_size)(transformer_decoder)\n",
    "\n",
    "# Output Bilinear layer (Similarity Predictor)\n",
    "document_embedding = Lambda(sequence_to_document_embedding, name='document_embedding')(embedding)\n",
    "out = shared_bilinear([topic_embedding, document_embedding])\n",
    "\n",
    "# Outputs\n",
    "model = ModelWithNCE(inputs=[X_in, A_in, I_in, input_ids, decoder_tokens_input], outputs=[out, out2])\n",
    "\n",
    "# compiling model and adding metrics\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy', perplexity], run_eagerly=True)\n",
    "\n",
    "# TODO: Remove half of the leaf nodes, and replace the elements in documents_labels that have a removed leaf node with the parent node BEFORE passing to the Generator\n",
    "topic_expan_generator = TopicExpanTrainGen(graph_list, documents[:-20000], replaced_document_labels[:-20000], terms[:-20000], batch_size, mini_batch_size, encoded_topic_to_tokenized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(topic_expan_generator.__getitem__(0)[1][1][28], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "# NOTE: Ignore warning about gradients not existing for BERT's dense layer since \n",
    "# the dense layers are not used and are thus unconnected and do not need training\n",
    "\n",
    "checkpoint_filepath = './checkpoint.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='decoder_perplexity',\n",
    "    save_weights_only=True,\n",
    "    save_freq=1000,\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(topic_expan_generator, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[TqdmCallback(verbose=1), model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.array([0.0, 1.0, 1.0, 0], dtype=bool)\n",
    "pred = tf.convert_to_tensor(np.ones(shape=(4,512,3)))\n",
    "target = tf.convert_to_tensor(np.ones(shape=(4,512)))\n",
    "\n",
    "tf.boolean_mask(pred, mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = TopicExpanTrainGen(graph_list, documents[-1000:], replaced_document_labels[-1000:], batch_size, 1, encoded_topic_to_tokenized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 10s 296ms/step - loss: nan - bilinear_loss: nan - dense_loss: 0.6924 - bilinear_accuracy: 0.9844 - bilinear_perplexity: nan - dense_accuracy: 0.8218 - dense_perplexity: 1.9985\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " nan,\n",
       " 0.6924110651016235,\n",
       " 0.984375,\n",
       " nan,\n",
       " 0.8218246698379517,\n",
       " 1.9985281229019165]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ZZ_VIRTUAL'], dtype=object)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelEncoder.inverse_transform([298])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "text          The World Table Tennis Championships have been...\n",
       "l1                                                        Event\n",
       "l2                                                SocietalEvent\n",
       "l3                                                   Convention\n",
       "l1_encoded                                                  100\n",
       "l2_encoded                                                  251\n",
       "l3_encoded                                                   75\n",
       "Name: 238230, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[238230]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(documents[77784])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.iloc[77784]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_gen = TopicExpanTrainGen(graph_list, documents[-1000:], documents_labels[-1000:], 1, 1, encoded_topic_to_tokenized_dict)\n",
    "test_gen = TopicExpanVirtualPhraseGen(virtual_ego_graph, filtered_docs)\n",
    "x1, x2, x3, x4, x5 = test_gen.__getitem__(0)\n",
    "# sequence = x5.numpy()\n",
    "# sequence[:, 1:] = 0\n",
    "# sequence[:, 1] = 2447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequence = tf.pad(x5, [[0, 0], [0, max_len-tf.shape(x5)[1]]])\n",
    "pred = model((x1, x2, x3, x4, padded_sequence))[1]\n",
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 512), dtype=float64, numpy=\n",
       "array([[  101., 14641.,  2080., ...,     0.,     0.,     0.],\n",
       "       [  101.,  1996.,  4936., ...,     0.,     0.,     0.],\n",
       "       [  101.,  5754., 19190., ...,     0.,     0.,     0.],\n",
       "       ...,\n",
       "       [  101.,  1996.,  7326., ...,     0.,     0.,     0.],\n",
       "       [  101.,  4894.,  2041., ...,     0.,     0.,     0.],\n",
       "       [  101.,  1996.,  2541., ...,     0.,     0.,     0.]])>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32,), dtype=int64, numpy=\n",
       "array([3586, 3586, 3586, 3586, 3586, 3586, 3586, 3586, 3586, 3586, 3586,\n",
       "       3586, 3586, 3586, 3586, 3586, 3586, 3586, 3586, 3586, 3586, 3586,\n",
       "       3586, 3586, 3586, 3586, 3586, 3586, 3586, 3586, 3586, 3586])>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argmax(pred[:, 0, :], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(512, 30522), dtype=float32, numpy=\n",
       "array([[ 2.6153495e+00, -9.5019937e-02,  1.4651502e-02, ...,\n",
       "        -3.5517168e-01, -3.5281119e-01, -5.6236595e-01],\n",
       "       [ 3.6694511e+01, -4.5734015e+00, -4.6839118e+00, ...,\n",
       "        -4.4302564e+00, -4.5606627e+00, -4.7432141e+00],\n",
       "       [ 3.1848719e+01, -4.4285512e+00, -4.5817451e+00, ...,\n",
       "        -4.3376241e+00, -4.3683510e+00, -4.4911666e+00],\n",
       "       ...,\n",
       "       [ 3.5318607e+01, -5.3323512e+00, -5.3753605e+00, ...,\n",
       "        -5.1716352e+00, -5.2236719e+00, -5.3112555e+00],\n",
       "       [ 3.5602047e+01, -5.2717361e+00, -5.3498573e+00, ...,\n",
       "        -5.1118798e+00, -5.2239351e+00, -5.2790284e+00],\n",
       "       [ 3.4962620e+01, -4.9379449e+00, -4.9965191e+00, ...,\n",
       "        -4.8068523e+00, -4.8717856e+00, -5.0343022e+00]], dtype=float32)>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] the 1915 grand national was the 77th renewal of the world - famous grand national horse race that took place at aintree near liverpool, england, on 26 march 1915. [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]'"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(x4[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.TopicExpanExpansionGen import TopicExpanExpansionGen\n",
    "x = np.arange(299)\n",
    "x = labelEncoder.inverse_transform(x)\n",
    "feature_array = x.reshape(299, -1)\n",
    "\n",
    "model.load_weights('checkpoint.h5')\n",
    "\n",
    "graph_dict[100][212][298] = 1 # adding virtual node\n",
    "l1_topic = 100\n",
    "l2_topic = 212\n",
    "virtual_node = 298\n",
    "\n",
    "virtual_ego_graph = create_ego_graph(l1_topic, l2_topic, virtual_node, graph_dict, feature_array, is_virtual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer 'topic_attentive_embedding_2' (type TopicAttentiveEmbedding).\n\nDimensions 32 and 14 are not compatible\n\nCall arguments received by layer 'topic_attentive_embedding_2' (type TopicAttentiveEmbedding):\n  • topic_embedding=tf.Tensor(shape=(32, 300), dtype=float32)\n  • sequence_embedding=tf.Tensor(shape=(14, 512, 384), dtype=float32)\n  • shared_bilinear_layer=<layers.Bilinear.Bilinear object at 0x7f92c6aa0b80>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m outputs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_expansion_input, doc_ids \u001b[38;5;129;01min\u001b[39;00m expan_gen:\n\u001b[0;32m---> 10\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_expansion_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, sim_pred \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_output[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m     13\u001b[0m         outputs[doc_ids[i]] \u001b[38;5;241m=\u001b[39m (math\u001b[38;5;241m.\u001b[39me\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msim_pred)\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/topic_taxonomy_generation/ESG-Topic-Taxonomy-Generation-from-ESG-Reports/notebooks_testing/layers/TopicAttentiveEmbedding.py:23\u001b[0m, in \u001b[0;36mTopicAttentiveEmbedding.call\u001b[0;34m(self, topic_embedding, sequence_embedding, shared_bilinear_layer)\u001b[0m\n\u001b[1;32m     20\u001b[0m     softmax_bilinear_embedding \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39msoftmax(bilinear_embedding)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m softmax_bilinear_embedding\n\u001b[0;32m---> 23\u001b[0m beta_embedding \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mapply_bilinear\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43melems\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreshaped_topic_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msequence_embedding\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfn_output_signature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensorSpec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msequence_embedding\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m beta_embedding\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling layer 'topic_attentive_embedding_2' (type TopicAttentiveEmbedding).\n\nDimensions 32 and 14 are not compatible\n\nCall arguments received by layer 'topic_attentive_embedding_2' (type TopicAttentiveEmbedding):\n  • topic_embedding=tf.Tensor(shape=(32, 300), dtype=float32)\n  • sequence_embedding=tf.Tensor(shape=(14, 512, 384), dtype=float32)\n  • shared_bilinear_layer=<layers.Bilinear.Bilinear object at 0x7f92c6aa0b80>"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import math\n",
    "\n",
    "test_batch_size = 32\n",
    "expan_gen = TopicExpanExpansionGen(virtual_ego_graph, documents, test_batch_size)\n",
    "\n",
    "# outputs = model.predict_generator(expan_gen)\n",
    "outputs = {}\n",
    "for model_expansion_input, doc_ids in expan_gen:\n",
    "    batch_output = model(model_expansion_input)\n",
    "\n",
    "    for i, sim_pred in enumerate(batch_output[0]):\n",
    "        outputs[doc_ids[i]] = (math.e**sim_pred)\n",
    "            \n",
    "    # resetting model to clear memory, model runs OOM without resetting\n",
    "    tf.keras.backend.clear_session()\n",
    "    _ = gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('expansion_out.pickle', 'wb') as handle:\n",
    "    pickle.dump(outputs, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('expansion_out.pickle', 'rb') as handle:\n",
    "    outputs = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+(math.e**-x))\n",
    "\n",
    "def normalize(x):\n",
    "    return (x-min(x))/(max(x)-min(x))\n",
    "\n",
    "norm_outputs = normalize(np.concatenate([outputs[i].numpy() for i in outputs]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from spektral.data.utils import to_disjoint\n",
    "from spektral.data import Graph\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "class TopicExpanVirtualPhraseGen(tf.keras.utils.Sequence):\n",
    "    # Generates batches for phrase generation predictions for a specific virtual node\n",
    "    # Used for expansion\n",
    "    def __init__(self, virtual_graph: Graph, documents: np.array):\n",
    "        self._virtual_graph = virtual_graph # ego-graph of the new virtual node\n",
    "        self.documents = documents\n",
    "        self.batch_size = len(documents)\n",
    "        \n",
    "        # generating initial prompt with the start ID, this is the same prompt\n",
    "        # for all batches since the initial prompt is always [101]\n",
    "        self.initial_prompt = tf.fill((self.batch_size, 1), 101)\n",
    "        # self.initial_prompt = tf.pad(unpadded_prompt, [[0, 0], [0, max_len-tf.shape(unpadded_prompt)[1]]])\n",
    "        \n",
    "        # the same virtual node ego-graph is used for the whole batch,\n",
    "        # so generating it in the init to speed up computation\n",
    "        self.x_in, self.a_in, self.i_in = to_disjoint(\n",
    "            x_list=[self._virtual_graph.x for _ in range(self.batch_size)],\n",
    "            a_list=[self._virtual_graph.a for _ in range(self.batch_size)]\n",
    "        )\n",
    "        \n",
    "        self.x_in = tf.convert_to_tensor(self.x_in)\n",
    "        self.a_in = tf.sparse.SparseTensor(\n",
    "            indices=np.array([self.a_in.row, self.a_in.col]).T,\n",
    "            values=self.a_in.data,\n",
    "            dense_shape=self.a_in.shape\n",
    "        )\n",
    "        self.i_in = tf.convert_to_tensor(self.i_in)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return 1\n",
    "    \n",
    "    def __getitem__(self, non_batch_index: int):\n",
    "        idx = non_batch_index * self.batch_size\n",
    "        idx_limiter = min(idx + self.batch_size, len(self.documents)) # for limiting batch size at end of list\n",
    "        fixed_batch_size = idx_limiter - idx # gets the fixed batch size in case this is the last batch\n",
    "        \n",
    "        model_expansion_inputs = (\n",
    "            self.x_in,\n",
    "            self.a_in,\n",
    "            self.i_in,\n",
    "            tf.convert_to_tensor(self.documents[idx:idx_limiter]),\n",
    "            self.initial_prompt\n",
    "        )\n",
    "        \n",
    "        doc_ids = range(idx, idx_limiter)\n",
    "            \n",
    "        return model_expansion_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  101.,  2520.,  3656., ...,     0.,     0.,     0.],\n",
       "       [  101.,  7212.,  2003., ...,     0.,     0.,     0.],\n",
       "       [  101., 14255.,  2099., ...,     0.,     0.,     0.],\n",
       "       ...,\n",
       "       [  101.,  1996.,  3731., ...,     0.,     0.,     0.],\n",
       "       [  101.,  1996., 22440., ...,     0.,     0.,     0.],\n",
       "       [  101., 28722., 12403., ...,     0.,     0.,     0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[np.argwhere(norm_outputs < 0.7)].reshape(-1, max_len)[0:64]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] racehorse [SEP] [PAD] [PAD]',\n",
       " '[CLS] racehorse [SEP] [PAD] [PAD]',\n",
       " '[CLS] racehorse [SEP] [PAD] [PAD]',\n",
       " '[CLS] racehorse [SEP] [PAD] [PAD]',\n",
       " '[CLS] racehorse [SEP] [PAD] [PAD]',\n",
       " '[CLS] racehorse [SEP] [PAD] [PAD]',\n",
       " '[CLS] racehorse [SEP] [PAD] [PAD]',\n",
       " '[CLS] thoroughbred [SEP] [PAD] [PAD]',\n",
       " '[CLS] racehorse [SEP] [PAD] [PAD]']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_nlp.utils import beam_search\n",
    "from random import randint\n",
    "\n",
    "filtered_docs = documents[np.argwhere(norm_outputs > 0.8)].reshape(-1, max_len)[0:32]\n",
    "test_batch_size = len(filtered_docs)\n",
    "test_gen = TopicExpanVirtualPhraseGen(virtual_ego_graph, filtered_docs)\n",
    "x1, x2, x3, x4, x5 = test_gen[0]\n",
    "# print(x5)\n",
    "START_ID = 101\n",
    "END_ID = 102\n",
    "\n",
    "def token_probability_fn(inputs):\n",
    "    padded_inputs = tf.pad(inputs, [[0, 0], [0, max_len-tf.shape(inputs)[1]]])\n",
    "    \n",
    "    # print(inputs)\n",
    "    repeats = int(padded_inputs.shape[0] / test_batch_size)\n",
    "\n",
    "    preds = [\n",
    "        model((\n",
    "            x1,\n",
    "            x2,\n",
    "            x3,\n",
    "            x4,\n",
    "            padded_inputs[repeat_idx*test_batch_size:(repeat_idx+1)*test_batch_size]\n",
    "        )) for repeat_idx in range(repeats)]\n",
    "    \n",
    "    # print([pred[0] for pred in preds])\n",
    "    \n",
    "    concatenated_preds = tf.concat([pred[1] for pred in preds], axis=0)\n",
    "    # the first zero index is the position in the sequence we're trying to find to add to the sequence\n",
    "    first_zero_index = (padded_inputs.numpy()[0]==0).argmax(axis=0)\n",
    "    # print(first_zero_index)\n",
    "    # print(tf.argmax(concatenated_preds[:, first_zero_index, :], axis=1))\n",
    "    \n",
    "    return concatenated_preds[:, first_zero_index-1, :]\n",
    "\n",
    "# prompt = tf.fill((test_batch_size, 1), START_ID)\n",
    "\n",
    "predicted_phrases = keras_nlp.utils.beam_search(\n",
    "    token_probability_fn,\n",
    "    x5,\n",
    "    max_length=5,\n",
    "    num_beams=2,\n",
    "    # p=0.92,\n",
    "    end_token_id=END_ID,\n",
    "    from_logits=True\n",
    ")\n",
    "[tokenizer.decode(phrase) for phrase in predicted_phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"it's anybodys race as they run into the final furlong, and it's rubstic on the nearside with the advantage over zongalero and rough and tumble as they race up towards the line, it's gonna be a victory for scotland, it's rubstic from zongalero in the national, and as they come to the line, rubstic is the winner! “ ” commentator peter o'sullevan describes the climax of the 1979 national the 1979 grand national was the 133rd renewal of the world - famous grand national horse race that took place at aintree near liverpool, england, on 31 march 1979. the race was won by rubstic who was the first ever scottish - trained winner. the favourite alverton died in the race, a month after winning the cheltenham gold cup. another fatality was kintai, who had to be put down later on.\", \"the 1848 grand national steeplechase was the tenth official annual running of a handicap steeplechase horse race at aintree racecourse near liverpool on wednesday, 1 march. it attracted a then record, field of 29 competitors for a prize valued at £1, 015 to the winner. the race was won by lieutenant josey little on captain william peel's chandler trained by tom eskrett. lieutenant little wore captain peel's colours of white silks with a black cap. the horse won in a time of 11 minutes and 21 seconds, forty - two seconds slower than the course record set the previous year. with the proceeds of the race lieutenant little was able to purchase his promotion to the rank of captain in the 1st king's dragoon guards. tom olliver rode in the race for a record tenth time, finishing second on the curate. the race was marred by the fatal falls of three competitors at the same fence in the latter stages of the second circuit, taking the total number of fatalities in the history of the race to five.\", 'the 2015 kentucky derby ( in full, the kentucky derby presented by yum! brands, due to sponsorship ) was the 141st running of the kentucky derby. the race was run at 6 : 44 pm eastern daylight time ( edt ) on may 2, 2015, at churchill downs. it was broadcast in the united states on the nbc television network. kentucky native ashley judd voiced the opening for the telecast of the race, and was the first woman to do so. the weather was warm, and a record 170, 513 people attended. the 2015 race also set a wagering record with parimutuel betting of $ 137. 9 million. jockey victor espinoza rode american pharoah to victory after taking the lead in the homestretch. the race marked the fourth kentucky derby win for horse trainer bob baffert and the third win for espinoza.', 'the 1905 kentucky derby was the 31st running of the kentucky derby. the race took place on may 10, 1905. the field was reduced to only three competitors when dr. leggo and mcclellan scratched.', 'the 2016 kentucky derby ( in full, the kentucky derby presented by yum! brands, due to sponsorship ) was the 142nd running of the kentucky derby. the race was run at 6 : 51 pm eastern daylight time ( edt ) on may 7, 2016 at churchill downs. the race was broadcast in the united states on the nbc television network. the second largest attendance of 167, 227 was on hand for the event.', \"the fulke walwyn kim muir challenge cup is a national hunt chase in great britain for amateur riders which is open to horses aged five years or older. it is run on the new course at cheltenham over a distance of about 3 miles and 1½ furlongs ( 5, 130 metres ), and during its running there are nineteen fences to be jumped. it is a handicap race, and it is scheduled to take place each year during the cheltenham festival in march. the event was established in 1946, and it was originally called the kim muir amateur riders'steeplechase. it was introduced by mrs evan williams, and it was named in memory of her brother kim muir, a cavalry officer who lost his life during world war ii. the name of fulke walwyn was added to the title in 1991. this was in honour of the highly successful trainer, whose 211 victories at cheltenham included 40 at the festival.\", 'the 1995 grand national ( known as the martell grand national for sponsorship reasons ) was the 148th official renewal of the famous grand national steeplechase that took place at aintree near liverpool, england, on 8 april 1995. the race was won in a time of nine minutes and 4. 1 seconds and by a distance of seven lengths by royal athlete, at 40 / 1, ridden by irish rider jason titley. the winner was a second victory in the race for trainer jenny pitman of lambourn, berkshire, and ran in the colours of gary and libby johnson. pitman collected £118, 854 of a total prize fund shared through the first five finishers of £200, 000. a maximum of 40 competitors was permitted but only 35 ran. all of the horses that took part returned safely.', 'the 2014 kentucky derby ( in full, the kentucky derby presented by yum! brands, due to sponsorship ) was the 140th running of the kentucky derby. the race was scheduled to startat 6 : 24 pm eastern daylight time ( edt ) on may 3, 2014 at churchill downs and was run as the eleventh race on a racecard with thirteen races. the race was broadcast in the united states on the nbc television network. the attendance for the race was 164, 906, the second - largest after the 2012 race with 165, 307 spectators. the winner was california chrome.', 'the interborough stakes, previously interborough handicap is an american thoroughbred horse race held annually at the beginning of january at aqueduct racetrack in ozone park, queens, new york. a non - graded stakes race open to fillies & mares age three and older, it is contested on dirt over a distance of six furlongs. from its inaugural race in 1921 through 1955, the interborough was open to both males and females. from 1956 on, it became a filly and mare event. inaugurated in 1921 at the jamaica racetrack in jamaica, queens, it was raced there through 1958 after which it was hosted by the aqueduct track. from 1968 through 1970, belmont park was home to the race. from 1921 through 1924, the interborough handicap was contested at a distance of a mile and a sixteenth. ta wee won this race in 1969 and 1970. affectionately won it in 1963 and 1964. in 2013, nicole h became the first horse to win this race three times as well as consecutively.']\n"
     ]
    }
   ],
   "source": [
    "print([tokenizer.decode(phrase, skip_special_tokens=True) for phrase in filtered_docs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l3</th>\n",
       "      <th>l1_encoded</th>\n",
       "      <th>l2_encoded</th>\n",
       "      <th>l3_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>Longboat (24 March 1981 – ca. 1997) was a Brit...</td>\n",
       "      <td>Species</td>\n",
       "      <td>Horse</td>\n",
       "      <td>RaceHorse</td>\n",
       "      <td>255</td>\n",
       "      <td>132</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>336</th>\n",
       "      <td>Superstar Leo is an Irish-bred, British-traine...</td>\n",
       "      <td>Species</td>\n",
       "      <td>Horse</td>\n",
       "      <td>RaceHorse</td>\n",
       "      <td>255</td>\n",
       "      <td>132</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>Donau (1907–1913) was an American Thoroughbred...</td>\n",
       "      <td>Species</td>\n",
       "      <td>Horse</td>\n",
       "      <td>RaceHorse</td>\n",
       "      <td>255</td>\n",
       "      <td>132</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>377</th>\n",
       "      <td>Came Home (foaled in 1999) is an American Thor...</td>\n",
       "      <td>Species</td>\n",
       "      <td>Horse</td>\n",
       "      <td>RaceHorse</td>\n",
       "      <td>255</td>\n",
       "      <td>132</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>Kodiak Kowboy (foaled April 16, 2005 in Kentuc...</td>\n",
       "      <td>Species</td>\n",
       "      <td>Horse</td>\n",
       "      <td>RaceHorse</td>\n",
       "      <td>255</td>\n",
       "      <td>132</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240056</th>\n",
       "      <td>So Casual (foaled 25 October 1995) is a Thorou...</td>\n",
       "      <td>Species</td>\n",
       "      <td>Horse</td>\n",
       "      <td>RaceHorse</td>\n",
       "      <td>255</td>\n",
       "      <td>132</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240248</th>\n",
       "      <td>Humorist (1918–1921) was a British Thoroughbre...</td>\n",
       "      <td>Species</td>\n",
       "      <td>Horse</td>\n",
       "      <td>RaceHorse</td>\n",
       "      <td>255</td>\n",
       "      <td>132</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240779</th>\n",
       "      <td>Goldencents (foaled March 7, 2010) is an Ameri...</td>\n",
       "      <td>Species</td>\n",
       "      <td>Horse</td>\n",
       "      <td>RaceHorse</td>\n",
       "      <td>255</td>\n",
       "      <td>132</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240804</th>\n",
       "      <td>Lakeway (foaled February 19, 1991 in Kentucky)...</td>\n",
       "      <td>Species</td>\n",
       "      <td>Horse</td>\n",
       "      <td>RaceHorse</td>\n",
       "      <td>255</td>\n",
       "      <td>132</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240834</th>\n",
       "      <td>Stately Victor (foaled May 1, 2007 in Kentucky...</td>\n",
       "      <td>Species</td>\n",
       "      <td>Horse</td>\n",
       "      <td>RaceHorse</td>\n",
       "      <td>255</td>\n",
       "      <td>132</td>\n",
       "      <td>213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1900 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text       l1     l2  \\\n",
       "232     Longboat (24 March 1981 – ca. 1997) was a Brit...  Species  Horse   \n",
       "336     Superstar Leo is an Irish-bred, British-traine...  Species  Horse   \n",
       "343     Donau (1907–1913) was an American Thoroughbred...  Species  Horse   \n",
       "377     Came Home (foaled in 1999) is an American Thor...  Species  Horse   \n",
       "394     Kodiak Kowboy (foaled April 16, 2005 in Kentuc...  Species  Horse   \n",
       "...                                                   ...      ...    ...   \n",
       "240056  So Casual (foaled 25 October 1995) is a Thorou...  Species  Horse   \n",
       "240248  Humorist (1918–1921) was a British Thoroughbre...  Species  Horse   \n",
       "240779  Goldencents (foaled March 7, 2010) is an Ameri...  Species  Horse   \n",
       "240804  Lakeway (foaled February 19, 1991 in Kentucky)...  Species  Horse   \n",
       "240834  Stately Victor (foaled May 1, 2007 in Kentucky...  Species  Horse   \n",
       "\n",
       "               l3  l1_encoded  l2_encoded  l3_encoded  \n",
       "232     RaceHorse         255         132         213  \n",
       "336     RaceHorse         255         132         213  \n",
       "343     RaceHorse         255         132         213  \n",
       "377     RaceHorse         255         132         213  \n",
       "394     RaceHorse         255         132         213  \n",
       "...           ...         ...         ...         ...  \n",
       "240056  RaceHorse         255         132         213  \n",
       "240248  RaceHorse         255         132         213  \n",
       "240779  RaceHorse         255         132         213  \n",
       "240804  RaceHorse         255         132         213  \n",
       "240834  RaceHorse         255         132         213  \n",
       "\n",
       "[1900 rows x 7 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['l3'] == 'RaceHorse']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 512), dtype=int32, numpy=\n",
       "array([[101,   0,   0, ...,   0,   0,   0],\n",
       "       [101,   0,   0, ...,   0,   0,   0],\n",
       "       [101,   0,   0, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [101,   0,   0, ...,   0,   0,   0],\n",
       "       [101,   0,   0, ...,   0,   0,   0],\n",
       "       [101,   0,   0, ...,   0,   0,   0]], dtype=int32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = tf.fill((batch_size, 1), START_ID)\n",
    "padded_prompt = tf.pad(prompt, [[0, 0], [0, max_len-tf.shape(prompt)[1]]])\n",
    "padded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4,\n",
       " 7,\n",
       " 8,\n",
       " 19,\n",
       " 23,\n",
       " 24,\n",
       " 27,\n",
       " 28,\n",
       " 30,\n",
       " 36,\n",
       " 37,\n",
       " 40,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 53,\n",
       " 60,\n",
       " 67,\n",
       " 79,\n",
       " 80,\n",
       " 83,\n",
       " 101,\n",
       " 102,\n",
       " 113,\n",
       " 116,\n",
       " 118,\n",
       " 121,\n",
       " 126,\n",
       " 127,\n",
       " 130,\n",
       " 131,\n",
       " 134,\n",
       " 141,\n",
       " 142,\n",
       " 147,\n",
       " 150,\n",
       " 154,\n",
       " 158,\n",
       " 162,\n",
       " 163,\n",
       " 172,\n",
       " 173,\n",
       " 177,\n",
       " 178,\n",
       " 182,\n",
       " 190,\n",
       " 194,\n",
       " 198,\n",
       " 200,\n",
       " 201,\n",
       " 203,\n",
       " 205,\n",
       " 218,\n",
       " 219,\n",
       " 222,\n",
       " 226,\n",
       " 227,\n",
       " 228,\n",
       " 230,\n",
       " 233,\n",
       " 239,\n",
       " 242,\n",
       " 244,\n",
       " 245,\n",
       " 246,\n",
       " 253,\n",
       " 266,\n",
       " 273,\n",
       " 274,\n",
       " 275,\n",
       " 292}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leaves_to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model((x1, x2, x3, x4, prompt)), tokenizer.decode(x4[2], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l3</th>\n",
       "      <th>l1_encoded</th>\n",
       "      <th>l2_encoded</th>\n",
       "      <th>l3_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>The Ardee Baroque Festival is a celebration of...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>MusicFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>The Slamdance Film Festival is an annual film ...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>The Stan Rogers Folk Festival, informally know...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>MusicFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>The 5th Toronto International Film Festival (T...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>The 2010 Slamdance Film Festival took place in...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240049</th>\n",
       "      <td>The São Paulo International Film Festival (Por...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240163</th>\n",
       "      <td>The 40th annual Toronto International Film Fes...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240210</th>\n",
       "      <td>During the 19th century Trinidadians and other...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>MusicFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240226</th>\n",
       "      <td>New York Polish Film Festival (abbreviated to ...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240253</th>\n",
       "      <td>The 40th Venice International Film Festival wa...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>881 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     l1  \\\n",
       "214     The Ardee Baroque Festival is a celebration of...  Event   \n",
       "653     The Slamdance Film Festival is an annual film ...  Event   \n",
       "774     The Stan Rogers Folk Festival, informally know...  Event   \n",
       "1350    The 5th Toronto International Film Festival (T...  Event   \n",
       "1886    The 2010 Slamdance Film Festival took place in...  Event   \n",
       "...                                                   ...    ...   \n",
       "240049  The São Paulo International Film Festival (Por...  Event   \n",
       "240163  The 40th annual Toronto International Film Fes...  Event   \n",
       "240210  During the 19th century Trinidadians and other...  Event   \n",
       "240226  New York Polish Film Festival (abbreviated to ...  Event   \n",
       "240253  The 40th Venice International Film Festival wa...  Event   \n",
       "\n",
       "                   l2             l3  l1_encoded  l2_encoded  l3_encoded  \n",
       "214     SocietalEvent  MusicFestival         100         251         171  \n",
       "653     SocietalEvent   FilmFestival         100         251         105  \n",
       "774     SocietalEvent  MusicFestival         100         251         171  \n",
       "1350    SocietalEvent   FilmFestival         100         251         105  \n",
       "1886    SocietalEvent   FilmFestival         100         251         105  \n",
       "...               ...            ...         ...         ...         ...  \n",
       "240049  SocietalEvent   FilmFestival         100         251         105  \n",
       "240163  SocietalEvent   FilmFestival         100         251         105  \n",
       "240210  SocietalEvent  MusicFestival         100         251         171  \n",
       "240226  SocietalEvent   FilmFestival         100         251         105  \n",
       "240253  SocietalEvent   FilmFestival         100         251         105  \n",
       "\n",
       "[881 rows x 7 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['l3'].str.contains('Festival')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ZZ_VIRTUAL'], dtype=object)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_with_virtual = np.append(topics, 'ZZ_VIRTUAL')\n",
    "le = LabelEncoder()\n",
    "le.fit(topics_with_virtual)\n",
    "le.inverse_transform([298])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_ranking'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Input\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_ranking\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspektral\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GCNConv, GlobalAvgPool, GraphMasking\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# n_out = dataset.n_labels\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_ranking'"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "from tensorflow import int64\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import tensorflow_ranking\n",
    "\n",
    "from spektral.layers import GCNConv, GlobalAvgPool, GraphMasking\n",
    "\n",
    "# n_out = dataset.n_labels\n",
    "\n",
    "X_in = Input(shape=(50))\n",
    "A_in = Input(shape=(None,), sparse=True)\n",
    "I_in = Input(shape=(), dtype=int64)\n",
    "\n",
    "X = GCNConv(32, activation='relu')([X_in, A_in])\n",
    "X = GCNConv(32, activation='relu')([X, A_in])\n",
    "X = GlobalAvgPool()([X, I_in])\n",
    "\n",
    "shared_bilinear = tensorflow_ranking.keras.layers.Bilinear(32, 32)\n",
    "X_1 = shared_bilinear([X, X])\n",
    "X = shared_bilinear([X, X], training=False)\n",
    "\n",
    "out = Dense(2, activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs=[X_in, A_in, I_in], outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "preds = tf.constant([[[-11.7803297],\n",
    " [-9.34260654],\n",
    " [-14.0992193],\n",
    " [-9.90242]],[[-11.7803297],\n",
    " [-9.34260654],\n",
    " [-14.0992193],\n",
    " [-9.90242]]], dtype=float)\n",
    "\n",
    "target = tf.constant([[[0],\n",
    " [0],\n",
    " [0],\n",
    " [1]],[[0],\n",
    " [0],\n",
    " [0],\n",
    " [1]]], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoided = tf.keras.activations.sigmoid(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=tf.reshape(target, shape=(2, -1)), logits=tf.reshape(sigmoided, shape=(2, -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.activations.softmax(tf.reshape(preds, shape=(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(target, shape=(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e907c9c662b38b6fe9e69d6881d7a36bc6e4ecfbfed88be2a66ef06ac6f6bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
