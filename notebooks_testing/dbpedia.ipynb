{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading df\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../dataset/DBPEDIA_train.csv')\n",
    "document_list = df['text'].to_numpy().astype('str')\n",
    "document_list.dtype\n",
    "\n",
    "print('Finished reading df')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding topics to create the adjacency matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "topics = np.concatenate((df['l1'].unique(), df['l2'].unique(), df['l3'].unique()))\n",
    "# For virtual node when expanding topic taxonomy\n",
    "topics = np.append(topics, 'ZZ_VIRTUAL')\n",
    "labelEncoder.fit(topics)\n",
    "\n",
    "def encode_topic(topic):\n",
    "    print(type(topic))\n",
    "    return labelEncoder.transform(topic)\n",
    "\n",
    "df['l1_encoded'] = labelEncoder.transform(df['l1'])\n",
    "df['l2_encoded'] = labelEncoder.transform(df['l2'])\n",
    "df['l3_encoded'] = labelEncoder.transform(df['l3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 03:48:13.792694: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 03:48:13.964319: I tensorflow/core/util/port.cc:104] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-04-04 03:48:16.407422: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cudnn-8.2.4.15-11.4-aa4jklovfx75n2q4jcbyc3dgemu5nqpu/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cuda-11.4.2-sg5bamqhm7bmiz3nf43yzo5thibytq7t/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libxml2-2.9.12-i7nsnmeeeukas6sc2rqa6adykxtky4yj/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/xz-5.2.5-wfweai2jkkplgry3aqkfb5fqg44fuxtf/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libiconv-1.16-xa6oobihv7qsbf743iq5g3ioyriyjxwj/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/zlib-1.2.11-un5j7szis72ayqdk4yzjlp6vkjrcbtn7/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/openssl-1.1.1l-fw26x5iyyvh4yqpxd6eq7jdnu2kydwkm/lib\n",
      "2023-04-04 03:48:16.407510: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cudnn-8.2.4.15-11.4-aa4jklovfx75n2q4jcbyc3dgemu5nqpu/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/cuda-11.4.2-sg5bamqhm7bmiz3nf43yzo5thibytq7t/lib64:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libxml2-2.9.12-i7nsnmeeeukas6sc2rqa6adykxtky4yj/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/xz-5.2.5-wfweai2jkkplgry3aqkfb5fqg44fuxtf/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/libiconv-1.16-xa6oobihv7qsbf743iq5g3ioyriyjxwj/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/zlib-1.2.11-un5j7szis72ayqdk4yzjlp6vkjrcbtn7/lib:/software/spackages_prod/apps/linux-ubuntu20.04-zen2/gcc-9.4.0/openssl-1.1.1l-fw26x5iyyvh4yqpxd6eq7jdnu2kydwkm/lib\n",
      "2023-04-04 03:48:16.407516: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "max_len = 512\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "encoded_topic_to_tokenized_dict = {}\n",
    "for topic in topics:\n",
    "    # dbpedia categories are in PascalCase, so this makes them spaced\n",
    "    spaced_words = re.sub( r\"([A-Z])\", r\" \\1\", topic)[1:]\n",
    "    tokenized_sequence = tokenizer.encode_plus(spaced_words, add_special_tokens=True, max_length=max_len, padding='max_length')['input_ids']\n",
    "\n",
    "    encoded_topic_to_tokenized_dict[labelEncoder.transform([topic])[0]] = tokenized_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = df['text'].apply(lambda doc: np.array(tokenizer.encode_plus(doc, add_special_tokens=True, max_length=max_len, padding='max_length', truncation=True)['input_ids'])).to_numpy()\n",
    "# documents_labels = labelEncoder.transform(df['l3'].to_numpy())\n",
    "# documents_fixed = np.empty(shape=(len(documents), max_len))\n",
    "# for i, doc in enumerate(documents):\n",
    "#     documents_fixed[i] = doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# with open(\"tokenized_dbpedia.pkl\", \"wb\") as f:\n",
    "#     pickle.dump([documents_fixed, documents_labels], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished getting tokenized file\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "with (open(\"./tokenized_dbpedia.pkl\", \"rb\")) as f:\n",
    "    documents, documents_labels = pickle.load(f)\n",
    "print('Finished getting tokenized file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    l1 = row['l1_encoded']\n",
    "    l2 = row['l2_encoded']\n",
    "    l3 = row['l3_encoded']\n",
    "\n",
    "    if l1 not in graph_dict:\n",
    "        graph_dict[l1] = {}\n",
    "    if l2 not in graph_dict[l1]:\n",
    "        graph_dict[l1][l2] = {} \n",
    "\n",
    "    graph_dict[l1][l2][l3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating node features\n",
    "x = np.arange(298)\n",
    "x = labelEncoder.inverse_transform(x)\n",
    "feature_array = x.reshape(298, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished getting GlOVE embedder\n"
     ]
    }
   ],
   "source": [
    "# loading GloVe model to get topic word embeddings\n",
    "# from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "import torchtext\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)\n",
    "print('Finished getting GlOVE embedder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['MASK'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import Graph\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def create_ego_graph(l1_topic, l2_topic, l3_topic, graph_dict, feature_array, is_virtual=False):\n",
    "    # Creating \"ego-graphs\" (each node is seperated into a graph with itself, parent, and siblings)\n",
    "    # The base node (so the node itself) will be masked, aka. have a [MASK] embedding\n",
    "    # The sibling nodes need to have a negative relationship with the base node (so negative value in adjacency matrix?)\n",
    "    if l3_topic != None:\n",
    "        siblings_list = list(graph_dict[l1_topic][l2_topic].keys())\n",
    "        siblings_list.remove(l3_topic)\n",
    "        base = l3_topic\n",
    "        parent = l2_topic\n",
    "        grandparent = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        all_nodes_list.append(parent)\n",
    "        all_nodes_list.append(grandparent)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "        encoded_parent = node_label_encoder.transform([parent])[0]\n",
    "        encoded_grandparent = node_label_encoder.transform([grandparent])[0]\n",
    "\n",
    "        adj_matrix[encoded_base][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_base] = 1\n",
    "\n",
    "        adj_matrix[encoded_grandparent][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_grandparent] = 1\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "        \n",
    "    elif l2_topic != None:\n",
    "        siblings_list = list(graph_dict[l1_topic].keys())\n",
    "        siblings_list.remove(l2_topic)\n",
    "        base = l2_topic\n",
    "        parent = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        all_nodes_list.append(parent)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "        encoded_parent = node_label_encoder.transform([parent])[0]\n",
    "\n",
    "        adj_matrix[encoded_base][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_base] = 1\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "    \n",
    "    elif l1_topic != None:\n",
    "        siblings_list = list(graph_dict.keys())\n",
    "        siblings_list.remove(l1_topic)\n",
    "        base = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "\n",
    "    ego_features = np.zeros((n_nodes, 50))\n",
    "    encoded_nodes_list = node_label_encoder.transform(all_nodes_list)\n",
    "\n",
    "    for i, node in enumerate(all_nodes_list):\n",
    "        # Masking base node, setting the embedding to all 0's\n",
    "        if (node == base):\n",
    "            embedding_avg = glove['MASK']\n",
    "        else:\n",
    "            feature = feature_array[node]\n",
    "            split_words_list = re.sub( r\"([A-Z])\", r\" \\1\", feature[0]).split()\n",
    "            n_words = len(split_words_list)\n",
    "            embedding_avg = np.array([glove[word.lower()].numpy() for word in split_words_list]).sum(axis=0)/n_words\n",
    "\n",
    "        ego_features[encoded_nodes_list[i]] = embedding_avg\n",
    "\n",
    "    return Graph(a=adj_matrix, x=ego_features, y=(l1_topic, l2_topic, l3_topic))\n",
    "\n",
    "graph_list = []\n",
    "\n",
    "for l1_topic in graph_dict:\n",
    "    for l2_topic in graph_dict[l1_topic]:\n",
    "        for l3_topic in graph_dict[l1_topic][l2_topic]:\n",
    "            graph_list.append(create_ego_graph(l1_topic, l2_topic, l3_topic, graph_dict, feature_array))\n",
    "        graph_list.append(create_ego_graph(l1_topic, l2_topic, None, graph_dict, feature_array))\n",
    "    graph_list.append(create_ego_graph(l1_topic, None, None, graph_dict, feature_array))\n",
    "\n",
    "graph_list = np.array(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, graph_list: list[Graph], **kwargs):\n",
    "        self.graph_list = graph_list\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        num_l =  np.random.permutation(len(self.graph_list))\n",
    "        return [self.graph_list[i] for i in num_l]\n",
    "    \n",
    "dataset = MyDataset(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.Bilinear import Bilinear\n",
    "from layers.ContextEmbedding import ContextEmbedding\n",
    "from layers.TopicAttentiveEmbedding import TopicAttentiveEmbedding\n",
    "\n",
    "from utils.TopicExpanTrainGen import TopicExpanTrainGen\n",
    "\n",
    "def sequence_to_document_embedding(sequence_embedding: tf.Tensor):\n",
    "    # gets the document representation/embedding from a BERT sequence embedding\n",
    "    # by getting the mean-pooling of the sequence \n",
    "    return tf.math.reduce_mean(sequence_embedding, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 03:48:36.302175: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-04 03:48:37.527261: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38220 MB memory:  -> device: 0, name: NVIDIA A100-SXM4-40GB, pci bus id: 0000:31:00.0, compute capability: 8.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import int64\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input, Lambda\n",
    "from spektral.layers import GCNConv, GlobalAvgPool\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "import keras_nlp\n",
    "\n",
    "learning_rate = 5e-5  # Learning rate\n",
    "epochs = 4  # Number of training epochs\n",
    "batch_size = 32  # Batch size\n",
    "weight_decay = 5e-6\n",
    "mini_batch_size = 4 # a mini-batch will always have 1 positive triple, and (n-1) negative triples\n",
    "                    # i.e. with a mini_batch_size of 4, we have 1 pos. doc. and 3 neg. docs.\n",
    "batch_ratio = int(batch_size / mini_batch_size)\n",
    "\n",
    "max_len = 512\n",
    "vocab_size = tokenizer.vocab_size\n",
    "infoNCE_temprature = 0.1\n",
    "\n",
    "optimizer = Adam(learning_rate, weight_decay=weight_decay)\n",
    "loss_fn = SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_fn_binary = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "loss_fn_crossentropy = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "n_out = dataset.n_labels\n",
    "topic_embedding_dimension = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "# Custom training loop\n",
    "class ModelWithNCE(Model):\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(self, data):\n",
    "        inputs, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            similarity_prediction, phrase_prediction = self(inputs, training=True)\n",
    "            similarity_prediction_infonce = tf.reshape(similarity_prediction / infoNCE_temprature, shape=(mini_batch_size, -1))\n",
    "\n",
    "            # infoNCE_loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=tf.reshape(target[0], shape=(mini_batch_size, -1)), logits=similarity_prediction_infonce))\n",
    "            # infoNCE_loss = loss_fn_crossentropy(tf.reshape(target[0], shape=(mini_batch_size, -1)), similarity_prediction_infonce)\n",
    "            bce_loss = loss_fn_binary(target[0], similarity_prediction)\n",
    "            phrase_loss = loss_fn(target[1], phrase_prediction)\n",
    "\n",
    "            # tf.print(similarity_prediction[:4], bce_loss, phrase_loss, output_stream=sys.stderr)\n",
    "        gradients = tape.gradient([bce_loss, phrase_loss], self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(target, (similarity_prediction, phrase_prediction))\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_expan_generator = TopicExpanTrainGen(graph_list, documents, documents_labels, batch_size, mini_batch_size, encoded_topic_to_tokenized_dict)\n",
    "#, arr = topic_expan_generator.__getitem__(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# arr[1][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 03:48:40.975226: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:630] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n",
      "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /users/k21148846/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    }
   ],
   "source": [
    "shared_bilinear = Bilinear(topic_embedding_dimension, 768, 1)\n",
    "\n",
    "# GNNs (Topic Encoder)\n",
    "X_in = Input(shape=(dataset.n_node_features))\n",
    "A_in = Input(shape=(None,), sparse=True)\n",
    "I_in = Input(shape=(), dtype=int64)\n",
    "\n",
    "topic_embedding = GCNConv(topic_embedding_dimension, activation='relu')([X_in, A_in])\n",
    "topic_embedding = GCNConv(topic_embedding_dimension, activation='relu')([topic_embedding, A_in])\n",
    "topic_embedding = GlobalAvgPool(name='topic_embedding')([topic_embedding, I_in])\n",
    "\n",
    "# BERT Embedding (Document Encoder)\n",
    "max_seq_length = max_len\n",
    "encoder = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "embedding = encoder(input_ids)[0]\n",
    "\n",
    "# Transformer Decoders (Phrase Generator)\n",
    "decoder_tokens_input = Input(shape=(max_len,), name=\"decoder_phrase_input\")\n",
    "decoder_embedding = keras_nlp.layers.TokenAndPositionEmbedding(vocabulary_size=vocab_size, sequence_length=max_len, embedding_dim=768, mask_zero=True)(decoder_tokens_input)\n",
    "\n",
    "# Getting context embedding for decoder\n",
    "topic_attentive_embedding = TopicAttentiveEmbedding()(topic_embedding, embedding, shared_bilinear, training=False)\n",
    "topic_attentive_embedding = tf.keras.layers.Reshape((max_len, 1))(topic_attentive_embedding)\n",
    "context_embedding = ContextEmbedding()([topic_attentive_embedding, embedding])\n",
    "\n",
    "transformer_decoder = keras_nlp.layers.TransformerDecoder(\n",
    "    num_heads=16, \n",
    "    intermediate_dim=max_len,\n",
    "    dropout=0.1\n",
    ")(decoder_embedding, context_embedding)\n",
    "\n",
    "# Transformer Output\n",
    "out2 = Dense(vocab_size)(transformer_decoder)\n",
    "\n",
    "# Output Bilinear layer (Similarity Predictor)\n",
    "document_embedding = Lambda(sequence_to_document_embedding, name='document_embedding')(embedding)\n",
    "out = shared_bilinear([topic_embedding, document_embedding])\n",
    "\n",
    "# Outputs\n",
    "model = ModelWithNCE(inputs=[X_in, A_in, I_in, input_ids, decoder_tokens_input], outputs=[out, out2])\n",
    "\n",
    "# compiling model and adding metrics\n",
    "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy', perplexity], run_eagerly=True)\n",
    "\n",
    "topic_expan_generator = TopicExpanTrainGen(graph_list, documents[:-20000], documents_labels[:-20000], batch_size, mini_batch_size, encoded_topic_to_tokenized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_with_nce\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " gcn_conv (GCNConv)             (None, 300)          15300       ['input_1[0][0]',                \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " gcn_conv_1 (GCNConv)           (None, 300)          90300       ['gcn_conv[0][0]',               \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " input_ids (InputLayer)         [(None, 512)]        0           []                               \n",
      "                                                                                                  \n",
      " topic_embedding (GlobalAvgPool  (None, 300)         0           ['gcn_conv_1[0][0]',             \n",
      " )                                                                'input_3[0][0]']                \n",
      "                                                                                                  \n",
      " tf_bert_model (TFBertModel)    TFBaseModelOutputWi  109482240   ['input_ids[0][0]']              \n",
      "                                thPoolingAndCrossAt                                               \n",
      "                                tentions(last_hidde                                               \n",
      "                                n_state=(None, 512,                                               \n",
      "                                 768),                                                            \n",
      "                                 pooler_output=(Non                                               \n",
      "                                e, 768),                                                          \n",
      "                                 past_key_values=No                                               \n",
      "                                ne, hidden_states=N                                               \n",
      "                                one, attentions=Non                                               \n",
      "                                e, cross_attentions                                               \n",
      "                                =None)                                                            \n",
      "                                                                                                  \n",
      " topic_attentive_embedding (Top  (None, 512)         0           ['topic_embedding[0][0]',        \n",
      " icAttentiveEmbedding)                                            'tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " decoder_phrase_input (InputLay  [(None, 512)]       0           []                               \n",
      " er)                                                                                              \n",
      "                                                                                                  \n",
      " reshape (Reshape)              (None, 512, 1)       0           ['topic_attentive_embedding[0][0]\n",
      "                                                                 ']                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding (  (None, 512, 768)    23834112    ['decoder_phrase_input[0][0]']   \n",
      " TokenAndPositionEmbedding)                                                                       \n",
      "                                                                                                  \n",
      " context_embedding (ContextEmbe  (None, 512, 768)    0           ['reshape[0][0]',                \n",
      " dding)                                                           'tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " document_embedding (Lambda)    (None, 768)          0           ['tf_bert_model[0][0]']          \n",
      "                                                                                                  \n",
      " transformer_decoder (Transform  (None, 512, 768)    5517056     ['token_and_position_embedding[0]\n",
      " erDecoder)                                                      [0]',                            \n",
      "                                                                  'context_embedding[0][0]']      \n",
      "                                                                                                  \n",
      " bilinear (Bilinear)            (None, 1)            230401      ['topic_embedding[0][0]',        \n",
      "                                                                  'document_embedding[0][0]']     \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 512, 30522)   23471418    ['transformer_decoder[0][0]']    \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 162,640,827\n",
      "Trainable params: 162,640,827\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.keras import TqdmCallback\n",
    "\n",
    "# NOTE: Ignore warning about gradients not existing for BERT's dense layer since \n",
    "# the dense layers are not used and are thus unconnected and do not need training\n",
    "\n",
    "checkpoint_filepath = './checkpoint.h5'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='dense_perplexity',\n",
    "    save_weights_only=True,\n",
    "    save_freq=1000,\n",
    "    mode='min',\n",
    "    save_best_only=True)\n",
    "\n",
    "model.fit(topic_expan_generator, batch_size=batch_size, epochs=epochs, verbose=1, callbacks=[TqdmCallback(verbose=1), model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('checkpoint_april1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = TopicExpanTrainGen(graph_list, documents[-1000:], documents_labels[-1000:], batch_size, 2, encoded_topic_to_tokenized_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 18s 290ms/step - loss: nan - bilinear_1_loss: nan - dense_loss: 0.1204 - bilinear_1_accuracy: 0.9876 - bilinear_1_perplexity: nan - dense_accuracy: 0.9820 - dense_perplexity: 1.1279\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[nan,\n",
       " nan,\n",
       " 0.12036581337451935,\n",
       " 0.9875991940498352,\n",
       " nan,\n",
       " 0.9819642901420593,\n",
       " 1.1279094219207764]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from spektral.data.utils import to_disjoint\n",
    "import tensorflow as tf\n",
    "\n",
    "class TopicExpanExpansionGen(tf.keras.utils.Sequence):\n",
    "    def __init__(self, virtual_graph: Graph, documents: np.array, batch_size: int):\n",
    "        self._virtual_graph = virtual_graph # ego-graph of the new virtual node\n",
    "        self.documents = documents\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        # generating initial prompt with the start ID, this is the same prompt\n",
    "        # for all batches since the initial prompt is always [101]\n",
    "        unpadded_prompt = tf.fill((self.batch_size, 1), 101)\n",
    "        self.initial_prompt = tf.pad(unpadded_prompt, [[0, 0], [0, max_len-tf.shape(unpadded_prompt)[1]]])\n",
    "        \n",
    "        # the same virtual node ego-graph is used for the whole batch,\n",
    "        # so generating it in the init to speed up computation\n",
    "        self.x_in, self.a_in, self.i_in = to_disjoint(\n",
    "            x_list=[self._virtual_graph.x for _ in range(batch_size)],\n",
    "            a_list=[self._virtual_graph.a for _ in range(batch_size)]\n",
    "        )\n",
    "        \n",
    "        self.x_in = tf.convert_to_tensor(self.x_in)\n",
    "        self.a_in = tf.sparse.SparseTensor(\n",
    "            indices=np.array([self.a_in.row, self.a_in.col]).T,\n",
    "            values=self.a_in.data,\n",
    "            dense_shape=self.a_in.shape\n",
    "        )\n",
    "        self.i_in = tf.convert_to_tensor(self.i_in)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return math.ceil(len(self.documents) / (self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, non_batch_index: int):\n",
    "        idx = non_batch_index * self.batch_size\n",
    "        idx_limiter = min(idx + self.batch_size, len(self.documents)) # for limiting batch size at end of list\n",
    "        fixed_batch_size = idx_limiter - idx # gets the fixed batch size in case this is the last batch\n",
    "        \n",
    "        model_expansion_inputs = (\n",
    "            self.x_in,\n",
    "            self.a_in,\n",
    "            self.i_in,\n",
    "            tf.convert_to_tensor(self.documents[idx:idx_limiter]),\n",
    "            self.initial_prompt\n",
    "        )\n",
    "            \n",
    "        return model_expansion_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(299)\n",
    "x = labelEncoder.inverse_transform(x)\n",
    "feature_array = x.reshape(299, -1)\n",
    "\n",
    "graph_dict[100][212][298] = 1 # adding virtual node\n",
    "l1_topic = 100\n",
    "l2_topic = 212\n",
    "virtual_node = 298\n",
    "\n",
    "virtual_ego_graph = create_ego_graph(l1_topic, l2_topic, virtual_node, graph_dict, feature_array, is_virtual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n",
      "found positive prediction!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-04 03:49:02.305344: W tensorflow/tsl/framework/bfc_allocator.cc:479] Allocator (GPU_0_bfc) ran out of memory trying to allocate 192.00MiB (rounded to 201326592)requested by op Softmax\n",
      "If the cause is memory fragmentation maybe the environment variable 'TF_GPU_ALLOCATOR=cuda_malloc_async' will improve the situation. \n",
      "Current allocation summary follows.\n",
      "Current allocation summary follows.\n",
      "2023-04-04 03:49:02.305376: I tensorflow/tsl/framework/bfc_allocator.cc:1034] BFCAllocator dump for GPU_0_bfc\n",
      "2023-04-04 03:49:02.305386: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (256): \tTotal Chunks: 79, Chunks in use: 78. 19.8KiB allocated for chunks. 19.5KiB in use in bin. 2.5KiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305392: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (512): \tTotal Chunks: 1, Chunks in use: 1. 768B allocated for chunks. 768B in use in bin. 640B client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305399: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1024): \tTotal Chunks: 3, Chunks in use: 3. 3.8KiB allocated for chunks. 3.8KiB in use in bin. 3.3KiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305405: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2048): \tTotal Chunks: 125, Chunks in use: 125. 374.2KiB allocated for chunks. 374.2KiB in use in bin. 374.1KiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305410: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4096): \tTotal Chunks: 5, Chunks in use: 5. 27.5KiB allocated for chunks. 27.5KiB in use in bin. 21.2KiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305416: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8192): \tTotal Chunks: 50, Chunks in use: 50. 464.8KiB allocated for chunks. 464.8KiB in use in bin. 448.0KiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305421: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16384): \tTotal Chunks: 2, Chunks in use: 1. 55.0KiB allocated for chunks. 31.2KiB in use in bin. 31.2KiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305427: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (32768): \tTotal Chunks: 6, Chunks in use: 6. 252.5KiB allocated for chunks. 252.5KiB in use in bin. 218.6KiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305433: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (65536): \tTotal Chunks: 4, Chunks in use: 3. 364.8KiB allocated for chunks. 277.0KiB in use in bin. 277.0KiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305438: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (131072): \tTotal Chunks: 1, Chunks in use: 0. 155.5KiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305444: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (262144): \tTotal Chunks: 1, Chunks in use: 1. 351.8KiB allocated for chunks. 351.8KiB in use in bin. 351.6KiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305449: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (524288): \tTotal Chunks: 2, Chunks in use: 1. 1.44MiB allocated for chunks. 900.0KiB in use in bin. 900.0KiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305456: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (1048576): \tTotal Chunks: 3, Chunks in use: 1. 4.50MiB allocated for chunks. 1.50MiB in use in bin. 1.50MiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305462: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (2097152): \tTotal Chunks: 59, Chunks in use: 59. 132.75MiB allocated for chunks. 132.75MiB in use in bin. 130.50MiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305468: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (4194304): \tTotal Chunks: 1, Chunks in use: 1. 4.49MiB allocated for chunks. 4.49MiB in use in bin. 2.25MiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305473: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (8388608): \tTotal Chunks: 24, Chunks in use: 24. 222.75MiB allocated for chunks. 222.75MiB in use in bin. 216.00MiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305479: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (16777216): \tTotal Chunks: 5, Chunks in use: 5. 120.92MiB allocated for chunks. 120.92MiB in use in bin. 120.00MiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305485: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (33554432): \tTotal Chunks: 1, Chunks in use: 0. 41.42MiB allocated for chunks. 0B in use in bin. 0B client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305491: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (67108864): \tTotal Chunks: 4, Chunks in use: 3. 366.27MiB allocated for chunks. 268.26MiB in use in bin. 268.26MiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305498: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (134217728): \tTotal Chunks: 5, Chunks in use: 4. 953.81MiB allocated for chunks. 768.00MiB in use in bin. 768.00MiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305504: I tensorflow/tsl/framework/bfc_allocator.cc:1041] Bin (268435456): \tTotal Chunks: 38, Chunks in use: 38. 35.52GiB allocated for chunks. 35.52GiB in use in bin. 35.39GiB client-requested in use in bin.\n",
      "2023-04-04 03:49:02.305510: I tensorflow/tsl/framework/bfc_allocator.cc:1057] Bin for 192.00MiB was 128.00MiB, Chunk State: \n",
      "2023-04-04 03:49:02.305526: I tensorflow/tsl/framework/bfc_allocator.cc:1063]   Size: 185.81MiB | Requested Size: 24.00MiB | in_use: 0 | bin_num: 19, prev:   Size: 192.00MiB | Requested Size: 192.00MiB | in_use: 1 | bin_num: -1, next:   Size: 953.81MiB | Requested Size: 953.81MiB | in_use: 1 | bin_num: -1\n",
      "2023-04-04 03:49:02.305531: I tensorflow/tsl/framework/bfc_allocator.cc:1070] Next region of size 40076574720\n",
      "2023-04-04 03:49:02.305538: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a000000 of size 256 next 1\n",
      "2023-04-04 03:49:02.305544: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a000100 of size 1280 next 2\n",
      "2023-04-04 03:49:02.305549: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a000600 of size 256 next 3\n",
      "2023-04-04 03:49:02.305553: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a000700 of size 256 next 4\n",
      "2023-04-04 03:49:02.305557: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a000800 of size 256 next 5\n",
      "2023-04-04 03:49:02.305562: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a000900 of size 256 next 7\n",
      "2023-04-04 03:49:02.305566: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a000a00 of size 1280 next 8\n",
      "2023-04-04 03:49:02.305571: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a000f00 of size 256 next 6\n",
      "2023-04-04 03:49:02.305575: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a001000 of size 256 next 9\n",
      "2023-04-04 03:49:02.305579: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a001100 of size 1280 next 14\n",
      "2023-04-04 03:49:02.305584: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a001600 of size 256 next 12\n",
      "2023-04-04 03:49:02.305588: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a001700 of size 256 next 13\n",
      "2023-04-04 03:49:02.305592: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a001800 of size 256 next 19\n",
      "2023-04-04 03:49:02.305596: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a001900 of size 256 next 18\n",
      "2023-04-04 03:49:02.305600: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a001a00 of size 256 next 27\n",
      "2023-04-04 03:49:02.305605: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a001b00 of size 256 next 31\n",
      "2023-04-04 03:49:02.305609: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a001c00 of size 3072 next 32\n",
      "2023-04-04 03:49:02.305614: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a002800 of size 3072 next 33\n",
      "2023-04-04 03:49:02.305618: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a003400 of size 256 next 28\n",
      "2023-04-04 03:49:02.305622: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a003500 of size 256 next 35\n",
      "2023-04-04 03:49:02.305627: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a003600 of size 256 next 36\n",
      "2023-04-04 03:49:02.305631: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a003700 of size 256 next 17\n",
      "2023-04-04 03:49:02.305635: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a003800 of size 4608 next 22\n",
      "2023-04-04 03:49:02.305640: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a004a00 of size 3072 next 191\n",
      "2023-04-04 03:49:02.305644: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a005600 of size 3072 next 23\n",
      "2023-04-04 03:49:02.305649: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a006200 of size 3072 next 40\n",
      "2023-04-04 03:49:02.305653: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a006e00 of size 3072 next 43\n",
      "2023-04-04 03:49:02.305657: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a007a00 of size 3072 next 47\n",
      "2023-04-04 03:49:02.305661: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a008600 of size 3072 next 61\n",
      "2023-04-04 03:49:02.305666: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a009200 of size 3072 next 78\n",
      "2023-04-04 03:49:02.305670: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a009e00 of size 3072 next 100\n",
      "2023-04-04 03:49:02.305674: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a00aa00 of size 3072 next 153\n",
      "2023-04-04 03:49:02.305679: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a00b600 of size 3072 next 158\n",
      "2023-04-04 03:49:02.305683: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a00c200 of size 3072 next 146\n",
      "2023-04-04 03:49:02.305687: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a00ce00 of size 3072 next 162\n",
      "2023-04-04 03:49:02.305692: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a00da00 of size 15360 next 151\n",
      "2023-04-04 03:49:02.305696: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a011600 of size 3072 next 179\n",
      "2023-04-04 03:49:02.305700: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a012200 of size 3072 next 176\n",
      "2023-04-04 03:49:02.305704: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a012e00 of size 3072 next 177\n",
      "2023-04-04 03:49:02.305708: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a013a00 of size 3072 next 187\n",
      "2023-04-04 03:49:02.305713: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a014600 of size 3072 next 195\n",
      "2023-04-04 03:49:02.305717: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a015200 of size 3072 next 196\n",
      "2023-04-04 03:49:02.305721: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a015e00 of size 3072 next 194\n",
      "2023-04-04 03:49:02.305726: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a016a00 of size 3072 next 180\n",
      "2023-04-04 03:49:02.305730: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a017600 of size 3072 next 198\n",
      "2023-04-04 03:49:02.305734: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a018200 of size 3072 next 201\n",
      "2023-04-04 03:49:02.305738: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a018e00 of size 3072 next 205\n",
      "2023-04-04 03:49:02.305743: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a019a00 of size 3072 next 192\n",
      "2023-04-04 03:49:02.305747: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a01a600 of size 3072 next 193\n",
      "2023-04-04 03:49:02.305751: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a01b200 of size 12288 next 10\n",
      "2023-04-04 03:49:02.305756: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a01e200 of size 60160 next 11\n",
      "2023-04-04 03:49:02.305760: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a02cd00 of size 12288 next 41\n",
      "2023-04-04 03:49:02.305764: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a02fd00 of size 256 next 50\n",
      "2023-04-04 03:49:02.305769: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a02fe00 of size 256 next 54\n",
      "2023-04-04 03:49:02.305773: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a02ff00 of size 3072 next 58\n",
      "2023-04-04 03:49:02.305777: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a030b00 of size 3072 next 34\n",
      "2023-04-04 03:49:02.305782: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a031700 of size 3072 next 57\n",
      "2023-04-04 03:49:02.305786: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a032300 of size 3072 next 55\n",
      "2023-04-04 03:49:02.305790: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a032f00 of size 3072 next 60\n",
      "2023-04-04 03:49:02.305795: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a033b00 of size 3072 next 62\n",
      "2023-04-04 03:49:02.305799: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a034700 of size 3072 next 67\n",
      "2023-04-04 03:49:02.305803: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a035300 of size 3072 next 56\n",
      "2023-04-04 03:49:02.305808: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a035f00 of size 3072 next 70\n",
      "2023-04-04 03:49:02.305812: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a036b00 of size 5632 next 48\n",
      "2023-04-04 03:49:02.305816: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a038100 of size 3072 next 44\n",
      "2023-04-04 03:49:02.305821: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a038d00 of size 3072 next 45\n",
      "2023-04-04 03:49:02.305825: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a039900 of size 12288 next 84\n",
      "2023-04-04 03:49:02.305829: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a03c900 of size 3072 next 81\n",
      "2023-04-04 03:49:02.305833: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a03d500 of size 3072 next 85\n",
      "2023-04-04 03:49:02.305838: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a03e100 of size 3072 next 86\n",
      "2023-04-04 03:49:02.305842: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a03ed00 of size 3072 next 91\n",
      "2023-04-04 03:49:02.305846: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a03f900 of size 3072 next 89\n",
      "2023-04-04 03:49:02.305850: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a040500 of size 3072 next 94\n",
      "2023-04-04 03:49:02.305855: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a041100 of size 3072 next 99\n",
      "2023-04-04 03:49:02.305859: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a041d00 of size 3072 next 96\n",
      "2023-04-04 03:49:02.305863: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a042900 of size 3072 next 98\n",
      "2023-04-04 03:49:02.305867: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a043500 of size 12288 next 93\n",
      "2023-04-04 03:49:02.305872: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a046500 of size 3072 next 101\n",
      "2023-04-04 03:49:02.305876: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a047100 of size 3072 next 106\n",
      "2023-04-04 03:49:02.305880: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a047d00 of size 3072 next 104\n",
      "2023-04-04 03:49:02.305884: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a048900 of size 3072 next 74\n",
      "2023-04-04 03:49:02.305889: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a049500 of size 3072 next 76\n",
      "2023-04-04 03:49:02.305893: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a04a100 of size 3072 next 107\n",
      "2023-04-04 03:49:02.305897: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a04ad00 of size 3072 next 116\n",
      "2023-04-04 03:49:02.305902: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a04b900 of size 3072 next 115\n",
      "2023-04-04 03:49:02.305906: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a04c500 of size 3072 next 119\n",
      "2023-04-04 03:49:02.305910: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a04d100 of size 3072 next 103\n",
      "2023-04-04 03:49:02.305915: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a04dd00 of size 3072 next 113\n",
      "2023-04-04 03:49:02.305919: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a04e900 of size 3072 next 102\n",
      "2023-04-04 03:49:02.305923: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a04f500 of size 3072 next 127\n",
      "2023-04-04 03:49:02.305927: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a050100 of size 3072 next 133\n",
      "2023-04-04 03:49:02.305932: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a050d00 of size 3072 next 132\n",
      "2023-04-04 03:49:02.305936: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a051900 of size 3072 next 130\n",
      "2023-04-04 03:49:02.305940: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a052500 of size 3072 next 135\n",
      "2023-04-04 03:49:02.305945: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a053100 of size 3072 next 134\n",
      "2023-04-04 03:49:02.305949: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a053d00 of size 3072 next 131\n",
      "2023-04-04 03:49:02.305953: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a054900 of size 3072 next 138\n",
      "2023-04-04 03:49:02.305957: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a055500 of size 3072 next 142\n",
      "2023-04-04 03:49:02.305962: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a056100 of size 3072 next 145\n",
      "2023-04-04 03:49:02.305966: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a056d00 of size 3072 next 126\n",
      "2023-04-04 03:49:02.305971: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a057900 of size 3072 next 148\n",
      "2023-04-04 03:49:02.305975: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a058500 of size 3072 next 137\n",
      "2023-04-04 03:49:02.305979: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a059100 of size 3072 next 37\n",
      "2023-04-04 03:49:02.305984: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a059d00 of size 6144 next 240\n",
      "2023-04-04 03:49:02.305990: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a05b500 of size 12288 next 26\n",
      "2023-04-04 03:49:02.305994: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a05e500 of size 12288 next 117\n",
      "2023-04-04 03:49:02.305998: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a061500 of size 3072 next 112\n",
      "2023-04-04 03:49:02.306003: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a062100 of size 12288 next 21\n",
      "2023-04-04 03:49:02.306007: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a065100 of size 12288 next 141\n",
      "2023-04-04 03:49:02.306011: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a068100 of size 3072 next 159\n",
      "2023-04-04 03:49:02.306015: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a068d00 of size 3072 next 164\n",
      "2023-04-04 03:49:02.306020: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a069900 of size 3072 next 161\n",
      "2023-04-04 03:49:02.306024: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a06a500 of size 3072 next 155\n",
      "2023-04-04 03:49:02.306028: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a06b100 of size 3072 next 171\n",
      "2023-04-04 03:49:02.306033: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a06bd00 of size 3072 next 166\n",
      "2023-04-04 03:49:02.306037: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a06c900 of size 3072 next 175\n",
      "2023-04-04 03:49:02.306041: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a06d500 of size 3072 next 165\n",
      "2023-04-04 03:49:02.306045: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a06e100 of size 3072 next 178\n",
      "2023-04-04 03:49:02.306050: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a06ed00 of size 12288 next 152\n",
      "2023-04-04 03:49:02.306054: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a071d00 of size 3072 next 219\n",
      "2023-04-04 03:49:02.306058: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a072900 of size 12288 next 220\n",
      "2023-04-04 03:49:02.306062: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a075900 of size 3072 next 253\n",
      "2023-04-04 03:49:02.306067: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a076500 of size 3072 next 254\n",
      "2023-04-04 03:49:02.306071: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a077100 of size 3072 next 256\n",
      "2023-04-04 03:49:02.306075: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a077d00 of size 3072 next 257\n",
      "2023-04-04 03:49:02.306080: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a078900 of size 3072 next 259\n",
      "2023-04-04 03:49:02.306084: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a079500 of size 3072 next 261\n",
      "2023-04-04 03:49:02.306088: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a07a100 of size 3072 next 143\n",
      "2023-04-04 03:49:02.306092: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a07ad00 of size 3072 next 154\n",
      "2023-04-04 03:49:02.306096: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a07b900 of size 3072 next 208\n",
      "2023-04-04 03:49:02.306101: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a07c500 of size 3072 next 215\n",
      "2023-04-04 03:49:02.306105: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a07d100 of size 3072 next 221\n",
      "2023-04-04 03:49:02.306109: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a07dd00 of size 3072 next 226\n",
      "2023-04-04 03:49:02.306114: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a07e900 of size 3072 next 224\n",
      "2023-04-04 03:49:02.306118: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a07f500 of size 3072 next 223\n",
      "2023-04-04 03:49:02.306122: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a080100 of size 3072 next 228\n",
      "2023-04-04 03:49:02.306126: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a080d00 of size 3072 next 229\n",
      "2023-04-04 03:49:02.306131: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a081900 of size 3072 next 235\n",
      "2023-04-04 03:49:02.306135: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a082500 of size 3072 next 246\n",
      "2023-04-04 03:49:02.306139: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a083100 of size 3072 next 245\n",
      "2023-04-04 03:49:02.306144: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a083d00 of size 3072 next 29\n",
      "2023-04-04 03:49:02.306148: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a084900 of size 3072 next 236\n",
      "2023-04-04 03:49:02.306153: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a085500 of size 3072 next 68\n",
      "2023-04-04 03:49:02.306157: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a086100 of size 3072 next 75\n",
      "2023-04-04 03:49:02.306161: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a086d00 of size 5376 next 149\n",
      "2023-04-04 03:49:02.306165: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a088200 of size 3072 next 147\n",
      "2023-04-04 03:49:02.306170: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a088e00 of size 3072 next 144\n",
      "2023-04-04 03:49:02.306174: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a089a00 of size 3072 next 181\n",
      "2023-04-04 03:49:02.306178: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a08a600 of size 3072 next 182\n",
      "2023-04-04 03:49:02.306183: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a08b200 of size 3072 next 139\n",
      "2023-04-04 03:49:02.306187: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a08be00 of size 12288 next 189\n",
      "2023-04-04 03:49:02.306191: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a08ee00 of size 3072 next 210\n",
      "2023-04-04 03:49:02.306195: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a08fa00 of size 3072 next 206\n",
      "2023-04-04 03:49:02.306199: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a090600 of size 3072 next 209\n",
      "2023-04-04 03:49:02.306204: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a091200 of size 3072 next 212\n",
      "2023-04-04 03:49:02.306208: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a091e00 of size 3072 next 211\n",
      "2023-04-04 03:49:02.306212: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a092a00 of size 3072 next 263\n",
      "2023-04-04 03:49:02.306217: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a093600 of size 2048 next 265\n",
      "2023-04-04 03:49:02.306221: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a093e00 of size 3072 next 266\n",
      "2023-04-04 03:49:02.306225: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a094a00 of size 3072 next 267\n",
      "2023-04-04 03:49:02.306230: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a095600 of size 3072 next 268\n",
      "2023-04-04 03:49:02.306234: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a096200 of size 122112 next 269\n",
      "2023-04-04 03:49:02.306239: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0b3f00 of size 256 next 274\n",
      "2023-04-04 03:49:02.306243: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0b4000 of size 32000 next 275\n",
      "2023-04-04 03:49:02.306248: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0bbd00 of size 6400 next 276\n",
      "2023-04-04 03:49:02.306252: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0bd600 of size 3328 next 277\n",
      "2023-04-04 03:49:02.306256: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0be300 of size 256 next 278\n",
      "2023-04-04 03:49:02.306261: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0be400 of size 768 next 279\n",
      "2023-04-04 03:49:02.306265: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0be700 of size 8192 next 311\n",
      "2023-04-04 03:49:02.306271: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0c0700 of size 8192 next 316\n",
      "2023-04-04 03:49:02.306275: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0c2700 of size 8192 next 322\n",
      "2023-04-04 03:49:02.306279: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0c4700 of size 8192 next 328\n",
      "2023-04-04 03:49:02.306284: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0c6700 of size 8192 next 321\n",
      "2023-04-04 03:49:02.306288: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0c8700 of size 8192 next 327\n",
      "2023-04-04 03:49:02.306292: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0ca700 of size 8192 next 325\n",
      "2023-04-04 03:49:02.306297: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0cc700 of size 8192 next 351\n",
      "2023-04-04 03:49:02.306301: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0ce700 of size 8192 next 386\n",
      "2023-04-04 03:49:02.306305: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0d0700 of size 8192 next 390\n",
      "2023-04-04 03:49:02.306309: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0d2700 of size 8192 next 392\n",
      "2023-04-04 03:49:02.306314: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0d4700 of size 256 next 333\n",
      "2023-04-04 03:49:02.306318: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0d4800 of size 256 next 361\n",
      "2023-04-04 03:49:02.306323: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0d4900 of size 256 next 403\n",
      "2023-04-04 03:49:02.306327: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0d4a00 of size 256 next 405\n",
      "2023-04-04 03:49:02.306331: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0d4b00 of size 8192 next 400\n",
      "2023-04-04 03:49:02.306335: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0d6b00 of size 256 next 401\n",
      "2023-04-04 03:49:02.306340: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f066a0d6c00 of size 24320 next 16\n",
      "2023-04-04 03:49:02.306344: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a0dcb00 of size 360192 next 15\n",
      "2023-04-04 03:49:02.306349: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a134a00 of size 2359296 next 24\n",
      "2023-04-04 03:49:02.306353: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a374a00 of size 2359296 next 25\n",
      "2023-04-04 03:49:02.306357: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b4a00 of size 3072 next 108\n",
      "2023-04-04 03:49:02.306362: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b5600 of size 3072 next 110\n",
      "2023-04-04 03:49:02.306366: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6200 of size 256 next 237\n",
      "2023-04-04 03:49:02.306370: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6300 of size 256 next 230\n",
      "2023-04-04 03:49:02.306374: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6400 of size 256 next 66\n",
      "2023-04-04 03:49:02.306379: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6500 of size 256 next 238\n",
      "2023-04-04 03:49:02.306383: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6600 of size 256 next 243\n",
      "2023-04-04 03:49:02.306387: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6700 of size 256 next 242\n",
      "2023-04-04 03:49:02.306391: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6800 of size 256 next 249\n",
      "2023-04-04 03:49:02.306396: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6900 of size 256 next 252\n",
      "2023-04-04 03:49:02.306400: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6a00 of size 256 next 270\n",
      "2023-04-04 03:49:02.306404: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6b00 of size 256 next 272\n",
      "2023-04-04 03:49:02.306408: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6c00 of size 256 next 273\n",
      "2023-04-04 03:49:02.306412: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6d00 of size 256 next 222\n",
      "2023-04-04 03:49:02.306417: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066a5b6e00 of size 4709376 next 30\n",
      "2023-04-04 03:49:02.306422: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066aa34a00 of size 2359296 next 39\n",
      "2023-04-04 03:49:02.306426: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066ac74a00 of size 2359296 next 42\n",
      "2023-04-04 03:49:02.306430: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066aeb4a00 of size 2359296 next 46\n",
      "2023-04-04 03:49:02.306435: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066b0f4a00 of size 2359296 next 49\n",
      "2023-04-04 03:49:02.306439: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066b334a00 of size 2359296 next 51\n",
      "2023-04-04 03:49:02.306443: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066b574a00 of size 2359296 next 64\n",
      "2023-04-04 03:49:02.306447: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066b7b4a00 of size 2359296 next 65\n",
      "2023-04-04 03:49:02.306452: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066b9f4a00 of size 2359296 next 63\n",
      "2023-04-04 03:49:02.306456: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066bc34a00 of size 2359296 next 73\n",
      "2023-04-04 03:49:02.306460: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066be74a00 of size 2359296 next 80\n",
      "2023-04-04 03:49:02.306464: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066c0b4a00 of size 2359296 next 52\n",
      "2023-04-04 03:49:02.306469: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066c2f4a00 of size 9437184 next 53\n",
      "2023-04-04 03:49:02.306473: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066cbf4a00 of size 9437184 next 59\n",
      "2023-04-04 03:49:02.306478: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066d4f4a00 of size 2359296 next 82\n",
      "2023-04-04 03:49:02.306482: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066d734a00 of size 16515072 next 71\n",
      "2023-04-04 03:49:02.306487: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066e6f4a00 of size 9437184 next 72\n",
      "2023-04-04 03:49:02.306491: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066eff4a00 of size 9437184 next 77\n",
      "2023-04-04 03:49:02.306495: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066f8f4a00 of size 2359296 next 79\n",
      "2023-04-04 03:49:02.306499: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066fb34a00 of size 2359296 next 69\n",
      "2023-04-04 03:49:02.306504: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066fd74a00 of size 2359296 next 83\n",
      "2023-04-04 03:49:02.306508: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f066ffb4a00 of size 2359296 next 88\n",
      "2023-04-04 03:49:02.306512: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06701f4a00 of size 9437184 next 90\n",
      "2023-04-04 03:49:02.306517: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0670af4a00 of size 9437184 next 92\n",
      "2023-04-04 03:49:02.306521: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06713f4a00 of size 2359296 next 95\n",
      "2023-04-04 03:49:02.306525: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0671634a00 of size 2359296 next 97\n",
      "2023-04-04 03:49:02.306530: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0671874a00 of size 2359296 next 124\n",
      "2023-04-04 03:49:02.306534: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0671ab4a00 of size 2359296 next 87\n",
      "2023-04-04 03:49:02.306538: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0671cf4a00 of size 9437184 next 105\n",
      "2023-04-04 03:49:02.306542: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06725f4a00 of size 9437184 next 109\n",
      "2023-04-04 03:49:02.306546: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0672ef4a00 of size 2359296 next 121\n",
      "2023-04-04 03:49:02.306551: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0673134a00 of size 2359296 next 114\n",
      "2023-04-04 03:49:02.306555: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0673374a00 of size 2359296 next 140\n",
      "2023-04-04 03:49:02.306559: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06735b4a00 of size 2359296 next 120\n",
      "2023-04-04 03:49:02.306564: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06737f4a00 of size 9437184 next 122\n",
      "2023-04-04 03:49:02.306568: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06740f4a00 of size 9437184 next 125\n",
      "2023-04-04 03:49:02.306572: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06749f4a00 of size 2359296 next 118\n",
      "2023-04-04 03:49:02.306577: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0674c34a00 of size 2359296 next 123\n",
      "2023-04-04 03:49:02.306581: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0674e74a00 of size 25165824 next 407\n",
      "2023-04-04 03:49:02.306585: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f0676674a00 of size 102770688 next 150\n",
      "2023-04-04 03:49:02.306590: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067c877200 of size 9437184 next 136\n",
      "2023-04-04 03:49:02.306594: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067d177200 of size 2359296 next 128\n",
      "2023-04-04 03:49:02.306599: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067d3b7200 of size 2359296 next 168\n",
      "2023-04-04 03:49:02.306603: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067d5f7200 of size 2359296 next 173\n",
      "2023-04-04 03:49:02.306607: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067d837200 of size 2359296 next 160\n",
      "2023-04-04 03:49:02.306612: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067da77200 of size 2359296 next 170\n",
      "2023-04-04 03:49:02.306616: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067dcb7200 of size 2359296 next 185\n",
      "2023-04-04 03:49:02.306620: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f067def7200 of size 1572864 next 234\n",
      "2023-04-04 03:49:02.306624: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067e077200 of size 1572864 next 38\n",
      "2023-04-04 03:49:02.306630: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f067e1f7200 of size 1572864 next 167\n",
      "2023-04-04 03:49:02.306634: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067e377200 of size 9437184 next 169\n",
      "2023-04-04 03:49:02.306639: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067ec77200 of size 9437184 next 172\n",
      "2023-04-04 03:49:02.306644: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067f577200 of size 2359296 next 186\n",
      "2023-04-04 03:49:02.306648: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067f7b7200 of size 2359296 next 183\n",
      "2023-04-04 03:49:02.306653: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067f9f7200 of size 2359296 next 190\n",
      "2023-04-04 03:49:02.306657: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067fc37200 of size 2359296 next 174\n",
      "2023-04-04 03:49:02.306662: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f067fe77200 of size 9437184 next 184\n",
      "2023-04-04 03:49:02.306666: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0680777200 of size 9437184 next 188\n",
      "2023-04-04 03:49:02.306671: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0681077200 of size 2359296 next 202\n",
      "2023-04-04 03:49:02.306675: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06812b7200 of size 2359296 next 197\n",
      "2023-04-04 03:49:02.306680: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06814f7200 of size 2359296 next 207\n",
      "2023-04-04 03:49:02.306684: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0681737200 of size 2359296 next 200\n",
      "2023-04-04 03:49:02.306689: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0681977200 of size 9437184 next 199\n",
      "2023-04-04 03:49:02.306693: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0682277200 of size 9437184 next 204\n",
      "2023-04-04 03:49:02.306698: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0682b77200 of size 2359296 next 216\n",
      "2023-04-04 03:49:02.306702: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0682db7200 of size 2359296 next 217\n",
      "2023-04-04 03:49:02.306707: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0682ff7200 of size 2359296 next 225\n",
      "2023-04-04 03:49:02.306711: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0683237200 of size 2359296 next 203\n",
      "2023-04-04 03:49:02.306715: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0683477200 of size 9437184 next 214\n",
      "2023-04-04 03:49:02.306720: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0683d77200 of size 9437184 next 218\n",
      "2023-04-04 03:49:02.306725: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684677200 of size 921600 next 244\n",
      "2023-04-04 03:49:02.306729: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684758200 of size 256 next 286\n",
      "2023-04-04 03:49:02.306734: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684758300 of size 256 next 280\n",
      "2023-04-04 03:49:02.306739: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684758400 of size 256 next 281\n",
      "2023-04-04 03:49:02.306743: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684758500 of size 8192 next 295\n",
      "2023-04-04 03:49:02.306748: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068475a500 of size 8192 next 399\n",
      "2023-04-04 03:49:02.306752: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068475c500 of size 256 next 411\n",
      "2023-04-04 03:49:02.306756: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068475c600 of size 16128 next 305\n",
      "2023-04-04 03:49:02.306761: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684760500 of size 256 next 289\n",
      "2023-04-04 03:49:02.306765: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684760600 of size 8192 next 287\n",
      "2023-04-04 03:49:02.306770: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684762600 of size 256 next 298\n",
      "2023-04-04 03:49:02.306774: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684762700 of size 256 next 299\n",
      "2023-04-04 03:49:02.306778: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f0684762800 of size 256 next 292\n",
      "2023-04-04 03:49:02.306783: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684762900 of size 256 next 300\n",
      "2023-04-04 03:49:02.306787: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684762a00 of size 256 next 284\n",
      "2023-04-04 03:49:02.306791: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684762b00 of size 256 next 309\n",
      "2023-04-04 03:49:02.306796: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684762c00 of size 256 next 307\n",
      "2023-04-04 03:49:02.306800: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684762d00 of size 256 next 310\n",
      "2023-04-04 03:49:02.306805: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684762e00 of size 256 next 302\n",
      "2023-04-04 03:49:02.306809: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684762f00 of size 256 next 296\n",
      "2023-04-04 03:49:02.306813: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763000 of size 256 next 294\n",
      "2023-04-04 03:49:02.306817: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763100 of size 256 next 318\n",
      "2023-04-04 03:49:02.306822: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763200 of size 256 next 306\n",
      "2023-04-04 03:49:02.306826: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763300 of size 256 next 315\n",
      "2023-04-04 03:49:02.306830: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763400 of size 256 next 293\n",
      "2023-04-04 03:49:02.306834: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763500 of size 256 next 339\n",
      "2023-04-04 03:49:02.306839: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763600 of size 256 next 337\n",
      "2023-04-04 03:49:02.306843: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763700 of size 256 next 344\n",
      "2023-04-04 03:49:02.306848: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763800 of size 256 next 342\n",
      "2023-04-04 03:49:02.306852: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763900 of size 256 next 350\n",
      "2023-04-04 03:49:02.306856: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763a00 of size 256 next 348\n",
      "2023-04-04 03:49:02.306860: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763b00 of size 256 next 357\n",
      "2023-04-04 03:49:02.306865: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763c00 of size 256 next 331\n",
      "2023-04-04 03:49:02.306869: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763d00 of size 256 next 353\n",
      "2023-04-04 03:49:02.306873: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763e00 of size 256 next 366\n",
      "2023-04-04 03:49:02.306878: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684763f00 of size 256 next 368\n",
      "2023-04-04 03:49:02.306882: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684764000 of size 256 next 364\n",
      "2023-04-04 03:49:02.306886: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684764100 of size 256 next 362\n",
      "2023-04-04 03:49:02.306891: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684764200 of size 256 next 375\n",
      "2023-04-04 03:49:02.306895: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684764300 of size 256 next 382\n",
      "2023-04-04 03:49:02.306899: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684764400 of size 256 next 385\n",
      "2023-04-04 03:49:02.306904: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684764500 of size 256 next 379\n",
      "2023-04-04 03:49:02.306908: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684764600 of size 256 next 391\n",
      "2023-04-04 03:49:02.306912: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684764700 of size 256 next 282\n",
      "2023-04-04 03:49:02.306917: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684764800 of size 32768 next 283\n",
      "2023-04-04 03:49:02.306921: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068476c800 of size 8192 next 312\n",
      "2023-04-04 03:49:02.306926: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068476e800 of size 8192 next 308\n",
      "2023-04-04 03:49:02.306930: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684770800 of size 8192 next 291\n",
      "2023-04-04 03:49:02.306934: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684772800 of size 8192 next 301\n",
      "2023-04-04 03:49:02.306939: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684774800 of size 8192 next 324\n",
      "2023-04-04 03:49:02.306943: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684776800 of size 8192 next 330\n",
      "2023-04-04 03:49:02.306947: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684778800 of size 8192 next 329\n",
      "2023-04-04 03:49:02.306951: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068477a800 of size 8192 next 313\n",
      "2023-04-04 03:49:02.306956: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068477c800 of size 8192 next 288\n",
      "2023-04-04 03:49:02.306960: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068477e800 of size 8192 next 365\n",
      "2023-04-04 03:49:02.306964: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684780800 of size 8192 next 341\n",
      "2023-04-04 03:49:02.306969: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684782800 of size 8192 next 336\n",
      "2023-04-04 03:49:02.306973: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684784800 of size 8192 next 345\n",
      "2023-04-04 03:49:02.306977: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684786800 of size 8192 next 381\n",
      "2023-04-04 03:49:02.306981: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684788800 of size 8192 next 334\n",
      "2023-04-04 03:49:02.306986: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068478a800 of size 8192 next 370\n",
      "2023-04-04 03:49:02.306990: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068478c800 of size 8192 next 356\n",
      "2023-04-04 03:49:02.306995: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068478e800 of size 14336 next 374\n",
      "2023-04-04 03:49:02.306999: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684792000 of size 8192 next 393\n",
      "2023-04-04 03:49:02.307004: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684794000 of size 49152 next 398\n",
      "2023-04-04 03:49:02.307009: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06847a0000 of size 256 next 410\n",
      "2023-04-04 03:49:02.307013: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06847a0100 of size 50944 next 369\n",
      "2023-04-04 03:49:02.307018: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06847ac800 of size 8192 next 389\n",
      "2023-04-04 03:49:02.307022: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06847ae800 of size 8192 next 371\n",
      "2023-04-04 03:49:02.307026: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06847b0800 of size 32768 next 423\n",
      "2023-04-04 03:49:02.307031: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06847b8800 of size 8192 next 396\n",
      "2023-04-04 03:49:02.307035: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06847ba800 of size 256 next 417\n",
      "2023-04-04 03:49:02.307039: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f06847ba900 of size 89856 next 416\n",
      "2023-04-04 03:49:02.307044: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06847d0800 of size 65536 next 409\n",
      "2023-04-04 03:49:02.307048: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06847e0800 of size 32768 next 335\n",
      "2023-04-04 03:49:02.307052: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f06847e8800 of size 159232 next 402\n",
      "2023-04-04 03:49:02.307057: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068480f600 of size 96000 next 418\n",
      "2023-04-04 03:49:02.307062: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f0684826d00 of size 591104 next 231\n",
      "2023-04-04 03:49:02.307066: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06848b7200 of size 2359296 next 156\n",
      "2023-04-04 03:49:02.307070: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684af7200 of size 2359296 next 157\n",
      "2023-04-04 03:49:02.307075: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684d37200 of size 2359296 next 227\n",
      "2023-04-04 03:49:02.307079: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0684f77200 of size 9437184 next 213\n",
      "2023-04-04 03:49:02.307083: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0685877200 of size 2359296 next 250\n",
      "2023-04-04 03:49:02.307088: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0685ab7200 of size 2359296 next 248\n",
      "2023-04-04 03:49:02.307092: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0685cf7200 of size 2359296 next 247\n",
      "2023-04-04 03:49:02.307096: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0685f37200 of size 2359296 next 233\n",
      "2023-04-04 03:49:02.307101: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0686177200 of size 9437184 next 129\n",
      "2023-04-04 03:49:02.307105: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0686a77200 of size 9437184 next 20\n",
      "2023-04-04 03:49:02.307109: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0687377200 of size 9437184 next 163\n",
      "2023-04-04 03:49:02.307114: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0687c77200 of size 2359296 next 251\n",
      "2023-04-04 03:49:02.307118: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0687eb7200 of size 2359296 next 255\n",
      "2023-04-04 03:49:02.307122: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06880f7200 of size 2359296 next 258\n",
      "2023-04-04 03:49:02.307126: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0688337200 of size 2359296 next 260\n",
      "2023-04-04 03:49:02.307131: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0688577200 of size 2359296 next 262\n",
      "2023-04-04 03:49:02.307135: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06887b7200 of size 2359296 next 264\n",
      "2023-04-04 03:49:02.307139: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06889f7200 of size 25165824 next 347\n",
      "2023-04-04 03:49:02.307144: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068a1f7200 of size 26130432 next 232\n",
      "2023-04-04 03:49:02.307149: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f068bae2a00 of size 93763584 next 239\n",
      "2023-04-04 03:49:02.307153: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f069144e200 of size 25165824 next 413\n",
      "2023-04-04 03:49:02.307158: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0692c4e200 of size 25165824 next 377\n",
      "2023-04-04 03:49:02.307162: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f069444e200 of size 43431936 next 111\n",
      "2023-04-04 03:49:02.307166: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0696db9a00 of size 93763584 next 241\n",
      "2023-04-04 03:49:02.307171: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f069c725200 of size 93763584 next 271\n",
      "2023-04-04 03:49:02.307175: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06a2090a00 of size 201326592 next 414\n",
      "2023-04-04 03:49:02.307180: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06ae090a00 of size 201326592 next 419\n",
      "2023-04-04 03:49:02.307184: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06ba090a00 of size 201326592 next 408\n",
      "2023-04-04 03:49:02.307189: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06c6090a00 of size 201326592 next 406\n",
      "2023-04-04 03:49:02.307193: I tensorflow/tsl/framework/bfc_allocator.cc:1090] Free  at 7f06d2090a00 of size 194838528 next 297\n",
      "2023-04-04 03:49:02.307198: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f06dda60a00 of size 1000144896 next 290\n",
      "2023-04-04 03:49:02.307202: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0719430a00 of size 1000144896 next 285\n",
      "2023-04-04 03:49:02.307206: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0754e00a00 of size 1000144896 next 314\n",
      "2023-04-04 03:49:02.307210: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f07907d0a00 of size 1000144896 next 303\n",
      "2023-04-04 03:49:02.307215: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f07cc1a0a00 of size 1000144896 next 320\n",
      "2023-04-04 03:49:02.307219: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0807b70a00 of size 1000144896 next 304\n",
      "2023-04-04 03:49:02.307223: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0843540a00 of size 1000144896 next 326\n",
      "2023-04-04 03:49:02.307228: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f087ef10a00 of size 1000144896 next 317\n",
      "2023-04-04 03:49:02.307232: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f08ba8e0a00 of size 1000144896 next 332\n",
      "2023-04-04 03:49:02.307236: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f08f62b0a00 of size 1000144896 next 323\n",
      "2023-04-04 03:49:02.307241: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0931c80a00 of size 1000144896 next 338\n",
      "2023-04-04 03:49:02.307245: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f096d650a00 of size 1000144896 next 340\n",
      "2023-04-04 03:49:02.307249: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f09a9020a00 of size 1000144896 next 343\n",
      "2023-04-04 03:49:02.307254: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f09e49f0a00 of size 1000144896 next 346\n",
      "2023-04-04 03:49:02.307258: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0a203c0a00 of size 1000144896 next 349\n",
      "2023-04-04 03:49:02.307262: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0a5bd90a00 of size 1000144896 next 352\n",
      "2023-04-04 03:49:02.307267: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0a97760a00 of size 1000144896 next 355\n",
      "2023-04-04 03:49:02.307271: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0ad3130a00 of size 1000144896 next 358\n",
      "2023-04-04 03:49:02.307275: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0b0eb00a00 of size 1000144896 next 319\n",
      "2023-04-04 03:49:02.307279: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0b4a4d0a00 of size 1000144896 next 360\n",
      "2023-04-04 03:49:02.307284: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0b85ea0a00 of size 1000144896 next 367\n",
      "2023-04-04 03:49:02.307288: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0bc1870a00 of size 1000144896 next 359\n",
      "2023-04-04 03:49:02.307293: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0bfd240a00 of size 1000144896 next 363\n",
      "2023-04-04 03:49:02.307297: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0c38c10a00 of size 1000144896 next 376\n",
      "2023-04-04 03:49:02.307301: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0c745e0a00 of size 1000144896 next 378\n",
      "2023-04-04 03:49:02.307306: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0caffb0a00 of size 1000144896 next 383\n",
      "2023-04-04 03:49:02.307310: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0ceb980a00 of size 1000144896 next 380\n",
      "2023-04-04 03:49:02.307314: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0d27350a00 of size 1000144896 next 388\n",
      "2023-04-04 03:49:02.307319: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0d62d20a00 of size 1000144896 next 387\n",
      "2023-04-04 03:49:02.307323: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0d9e6f0a00 of size 1000144896 next 394\n",
      "2023-04-04 03:49:02.307328: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0dda0c0a00 of size 1000144896 next 384\n",
      "2023-04-04 03:49:02.307332: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0e15a90a00 of size 1000144896 next 373\n",
      "2023-04-04 03:49:02.307336: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0e51460a00 of size 1000144896 next 397\n",
      "2023-04-04 03:49:02.307341: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0e8ce30a00 of size 1000144896 next 372\n",
      "2023-04-04 03:49:02.307345: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0ec8800a00 of size 1000144896 next 354\n",
      "2023-04-04 03:49:02.307349: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0f041d0a00 of size 1000144896 next 412\n",
      "2023-04-04 03:49:02.307354: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0f3fba0a00 of size 1000144896 next 395\n",
      "2023-04-04 03:49:02.307359: I tensorflow/tsl/framework/bfc_allocator.cc:1090] InUse at 7f0f7b570a00 of size 1130952192 next 18446744073709551615\n",
      "2023-04-04 03:49:02.307363: I tensorflow/tsl/framework/bfc_allocator.cc:1095]      Summary of in-use Chunks by size: \n",
      "2023-04-04 03:49:02.307370: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 78 Chunks of size 256 totalling 19.5KiB\n",
      "2023-04-04 03:49:02.307375: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 768 totalling 768B\n",
      "2023-04-04 03:49:02.307380: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 1280 totalling 3.8KiB\n",
      "2023-04-04 03:49:02.307385: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 2048 totalling 2.0KiB\n",
      "2023-04-04 03:49:02.307390: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 123 Chunks of size 3072 totalling 369.0KiB\n",
      "2023-04-04 03:49:02.307395: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 3328 totalling 3.2KiB\n",
      "2023-04-04 03:49:02.307400: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 4608 totalling 4.5KiB\n",
      "2023-04-04 03:49:02.307405: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 5376 totalling 5.2KiB\n",
      "2023-04-04 03:49:02.307410: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 5632 totalling 5.5KiB\n",
      "2023-04-04 03:49:02.307415: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6144 totalling 6.0KiB\n",
      "2023-04-04 03:49:02.307420: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 6400 totalling 6.2KiB\n",
      "2023-04-04 03:49:02.307425: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 36 Chunks of size 8192 totalling 288.0KiB\n",
      "2023-04-04 03:49:02.307430: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 11 Chunks of size 12288 totalling 132.0KiB\n",
      "2023-04-04 03:49:02.307436: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 14336 totalling 14.0KiB\n",
      "2023-04-04 03:49:02.307452: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 15360 totalling 15.0KiB\n",
      "2023-04-04 03:49:02.307457: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 16128 totalling 15.8KiB\n",
      "2023-04-04 03:49:02.307461: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 32000 totalling 31.2KiB\n",
      "2023-04-04 03:49:02.307466: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 32768 totalling 96.0KiB\n",
      "2023-04-04 03:49:02.307470: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 49152 totalling 48.0KiB\n",
      "2023-04-04 03:49:02.307474: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 50944 totalling 49.8KiB\n",
      "2023-04-04 03:49:02.307479: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 60160 totalling 58.8KiB\n",
      "2023-04-04 03:49:02.307483: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 65536 totalling 64.0KiB\n",
      "2023-04-04 03:49:02.307487: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 96000 totalling 93.8KiB\n",
      "2023-04-04 03:49:02.307492: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 122112 totalling 119.2KiB\n",
      "2023-04-04 03:49:02.307496: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 360192 totalling 351.8KiB\n",
      "2023-04-04 03:49:02.307500: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 921600 totalling 900.0KiB\n",
      "2023-04-04 03:49:02.307504: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1572864 totalling 1.50MiB\n",
      "2023-04-04 03:49:02.307509: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 59 Chunks of size 2359296 totalling 132.75MiB\n",
      "2023-04-04 03:49:02.307513: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 4709376 totalling 4.49MiB\n",
      "2023-04-04 03:49:02.307517: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 23 Chunks of size 9437184 totalling 207.00MiB\n",
      "2023-04-04 03:49:02.307522: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 16515072 totalling 15.75MiB\n",
      "2023-04-04 03:49:02.307526: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 25165824 totalling 96.00MiB\n",
      "2023-04-04 03:49:02.307531: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 26130432 totalling 24.92MiB\n",
      "2023-04-04 03:49:02.307535: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 3 Chunks of size 93763584 totalling 268.26MiB\n",
      "2023-04-04 03:49:02.307540: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 4 Chunks of size 201326592 totalling 768.00MiB\n",
      "2023-04-04 03:49:02.307544: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 37 Chunks of size 1000144896 totalling 34.46GiB\n",
      "2023-04-04 03:49:02.307548: I tensorflow/tsl/framework/bfc_allocator.cc:1098] 1 Chunks of size 1130952192 totalling 1.05GiB\n",
      "2023-04-04 03:49:02.307553: I tensorflow/tsl/framework/bfc_allocator.cc:1102] Sum Total of in-use chunks: 37.00GiB\n",
      "2023-04-04 03:49:02.307558: I tensorflow/tsl/framework/bfc_allocator.cc:1104] total_region_allocated_bytes_: 40076574720 memory_limit_: 40076574720 available bytes: 0 curr_region_allocation_bytes_: 80153149440\n",
      "2023-04-04 03:49:02.307565: I tensorflow/tsl/framework/bfc_allocator.cc:1110] Stats: \n",
      "Limit:                     40076574720\n",
      "InUse:                     39731523072\n",
      "MaxInUse:                  39824545280\n",
      "NumAllocs:                       37232\n",
      "MaxAllocSize:               1130952192\n",
      "Reserved:                            0\n",
      "PeakReserved:                        0\n",
      "LargestFreeBlock:                    0\n",
      "\n",
      "2023-04-04 03:49:02.307579: W tensorflow/tsl/framework/bfc_allocator.cc:492] ****************************************************************************************************\n",
      "2023-04-04 03:49:02.307616: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at softmax_op_gpu.cu.cc:222 : RESOURCE_EXHAUSTED: OOM when allocating tensor with shape[16,12,512,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "Exception encountered when calling layer 'self' (type TFBertSelfAttention).\n\n{{function_node __wrapped__Softmax_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[16,12,512,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Softmax]\n\nCall arguments received by layer 'self' (type TFBertSelfAttention):\n   hidden_states=tf.Tensor(shape=(16, 512, 768), dtype=float32)\n   attention_mask=tf.Tensor(shape=(16, 1, 1, 512), dtype=float32)\n   head_mask=None\n   encoder_hidden_states=None\n   encoder_attention_mask=None\n   past_key_value=None\n   output_attentions=False\n   training=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m outputs \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_expansion_input \u001b[38;5;129;01min\u001b[39;00m expan_gen:\n\u001b[0;32m----> 6\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_expansion_input\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(batch_output)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sim_pred \u001b[38;5;129;01min\u001b[39;00m batch_output[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:433\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    432\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:1115\u001b[0m, in \u001b[0;36mTFBertModel.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;129m@unpack_inputs\u001b[39m\n\u001b[1;32m   1072\u001b[0m \u001b[38;5;129m@add_start_docstrings_to_model_forward\u001b[39m(BERT_INPUTS_DOCSTRING\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size, sequence_length\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1073\u001b[0m \u001b[38;5;129m@add_code_sample_docstrings\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1093\u001b[0m     training: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1094\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[TFBaseModelOutputWithPoolingAndCrossAttentions, Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[1;32m   1095\u001b[0m     \u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1096\u001b[0m \u001b[38;5;124;03m    encoder_hidden_states  (`tf.Tensor` of shape `(batch_size, sequence_length, hidden_size)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1097\u001b[0m \u001b[38;5;124;03m        Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention if\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[38;5;124;03m        `past_key_values`). Set to `False` during training, `True` during generation\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1115\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1126\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1128\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1129\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1130\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/transformers/modeling_tf_utils.py:433\u001b[0m, in \u001b[0;36munpack_inputs.<locals>.run_call_with_unpacked_inputs\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    430\u001b[0m     config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    432\u001b[0m unpacked_inputs \u001b[38;5;241m=\u001b[39m input_processing(func, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfn_args_and_kwargs)\n\u001b[0;32m--> 433\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43munpacked_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:871\u001b[0m, in \u001b[0;36mTFBertMainLayer.call\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    869\u001b[0m     head_mask \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers\n\u001b[0;32m--> 871\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    872\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    873\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    885\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    886\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(hidden_states\u001b[38;5;241m=\u001b[39msequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:562\u001b[0m, in \u001b[0;36mTFBertEncoder.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, training)\u001b[0m\n\u001b[1;32m    558\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m    560\u001b[0m past_key_value \u001b[38;5;241m=\u001b[39m past_key_values[i] \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 562\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    563\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    566\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    572\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:472\u001b[0m, in \u001b[0;36mTFBertLayer.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    461\u001b[0m     hidden_states: tf\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_tensor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:388\u001b[0m, in \u001b[0;36mTFBertAttention.call\u001b[0;34m(self, input_tensor, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\n\u001b[1;32m    378\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    379\u001b[0m     input_tensor: tf\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    386\u001b[0m     training: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    387\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[tf\u001b[38;5;241m.\u001b[39mTensor]:\n\u001b[0;32m--> 388\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_tensor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    391\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    392\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    393\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    394\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    395\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    396\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    397\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    398\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_output(\n\u001b[1;32m    399\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mself_outputs[\u001b[38;5;241m0\u001b[39m], input_tensor\u001b[38;5;241m=\u001b[39minput_tensor, training\u001b[38;5;241m=\u001b[39mtraining\n\u001b[1;32m    400\u001b[0m     )\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;66;03m# add attentions (possibly with past_key_value) if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/transformers/models/bert/modeling_tf_bert.py:327\u001b[0m, in \u001b[0;36mTFBertSelfAttention.call\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, training)\u001b[0m\n\u001b[1;32m    324\u001b[0m     attention_scores \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39madd(attention_scores, attention_mask)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# Normalize the attention scores to probabilities.\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[43mstable_softmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_scores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# This is actually dropping out entire tokens to attend to, which might\u001b[39;00m\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# seem a bit unusual, but is taken from the original Transformer paper.\u001b[39;00m\n\u001b[1;32m    331\u001b[0m attention_probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(inputs\u001b[38;5;241m=\u001b[39mattention_probs, training\u001b[38;5;241m=\u001b[39mtraining)\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/transformers/tf_utils.py:70\u001b[0m, in \u001b[0;36mstable_softmax\u001b[0;34m(logits, axis, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124;03mStable wrapper that returns the same output as `tf.nn.softmax`, but that works reliably with XLA on CPU. It is\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;124;03mmeant as a workaround for the [following issue](https://github.com/tensorflow/tensorflow/issues/55682), and will be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;124;03m        A Tensor. Has the same type and shape as logits.\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;66;03m# TODO: When the issue linked above gets sorted, add a check on TF version here and use the original function if\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# it has the fix. After we drop the support for unfixed versions, remove this function.\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlogits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogits\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m: Exception encountered when calling layer 'self' (type TFBertSelfAttention).\n\n{{function_node __wrapped__Softmax_device_/job:localhost/replica:0/task:0/device:GPU:0}} OOM when allocating tensor with shape[16,12,512,512] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:Softmax]\n\nCall arguments received by layer 'self' (type TFBertSelfAttention):\n   hidden_states=tf.Tensor(shape=(16, 512, 768), dtype=float32)\n   attention_mask=tf.Tensor(shape=(16, 1, 1, 512), dtype=float32)\n   head_mask=None\n   encoder_hidden_states=None\n   encoder_attention_mask=None\n   past_key_value=None\n   output_attentions=False\n   training=False"
     ]
    }
   ],
   "source": [
    "test_batch_size = 16\n",
    "expan_gen = TopicExpanExpansionGen(virtual_ego_graph, documents, test_batch_size)\n",
    "\n",
    "outputs = []\n",
    "for model_expansion_input in expan_gen:\n",
    "    batch_output = model(model_expansion_input)\n",
    "    outputs.append(batch_output)\n",
    "    \n",
    "    for sim_pred in batch_output[0]:\n",
    "        if sim_pred > 0:\n",
    "            print('found positive prediction!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((<tf.Tensor: shape=(29, 50), dtype=float32, numpy=\n",
       "  array([[ 0.23853   ,  0.34888   , -0.070754  , ...,  0.83062   ,\n",
       "          -0.23929   ,  1.1943    ],\n",
       "         [-1.5853    ,  1.3824    , -0.30704   , ..., -0.010681  ,\n",
       "          -0.066724  ,  0.1998    ],\n",
       "         [-1.41743   ,  0.111265  , -0.68006   , ...,  0.49370477,\n",
       "           0.06240024, -0.17009124],\n",
       "         ...,\n",
       "         [-1.9629    ,  1.1863    , -0.11088   , ..., -1.1283    ,\n",
       "          -0.073632  , -0.19008   ],\n",
       "         [-1.09367   ,  0.93693995, -0.59236664, ..., -0.28081235,\n",
       "          -0.01396667, -0.273598  ],\n",
       "         [-1.4572    ,  0.87621   , -0.56166   , ..., -0.45689002,\n",
       "          -0.33043998,  0.088323  ]], dtype=float32)>,\n",
       "  SparseTensor(indices=tf.Tensor(\n",
       "  [[ 0  0]\n",
       "   [ 0  1]\n",
       "   [ 0  2]\n",
       "   ...\n",
       "   [28 26]\n",
       "   [28 27]\n",
       "   [28 28]], shape=(841, 2), dtype=int64), values=tf.Tensor(\n",
       "  [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  1.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  1. -1. -1. -1.  0. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.\n",
       "   -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1. -1.  0.  0.  0.  0.  0. -1.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "   -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0. -1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
       "    0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.], shape=(841,), dtype=float64), dense_shape=tf.Tensor([29 29], shape=(2,), dtype=int64)),\n",
       "  <tf.Tensor: shape=(29,), dtype=int32, numpy=\n",
       "  array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0], dtype=int32)>,\n",
       "  <tf.Tensor: shape=(1, 512), dtype=float64, numpy=\n",
       "  array([[  101., 13955.,  2474., 29493.,  1011.,  7987.,  9336.,  5740.,\n",
       "           7903.,  1006.,  2141.,  2324.,  2257.,  3699.,  1999., 24665.,\n",
       "           8447.,  3540.,  2278.,  1010., 16420.,  2099.,  8936.,  1007.,\n",
       "           2003.,  1037., 16163.,  3455.,  2873.,  1998.,  2280.,  3455.,\n",
       "           2447.,  1012.,  2016.,  2318.,  2014.,  2476.,  1999., 27273.,\n",
       "          23808.,  6767.,  4681.,  2050.,  1010., 10722.,  2480.,  2721.,\n",
       "           1012.,  2016.,  2038.,  2180.,  1037.,  3165.,  3101.,  2012.,\n",
       "           1996.,  2997.,  2621.,  3783.,  2007.,  1996.,  8936.,  2308.,\n",
       "           1005.,  1055.,  2120.,  3455.,  2136.,  1012.,   102.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.,\n",
       "              0.,     0.,     0.,     0.,     0.,     0.,     0.,     0.]])>,\n",
       "  <tf.Tensor: shape=(1, 512), dtype=int64, numpy=\n",
       "  array([[ 101, 3455, 2447,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0]])>),\n",
       " (<tf.Tensor: shape=(1, 1), dtype=int64, numpy=array([[1]])>,\n",
       "  <tf.Tensor: shape=(1, 512), dtype=int64, numpy=\n",
       "  array([[3455, 2447,  102,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "             0,    0,    0,    0,    0,    0]])>))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gen[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "siblings_list = list(graph_dict[100][212].keys())\n",
    "\n",
    "siblings_list.remove(298)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1, x2, x3, x4, x5 = test_gen.__getitem__(2)[0]\n",
    "# model(test_gen.__getitem__(2)[0])[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = TopicExpanTrainGen(graph_list, documents[-1000:], documents_labels[-1000:], 1, 1, encoded_topic_to_tokenized_dict)\n",
    "x1, x2, x3, x4, x5 = test_gen.__getitem__(3)[0]\n",
    "sequence = x5.numpy()\n",
    "sequence[:, 1:] = 0\n",
    "# sequence[:, 1] = 2447"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 30522), dtype=float32, numpy=\n",
       "array([[-7.874007, -8.135588, -8.380221, ..., -8.34035 , -8.491272,\n",
       "        -8.455162]], dtype=float32)>"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model((x1, x2, x3, x4, sequence))[1]\n",
    "pred[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int64, numpy=3539>"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.argmax(pred[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (2185693414.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[119], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    model(x1, x2, x3, x4,\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "model(x1, x2, x3, x4, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'prime'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([3539])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] an [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]',\n",
       " '[CLS] tournament [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]']"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras_nlp.utils import beam_search\n",
    "from random import randint\n",
    "\n",
    "test_batch_size = 4\n",
    "test_gen = TopicExpanTrainGen(graph_list, documents[-20000:], documents_labels[-20000:], test_batch_size, 1, encoded_topic_to_tokenized_dict)\n",
    "x1, x2, x3, x4, x5 = test_gen[randint(0, 4000)][0]\n",
    "\n",
    "START_ID = 101\n",
    "END_ID = 102\n",
    "\n",
    "def token_probability_fn(inputs):\n",
    "    padded_inputs = tf.pad(inputs, [[0, 0], [0, max_len-tf.shape(inputs)[1]]])\n",
    "    repeats = int(padded_inputs.shape[0] / test_batch_size)\n",
    "    # print(inputs.shape)\n",
    "    preds = [\n",
    "        model((\n",
    "            x1, \n",
    "            x2, \n",
    "            x3, \n",
    "            x4, \n",
    "            padded_inputs[repeat_idx*test_batch_size:(repeat_idx+1)*test_batch_size]\n",
    "        ))[1] for repeat_idx in range(repeats)]\n",
    "    \n",
    "    # print(preds)\n",
    "    concatenated_preds = tf.concat(preds, axis=0)\n",
    "    # print(concatenated_preds[:, 0, :].shape)\n",
    "    \n",
    "    # the first zero index is the position in the sequence we're trying to find to add to the sequence\n",
    "    first_zero_index = (padded_inputs.numpy()[0]==0).argmax(axis=0)\n",
    "    return concatenated_preds[:, first_zero_index, :]\n",
    "\n",
    "prompt = tf.fill((test_batch_size, 1), START_ID)\n",
    "\n",
    "predicted_phrases = keras_nlp.utils.beam_search(\n",
    "    token_probability_fn,\n",
    "    prompt,\n",
    "    max_length=10,\n",
    "    num_beams=3,\n",
    "    end_token_id=END_ID,\n",
    "    from_logits=True\n",
    ")\n",
    "[tokenizer.decode(phrase) for phrase in predicted_phrases]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(32, 512), dtype=int32, numpy=\n",
       "array([[101,   0,   0, ...,   0,   0,   0],\n",
       "       [101,   0,   0, ...,   0,   0,   0],\n",
       "       [101,   0,   0, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [101,   0,   0, ...,   0,   0,   0],\n",
       "       [101,   0,   0, ...,   0,   0,   0],\n",
       "       [101,   0,   0, ...,   0,   0,   0]], dtype=int32)>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = tf.fill((batch_size, 1), START_ID)\n",
    "padded_prompt = tf.pad(prompt, [[0, 0], [0, max_len-tf.shape(prompt)[1]]])\n",
    "padded_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-03 18:51:23.138063: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at einsum_op_impl.h:498 : INVALID_ARGUMENT: Expected dimension 4 at axis 0 of the input shaped [32,1,16,48] but got dimension 32\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Exception encountered when calling layer 'multi_head_attention_1' (type MultiHeadAttention).\n\n{{function_node __wrapped__Einsum_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} Expected dimension 4 at axis 0 of the input shaped [32,1,16,48] but got dimension 32 [Op:Einsum]\n\nCall arguments received by layer 'multi_head_attention_1' (type MultiHeadAttention):\n   query=tf.Tensor(shape=(32, 1, 768), dtype=float32)\n   value=tf.Tensor(shape=(4, 512, 768), dtype=float32)\n   key=None\n   attention_mask=None\n   return_attention_scores=False\n   training=None\n   use_causal_mask=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m, tokenizer\u001b[38;5;241m.\u001b[39mdecode(x4[\u001b[38;5;241m2\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.conda/envs/esg/lib/python3.9/site-packages/tensorflow/python/framework/ops.py:7215\u001b[0m, in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   7213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mraise_from_not_ok_status\u001b[39m(e, name):\n\u001b[1;32m   7214\u001b[0m   e\u001b[38;5;241m.\u001b[39mmessage \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m name: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m name \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 7215\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_status_to_exception(e) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Exception encountered when calling layer 'multi_head_attention_1' (type MultiHeadAttention).\n\n{{function_node __wrapped__Einsum_N_2_device_/job:localhost/replica:0/task:0/device:GPU:0}} Expected dimension 4 at axis 0 of the input shaped [32,1,16,48] but got dimension 32 [Op:Einsum]\n\nCall arguments received by layer 'multi_head_attention_1' (type MultiHeadAttention):\n   query=tf.Tensor(shape=(32, 1, 768), dtype=float32)\n   value=tf.Tensor(shape=(4, 512, 768), dtype=float32)\n   key=None\n   attention_mask=None\n   return_attention_scores=False\n   training=None\n   use_causal_mask=False"
     ]
    }
   ],
   "source": [
    "print(model((x1, x2, x3, x4, prompt)), tokenizer.decode(x4[2], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(4, 1), dtype=int64, numpy=\n",
       " array([[1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1]])>,\n",
       " <tf.Tensor: shape=(4, 512), dtype=int64, numpy=\n",
       " array([[ 2679,  3586,   102, ...,     0,     0,     0],\n",
       "        [ 3137,  2846,   102, ...,     0,     0,     0],\n",
       "        [14211,   102,     0, ...,     0,     0,     0],\n",
       "        [ 2958,   102,     0, ...,     0,     0,     0]])>)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_gen[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>l1</th>\n",
       "      <th>l2</th>\n",
       "      <th>l3</th>\n",
       "      <th>l1_encoded</th>\n",
       "      <th>l2_encoded</th>\n",
       "      <th>l3_encoded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>The Ardee Baroque Festival is a celebration of...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>MusicFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>653</th>\n",
       "      <td>The Slamdance Film Festival is an annual film ...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>774</th>\n",
       "      <td>The Stan Rogers Folk Festival, informally know...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>MusicFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1350</th>\n",
       "      <td>The 5th Toronto International Film Festival (T...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>The 2010 Slamdance Film Festival took place in...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240049</th>\n",
       "      <td>The So Paulo International Film Festival (Por...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240163</th>\n",
       "      <td>The 40th annual Toronto International Film Fes...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240210</th>\n",
       "      <td>During the 19th century Trinidadians and other...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>MusicFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240226</th>\n",
       "      <td>New York Polish Film Festival (abbreviated to ...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>240253</th>\n",
       "      <td>The 40th Venice International Film Festival wa...</td>\n",
       "      <td>Event</td>\n",
       "      <td>SocietalEvent</td>\n",
       "      <td>FilmFestival</td>\n",
       "      <td>100</td>\n",
       "      <td>251</td>\n",
       "      <td>105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>881 rows  7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text     l1  \\\n",
       "214     The Ardee Baroque Festival is a celebration of...  Event   \n",
       "653     The Slamdance Film Festival is an annual film ...  Event   \n",
       "774     The Stan Rogers Folk Festival, informally know...  Event   \n",
       "1350    The 5th Toronto International Film Festival (T...  Event   \n",
       "1886    The 2010 Slamdance Film Festival took place in...  Event   \n",
       "...                                                   ...    ...   \n",
       "240049  The So Paulo International Film Festival (Por...  Event   \n",
       "240163  The 40th annual Toronto International Film Fes...  Event   \n",
       "240210  During the 19th century Trinidadians and other...  Event   \n",
       "240226  New York Polish Film Festival (abbreviated to ...  Event   \n",
       "240253  The 40th Venice International Film Festival wa...  Event   \n",
       "\n",
       "                   l2             l3  l1_encoded  l2_encoded  l3_encoded  \n",
       "214     SocietalEvent  MusicFestival         100         251         171  \n",
       "653     SocietalEvent   FilmFestival         100         251         105  \n",
       "774     SocietalEvent  MusicFestival         100         251         171  \n",
       "1350    SocietalEvent   FilmFestival         100         251         105  \n",
       "1886    SocietalEvent   FilmFestival         100         251         105  \n",
       "...               ...            ...         ...         ...         ...  \n",
       "240049  SocietalEvent   FilmFestival         100         251         105  \n",
       "240163  SocietalEvent   FilmFestival         100         251         105  \n",
       "240210  SocietalEvent  MusicFestival         100         251         171  \n",
       "240226  SocietalEvent   FilmFestival         100         251         105  \n",
       "240253  SocietalEvent   FilmFestival         100         251         105  \n",
       "\n",
       "[881 rows x 7 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['l3'].str.contains('Festival')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ZZ_VIRTUAL'], dtype=object)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_with_virtual = np.append(topics, 'ZZ_VIRTUAL')\n",
    "le = LabelEncoder()\n",
    "le.fit(topics_with_virtual)\n",
    "le.inverse_transform([298])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_ranking'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[160], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dense, Input\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_ranking\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspektral\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GCNConv, GlobalAvgPool, GraphMasking\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# n_out = dataset.n_labels\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_ranking'"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "from tensorflow import int64\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import tensorflow_ranking\n",
    "\n",
    "from spektral.layers import GCNConv, GlobalAvgPool, GraphMasking\n",
    "\n",
    "# n_out = dataset.n_labels\n",
    "\n",
    "X_in = Input(shape=(50))\n",
    "A_in = Input(shape=(None,), sparse=True)\n",
    "I_in = Input(shape=(), dtype=int64)\n",
    "\n",
    "X = GCNConv(32, activation='relu')([X_in, A_in])\n",
    "X = GCNConv(32, activation='relu')([X, A_in])\n",
    "X = GlobalAvgPool()([X, I_in])\n",
    "\n",
    "shared_bilinear = tensorflow_ranking.keras.layers.Bilinear(32, 32)\n",
    "X_1 = shared_bilinear([X, X])\n",
    "X = shared_bilinear([X, X], training=False)\n",
    "\n",
    "out = Dense(2, activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs=[X_in, A_in, I_in], outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "preds = tf.constant([[[-11.7803297],\n",
    " [-9.34260654],\n",
    " [-14.0992193],\n",
    " [-9.90242]],[[-11.7803297],\n",
    " [-9.34260654],\n",
    " [-14.0992193],\n",
    " [-9.90242]]], dtype=float)\n",
    "\n",
    "target = tf.constant([[[0],\n",
    " [0],\n",
    " [0],\n",
    " [1]],[[0],\n",
    " [0],\n",
    " [0],\n",
    " [1]]], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoided = tf.keras.activations.sigmoid(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=tf.reshape(target, shape=(2, -1)), logits=tf.reshape(sigmoided, shape=(2, -1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.activations.softmax(tf.reshape(preds, shape=(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reshape(target, shape=(2, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e907c9c662b38b6fe9e69d6881d7a36bc6e4ecfbfed88be2a66ef06ac6f6bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
