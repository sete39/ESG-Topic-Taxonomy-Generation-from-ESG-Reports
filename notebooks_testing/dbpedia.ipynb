{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<U3944')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../dataset/DBPEDIA_train.csv')\n",
    "document_list = df['text'].to_numpy().astype('str')\n",
    "document_list.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenizeing documents\n",
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# encoded_list = []\n",
    "\n",
    "# for i, doc in enumerate(document_list):\n",
    "#     encoded = tokenizer.encode_plus(doc, add_special_tokens=True, max_length=512)\n",
    "#     encoded_list.append(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text'] = encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding topics to create the adjacency matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "topics = np.concatenate((df['l1'].unique(), df['l2'].unique(), df['l3'].unique()))\n",
    "labelEncoder.fit(topics)\n",
    "\n",
    "def encode_topic(topic):\n",
    "    print(type(topic))\n",
    "    return labelEncoder.transform(topic)\n",
    "\n",
    "df['l1_encoded'] = labelEncoder.transform(df['l1'])\n",
    "df['l2_encoded'] = labelEncoder.transform(df['l2'])\n",
    "df['l3_encoded'] = labelEncoder.transform(df['l3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    l1 = row['l1_encoded']\n",
    "    l2 = row['l2_encoded']\n",
    "    l3 = row['l3_encoded']\n",
    "\n",
    "    if l1 not in graph_dict:\n",
    "        graph_dict[l1] = {}\n",
    "    if l2 not in graph_dict[l1]:\n",
    "        graph_dict[l1][l2] = {} \n",
    "\n",
    "    graph_dict[l1][l2][l3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating adjacency matrix\n",
    "adj_matrix = np.zeros((len(topics), len(topics)))\n",
    "square_numeric_dict = {'source': [], 'target': []}\n",
    "for i, df_row in df.iterrows():\n",
    "    l1 = df_row['l1_encoded']\n",
    "    l2 = df_row['l2_encoded']\n",
    "    l3 = df_row['l3_encoded']\n",
    "\n",
    "    adj_matrix[l1, l2] = 1\n",
    "    adj_matrix[l2, l3] = 1\n",
    "\n",
    "for i, row in enumerate(adj_matrix):\n",
    "    for j, value in enumerate(row):\n",
    "        if value == 0:\n",
    "            continue\n",
    "        square_numeric_dict['source'].append(i)\n",
    "        square_numeric_dict['target'].append(j)\n",
    "\n",
    "square_numeric_edges = pd.DataFrame(square_numeric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating node features\n",
    "x = np.arange(298)\n",
    "x = labelEncoder.inverse_transform(x)\n",
    "feature_array = x.reshape(298, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading GloVe model to get topic word embeddings\n",
    "# from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating \"ego-graphs\" (each node is seperated into a graph with itself, parent, and siblings)\n",
    "# The base node (so the node itself) will be masked, aka. have a [MASK] embedding\n",
    "# The sibling nodes need to have a negative relationship with the base node (so negative value in adjacency matrix?)\n",
    "from spektral.data import Graph\n",
    "import numpy as np\n",
    "import re\n",
    "def create_ego_graph(l1_topic, l2_topic, l3_topic, graph_dict):\n",
    "    if l3_topic != None:\n",
    "        siblings_list = list(graph_dict[l1_topic][l2_topic].keys())\n",
    "        siblings_list.remove(l3_topic)\n",
    "        base = l3_topic\n",
    "        parent = l2_topic\n",
    "        grandparent = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        all_nodes_list.append(parent)\n",
    "        all_nodes_list.append(grandparent)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "        encoded_parent = node_label_encoder.transform([parent])[0]\n",
    "        encoded_grandparent = node_label_encoder.transform([grandparent])[0]\n",
    "\n",
    "        adj_matrix[encoded_base][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_base] = 1\n",
    "\n",
    "        adj_matrix[encoded_grandparent][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_grandparent] = 1\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "        \n",
    "    elif l2_topic != None:\n",
    "        siblings_list = list(graph_dict[l1_topic].keys())\n",
    "        siblings_list.remove(l2_topic)\n",
    "        base = l2_topic\n",
    "        parent = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        all_nodes_list.append(parent)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "        encoded_parent = node_label_encoder.transform([parent])[0]\n",
    "\n",
    "        adj_matrix[encoded_base][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_base] = 1\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "    \n",
    "    elif l1_topic != None:\n",
    "        siblings_list = list(graph_dict.keys())\n",
    "        siblings_list.remove(l1_topic)\n",
    "        base = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "\n",
    "    ego_features = np.zeros((n_nodes, 50))\n",
    "    y = [0, 1] if l1_topic != 3 else [1, 0]\n",
    "    encoded_nodes_list = node_label_encoder.transform(all_nodes_list)\n",
    "\n",
    "    for i, node in enumerate(all_nodes_list):\n",
    "        feature = feature_array[node]\n",
    "        split_words_list = re.sub( r\"([A-Z])\", r\" \\1\", feature[0]).split()\n",
    "        n_words = len(split_words_list)\n",
    "        embedding_avg = np.array([glove[word.lower()].numpy() for word in split_words_list]).sum(axis=0)/n_words\n",
    "        \n",
    "        # Masking base node, setting the embedding to all 0's\n",
    "        if (node == base):\n",
    "            embedding_avg = glove['MASK']\n",
    "\n",
    "        ego_features[encoded_nodes_list[i]] = embedding_avg\n",
    "\n",
    "    return Graph(a=adj_matrix, x=ego_features, y=y)\n",
    "\n",
    "graph_list = []\n",
    "\n",
    "for l1_topic in graph_dict:\n",
    "    for l2_topic in graph_dict[l1_topic]:\n",
    "        for l3_topic in graph_dict[l1_topic][l2_topic]:\n",
    "            graph_list.append(create_ego_graph(l1_topic, l2_topic, l3_topic, graph_dict))\n",
    "        graph_list.append(create_ego_graph(l1_topic, l2_topic, None, graph_dict))\n",
    "    graph_list.append(create_ego_graph(l1_topic, None, None, graph_dict))\n",
    "\n",
    "graph_list = np.array(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, graph_list: list[Graph], **kwargs):\n",
    "        self.graph_list = graph_list\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        num_l =  np.random.permutation(len(self.graph_list))\n",
    "        return [self.graph_list[i] for i in num_l]\n",
    "    \n",
    "dataset = MyDataset(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saif8\\AppData\\Local\\Temp\\ipykernel_36396\\4187418381.py:4: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(dataset)\n"
     ]
    }
   ],
   "source": [
    "from spektral.data import DisjointLoader, BatchLoader\n",
    "\n",
    "# Train/test split\n",
    "np.random.shuffle(dataset)\n",
    "split = int(0.6 * len(dataset))\n",
    "data_tr, data_te = dataset[:split], dataset[split:]\n",
    "\n",
    "loader_tr = DisjointLoader(data_tr, batch_size=3, epochs=50)\n",
    "loader_te = DisjointLoader(data_te, batch_size=3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "    ################################################################################\n",
    "    # Build model\n",
    "    ################################################################################\n",
    "from tensorflow import int64\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from spektral.layers import GCNConv, GlobalAvgPool, GraphMasking\n",
    "\n",
    "n_out = dataset.n_labels\n",
    "\n",
    "    # class Net(Model):\n",
    "    #     def __init__(self):\n",
    "    #         super().__init__()\n",
    "    #         self.conv1 = GCNConv(32, activation=\"relu\")\n",
    "    #         self.conv2 = GCNConv(32, activation=\"relu\")\n",
    "    #         self.global_pool = GlobalAvgPool()\n",
    "    #         self.dense = Dense(n_out, activation='softmax')\n",
    "\n",
    "    #     def call(self, inputs):\n",
    "    #         x, a, i = inputs\n",
    "    #         print(x, a, i)\n",
    "    #         x = self.conv1([x, a])\n",
    "    #         x = self.conv2([x, a])\n",
    "    #         output = self.global_pool([x, i])\n",
    "    #         output = self.dense(output)\n",
    "\n",
    "    #         return output\n",
    "        \n",
    "X_in = Input(shape=(dataset.n_node_features))\n",
    "A_in = Input(shape=(None,), sparse=True)\n",
    "I_in = Input(shape=(), dtype=int64)\n",
    "\n",
    "X = GCNConv(32, activation='relu')([X_in, A_in])\n",
    "X = GCNConv(32, activation='relu')([X, A_in])\n",
    "X = GlobalAvgPool()([X, I_in])\n",
    "out = Dense(n_out, activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs=[X_in, A_in, I_in], outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_7 (InputLayer)           [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " input_8 (InputLayer)           [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " gcn_conv_4 (GCNConv)           (None, 32)           1632        ['input_7[0][0]',                \n",
      "                                                                  'input_8[0][0]']                \n",
      "                                                                                                  \n",
      " gcn_conv_5 (GCNConv)           (None, 32)           1056        ['gcn_conv_4[0][0]',             \n",
      "                                                                  'input_8[0][0]']                \n",
      "                                                                                                  \n",
      " input_9 (InputLayer)           [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " global_avg_pool_2 (GlobalAvgPo  (None, 32)          0           ['gcn_conv_5[0][0]',             \n",
      " ol)                                                              'input_9[0][0]']                \n",
      "                                                                                                  \n",
      " dense_2 (Dense)                (None, 2)            66          ['global_avg_pool_2[0][0]']      \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,754\n",
      "Trainable params: 2,754\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "import tensorflow as tf\n",
    "learning_rate = 1e-3  # Learning rate\n",
    "epochs = 10  # Number of training epochs\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# model = Net()\n",
    "optimizer = Adam(learning_rate)\n",
    "loss_fn = CategoricalCrossentropy()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "15/60 [======>.......................] - ETA: 0s - loss: 0.9262 - accuracy: 0.5778 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saif8\\miniconda3\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60/60 [==============================] - 2s 11ms/step - loss: 0.5719 - accuracy: 0.6910\n",
      "Epoch 2/10\n",
      "60/60 [==============================] - 1s 12ms/step - loss: 0.2828 - accuracy: 0.9551\n",
      "Epoch 3/10\n",
      "60/60 [==============================] - 1s 10ms/step - loss: 0.1922 - accuracy: 0.9831\n",
      "Epoch 4/10\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.1301 - accuracy: 0.9888\n",
      "Epoch 5/10\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.0894 - accuracy: 0.9944\n",
      "Epoch 6/10\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 0.0683 - accuracy: 0.9944\n",
      "Epoch 7/10\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.0521 - accuracy: 0.9944\n",
      "Epoch 8/10\n",
      "60/60 [==============================] - 1s 17ms/step - loss: 0.0449 - accuracy: 0.9944\n",
      "Epoch 9/10\n",
      "60/60 [==============================] - 1s 13ms/step - loss: 0.0347 - accuracy: 0.9944\n",
      "Epoch 10/10\n",
      "60/60 [==============================] - 1s 11ms/step - loss: 0.0334 - accuracy: 0.9944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x197d678ae80>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "40/40 [==============================] - 1s 10ms/step - loss: 0.0275 - accuracy: 1.0000\n",
      "Done. Test loss: [0.02752322517335415, 1.0]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Evaluate model\n",
    "################################################################################\n",
    "print(\"Testing model\")\n",
    "loss = model.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)\n",
    "print(\"Done. Test loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e907c9c662b38b6fe9e69d6881d7a36bc6e4ecfbfed88be2a66ef06ac6f6bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
