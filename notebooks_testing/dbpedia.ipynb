{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('<U3944')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../dataset/DBPEDIA_train.csv')\n",
    "document_list = df['text'].to_numpy().astype('str')\n",
    "document_list.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Tokenizeing documents\n",
    "# from transformers import BertTokenizer\n",
    "# tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "# encoded_list = []\n",
    "\n",
    "# for i, doc in enumerate(document_list):\n",
    "#     encoded = tokenizer.encode_plus(doc, add_special_tokens=True, max_length=512)\n",
    "#     encoded_list.append(encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['text'] = encoded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding topics to create the adjacency matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import numpy as np\n",
    "\n",
    "labelEncoder = LabelEncoder()\n",
    "topics = np.concatenate((df['l1'].unique(), df['l2'].unique(), df['l3'].unique()))\n",
    "labelEncoder.fit(topics)\n",
    "\n",
    "def encode_topic(topic):\n",
    "    print(type(topic))\n",
    "    return labelEncoder.transform(topic)\n",
    "\n",
    "df['l1_encoded'] = labelEncoder.transform(df['l1'])\n",
    "df['l2_encoded'] = labelEncoder.transform(df['l2'])\n",
    "df['l3_encoded'] = labelEncoder.transform(df['l3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dict = {}\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    l1 = row['l1_encoded']\n",
    "    l2 = row['l2_encoded']\n",
    "    l3 = row['l3_encoded']\n",
    "\n",
    "    if l1 not in graph_dict:\n",
    "        graph_dict[l1] = {}\n",
    "    if l2 not in graph_dict[l1]:\n",
    "        graph_dict[l1][l2] = {} \n",
    "\n",
    "    graph_dict[l1][l2][l3] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating adjacency matrix\n",
    "adj_matrix = np.zeros((len(topics), len(topics)))\n",
    "square_numeric_dict = {'source': [], 'target': []}\n",
    "for i, df_row in df.iterrows():\n",
    "    l1 = df_row['l1_encoded']\n",
    "    l2 = df_row['l2_encoded']\n",
    "    l3 = df_row['l3_encoded']\n",
    "\n",
    "    adj_matrix[l1, l2] = 1\n",
    "    adj_matrix[l2, l3] = 1\n",
    "\n",
    "for i, row in enumerate(adj_matrix):\n",
    "    for j, value in enumerate(row):\n",
    "        if value == 0:\n",
    "            continue\n",
    "        square_numeric_dict['source'].append(i)\n",
    "        square_numeric_dict['target'].append(j)\n",
    "\n",
    "square_numeric_edges = pd.DataFrame(square_numeric_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating node features\n",
    "x = np.arange(298)\n",
    "x = labelEncoder.inverse_transform(x)\n",
    "feature_array = x.reshape(298, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading GloVe model to get topic word embeddings\n",
    "# from https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "glove = torchtext.vocab.GloVe(name=\"6B\", dim=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating \"ego-graphs\" (each node is seperated into a graph with itself, parent, and siblings)\n",
    "# The base node (so the node itself) will be masked, aka. have a [MASK] embedding\n",
    "# The sibling nodes need to have a negative relationship with the base node (so negative value in adjacency matrix?)\n",
    "from spektral.data import Graph\n",
    "import numpy as np\n",
    "import re\n",
    "def create_ego_graph(l1_topic, l2_topic, l3_topic, graph_dict):\n",
    "    if l3_topic != None:\n",
    "        siblings_list = list(graph_dict[l1_topic][l2_topic].keys())\n",
    "        siblings_list.remove(l3_topic)\n",
    "        base = l3_topic\n",
    "        parent = l2_topic\n",
    "        grandparent = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        all_nodes_list.append(parent)\n",
    "        all_nodes_list.append(grandparent)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "        encoded_parent = node_label_encoder.transform([parent])[0]\n",
    "        encoded_grandparent = node_label_encoder.transform([grandparent])[0]\n",
    "\n",
    "        adj_matrix[encoded_base][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_base] = 1\n",
    "\n",
    "        adj_matrix[encoded_grandparent][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_grandparent] = 1\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "        \n",
    "    elif l2_topic != None:\n",
    "        siblings_list = list(graph_dict[l1_topic].keys())\n",
    "        siblings_list.remove(l2_topic)\n",
    "        base = l2_topic\n",
    "        parent = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        all_nodes_list.append(parent)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "        encoded_parent = node_label_encoder.transform([parent])[0]\n",
    "\n",
    "        adj_matrix[encoded_base][encoded_parent] = 1\n",
    "        adj_matrix[encoded_parent][encoded_base] = 1\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "    \n",
    "    elif l1_topic != None:\n",
    "        siblings_list = list(graph_dict.keys())\n",
    "        siblings_list.remove(l1_topic)\n",
    "        base = l1_topic\n",
    "\n",
    "        all_nodes_list = siblings_list.copy()\n",
    "        all_nodes_list.append(base)\n",
    "        \n",
    "        n_nodes = len(all_nodes_list)\n",
    "        adj_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "        node_label_encoder = LabelEncoder()\n",
    "        node_label_encoder.fit(all_nodes_list)\n",
    "        \n",
    "        encoded_base = node_label_encoder.transform([base])[0]\n",
    "\n",
    "        for sibling in siblings_list: \n",
    "            encoded_sibling = node_label_encoder.transform([sibling])[0]\n",
    "            adj_matrix[encoded_sibling][encoded_base] = -1\n",
    "            adj_matrix[encoded_base][encoded_sibling] = -1\n",
    "\n",
    "    ego_features = np.zeros((n_nodes, 50))\n",
    "    y = [0, 1] if l1_topic != 3 else [1, 0]\n",
    "    encoded_nodes_list = node_label_encoder.transform(all_nodes_list)\n",
    "\n",
    "    for i, node in enumerate(all_nodes_list):\n",
    "        feature = feature_array[node]\n",
    "        split_words_list = re.sub( r\"([A-Z])\", r\" \\1\", feature[0]).split()\n",
    "        n_words = len(split_words_list)\n",
    "        embedding_avg = np.array([glove[word.lower()].numpy() for word in split_words_list]).sum(axis=0)/n_words\n",
    "        \n",
    "        # Masking base node, setting the embedding to all 0's\n",
    "        if (node == base):\n",
    "            embedding_avg = glove['MASK']\n",
    "\n",
    "        ego_features[encoded_nodes_list[i]] = embedding_avg\n",
    "\n",
    "    return Graph(a=adj_matrix, x=ego_features, y=y)\n",
    "\n",
    "graph_list = []\n",
    "\n",
    "for l1_topic in graph_dict:\n",
    "    for l2_topic in graph_dict[l1_topic]:\n",
    "        for l3_topic in graph_dict[l1_topic][l2_topic]:\n",
    "            graph_list.append(create_ego_graph(l1_topic, l2_topic, l3_topic, graph_dict))\n",
    "        graph_list.append(create_ego_graph(l1_topic, l2_topic, None, graph_dict))\n",
    "    graph_list.append(create_ego_graph(l1_topic, None, None, graph_dict))\n",
    "\n",
    "graph_list = np.array(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spektral.data import Dataset\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    def __init__(self, graph_list: list[Graph], **kwargs):\n",
    "        self.graph_list = graph_list\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def read(self):\n",
    "        # We must return a list of Graph objects\n",
    "        num_l =  np.random.permutation(len(self.graph_list))\n",
    "        return [self.graph_list[i] for i in num_l]\n",
    "    \n",
    "dataset = MyDataset(graph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saif8\\AppData\\Local\\Temp\\ipykernel_33004\\2340940201.py:8: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(dataset)\n"
     ]
    }
   ],
   "source": [
    "from spektral.data import DisjointLoader, BatchLoader\n",
    "\n",
    "learning_rate = 1e-3  # Learning rate\n",
    "epochs = 20  # Number of training epochs\n",
    "batch_size = 32  # Batch size\n",
    "\n",
    "# Train/test split\n",
    "np.random.shuffle(dataset)\n",
    "split = int(0.6 * len(dataset))\n",
    "data_tr, data_te = dataset[:split], dataset[split:]\n",
    "\n",
    "loader_tr = DisjointLoader(data_tr, batch_size=3, epochs=epochs)\n",
    "loader_te = DisjointLoader(data_te, batch_size=3,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "from tensorflow import int64\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from spektral.layers import GCNConv, GlobalAvgPool, GraphMasking\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy\n",
    "import tensorflow as tf\n",
    "\n",
    "# model = Net()\n",
    "optimizer = Adam(learning_rate)\n",
    "loss_fn = CategoricalCrossentropy()\n",
    "\n",
    "# Custom training loop\n",
    "class ModelWithNCE(Model):\n",
    "    @tf.function(experimental_relax_shapes=True)\n",
    "    def train_step(self, data):\n",
    "        # print(data)\n",
    "        inputs, target = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self(inputs, training=True)\n",
    "            loss = loss_fn(target, predictions) + sum(self.losses)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(target, predictions)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "n_out = dataset.n_labels\n",
    "        \n",
    "X_in = Input(shape=(dataset.n_node_features))\n",
    "A_in = Input(shape=(None,), sparse=True)\n",
    "I_in = Input(shape=(), dtype=int64)\n",
    "\n",
    "X = GCNConv(32, activation='relu')([X_in, A_in])\n",
    "X = GCNConv(32, activation='relu')([X, A_in])\n",
    "X = GlobalAvgPool()([X, I_in])\n",
    "out = Dense(n_out, activation='softmax')(X)\n",
    "\n",
    "model = ModelWithNCE(inputs=[X_in, A_in, I_in], outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_with_nce_17\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_67 (InputLayer)          [(None, 50)]         0           []                               \n",
      "                                                                                                  \n",
      " input_68 (InputLayer)          [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " gcn_conv_44 (GCNConv)          (None, 32)           1632        ['input_67[0][0]',               \n",
      "                                                                  'input_68[0][0]']               \n",
      "                                                                                                  \n",
      " gcn_conv_45 (GCNConv)          (None, 32)           1056        ['gcn_conv_44[0][0]',            \n",
      "                                                                  'input_68[0][0]']               \n",
      "                                                                                                  \n",
      " input_69 (InputLayer)          [(None,)]            0           []                               \n",
      "                                                                                                  \n",
      " global_avg_pool_22 (GlobalAvgP  (None, 32)          0           ['gcn_conv_45[0][0]',            \n",
      " ool)                                                             'input_69[0][0]']               \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 2)            66          ['global_avg_pool_22[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,754\n",
      "Trainable params: 2,754\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/20\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saif8\\miniconda3\\lib\\site-packages\\spektral\\data\\utils.py:221: UserWarning: you are shuffling a 'MyDataset' object which is not a subclass of 'Sequence'; `shuffle` is not guaranteed to behave correctly. E.g., non-numpy array/tensor objects with view semantics may contain duplicates after shuffling.\n",
      "  np.random.shuffle(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      " 1/60 [..............................] - ETA: 44s - accuracy: 1.0000[]\n",
      "49/60 [=======================>......] - ETA: 0s - accuracy: 0.7755[]\n",
      "60/60 [==============================] - 2s 19ms/step - accuracy: 0.8034\n",
      "Epoch 2/20\n",
      "60/60 [==============================] - 0s 7ms/step - accuracy: 0.9888\n",
      "Epoch 3/20\n",
      "60/60 [==============================] - 0s 4ms/step - accuracy: 1.0000\n",
      "Epoch 4/20\n",
      "60/60 [==============================] - 0s 5ms/step - accuracy: 1.0000\n",
      "Epoch 5/20\n",
      "60/60 [==============================] - 0s 6ms/step - accuracy: 1.0000\n",
      "Epoch 6/20\n",
      "60/60 [==============================] - 0s 5ms/step - accuracy: 1.0000\n",
      "Epoch 7/20\n",
      "60/60 [==============================] - 0s 5ms/step - accuracy: 1.0000\n",
      "Epoch 8/20\n",
      "60/60 [==============================] - 0s 5ms/step - accuracy: 1.0000\n",
      "Epoch 9/20\n",
      "60/60 [==============================] - 0s 6ms/step - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "60/60 [==============================] - 0s 5ms/step - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "60/60 [==============================] - 0s 5ms/step - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "60/60 [==============================] - 0s 5ms/step - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "60/60 [==============================] - 0s 4ms/step - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "60/60 [==============================] - 0s 5ms/step - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "60/60 [==============================] - 0s 4ms/step - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "60/60 [==============================] - 0s 4ms/step - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "60/60 [==============================] - 0s 4ms/step - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "60/60 [==============================] - 0s 4ms/step - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "60/60 [==============================] - 0s 4ms/step - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "60/60 [==============================] - 0s 4ms/step - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e4d7cf5f70>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=['accuracy'], run_eagerly=True)\n",
    "model.summary()\n",
    "model.fit(loader_tr.load(), steps_per_epoch=loader_tr.steps_per_epoch, epochs=epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model\n",
      "40/40 [==============================] - 1s 14ms/step - loss: 0.0955 - accuracy: 0.9917\n",
      "Done. Test loss: [0.09553246200084686, 0.9916666746139526]\n"
     ]
    }
   ],
   "source": [
    "################################################################################\n",
    "# Evaluate model\n",
    "################################################################################\n",
    "print(\"Testing model\")\n",
    "loss = model.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)\n",
    "print(\"Done. Test loss: {}\".format(loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################\n",
    "# Build model\n",
    "################################################################################\n",
    "from tensorflow import int64\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "import tensorflow_ranking\n",
    "\n",
    "from spektral.layers import GCNConv, GlobalAvgPool, GraphMasking\n",
    "\n",
    "# n_out = dataset.n_labels\n",
    "\n",
    "X_in = Input(shape=(50))\n",
    "A_in = Input(shape=(None,), sparse=True)\n",
    "I_in = Input(shape=(), dtype=int64)\n",
    "\n",
    "X = GCNConv(32, activation='relu')([X_in, A_in])\n",
    "X = GCNConv(32, activation='relu')([X, A_in])\n",
    "X = GlobalAvgPool()([X, I_in])\n",
    "\n",
    "shared_bilinear = tensorflow_ranking.keras.layers.Bilinear(32, 32)\n",
    "X_1 = shared_bilinear([X, X])\n",
    "X = shared_bilinear([X, X], training=False)\n",
    "\n",
    "out = Dense(2, activation='softmax')(X)\n",
    "\n",
    "model = Model(inputs=[X_in, A_in, I_in], outputs=out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You must install pydot (`pip install pydot`) and install graphviz (see instructions at https://graphviz.gitlab.io/download/) for plot_model to work.\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4e907c9c662b38b6fe9e69d6881d7a36bc6e4ecfbfed88be2a66ef06ac6f6bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
