{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a3044f64",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'type' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-9ec89917664e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mReports\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/topic_taxonomy_generation/ESG-Topic-Taxonomy-Generation-from-ESG-Reports/dataset/Reports.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# import pandas as pd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     def __init__(self, page_count: int, block_number: int, size: float, font: str, color: int,\n\u001b[1;32m      7\u001b[0m                   bbox: list[float], texts: list[str], text_str: str) -> None:\n",
      "\u001b[0;32m~/topic_taxonomy_generation/ESG-Topic-Taxonomy-Generation-from-ESG-Reports/dataset/Reports.py\u001b[0m in \u001b[0;36mBlock\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBlock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     def __init__(self, page_count: int, block_number: int, size: float, font: str, color: int,\n\u001b[0;32m----> 7\u001b[0;31m                   bbox: list[float], texts: list[str], text_str: str) -> None:\n\u001b[0m\u001b[1;32m      8\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpage_count\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock_number\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblock_number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'type' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f388dced",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "report_list = Reports.get_all_reports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9d0956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10214"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(report_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eeff2eb-9e5e-4f30-a139-7fcfc29d0c74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def read_normalized_file(filename: str) :\n",
    "    # NOTE: each file represents an ESG report, and each line represents a page. \n",
    "    # In a page, each dict represents a block, which could be a text block or an image block.\n",
    "    # The images are all deleted. Each block tells the size, font, and location, of a text.\n",
    "    with open(filename, encoding=\"utf8\") as json_file:\n",
    "        json_list = list(json_file)\n",
    "    \n",
    "    return json_list\n",
    "        \n",
    "summ_json = read_normalized_file('/scratch/users/k21148846/report_summary_extracted_data.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2e72f453-034d-4aa9-8147-8cf86f936e7c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'original_file_path': 'full/5af136d35bf8ab9cc6116a3498f2a0a943d54c27.pdf',\n",
       " 'to_name': '5af136d35bf8ab9cc6116a3498f2a0a943d54c27',\n",
       " 'url': 'https://www.responsibilityreports.com/HostedData/ResponsibilityReportArchive/w/NASDAQ_WNFM_2019.pdf',\n",
       " 'url_depth': 1,\n",
       " 'date': '2019',\n",
       " 'title': '2019 Corporate Sustainability Report',\n",
       " 'content_type': '',\n",
       " 'content_disposition': '',\n",
       " 'content_disposition_file_name': '',\n",
       " 'upper_url': 'https://www.responsibilityreports.com/Company/wayne-farms-inc',\n",
       " 'metadata': {'format': 'PDF 1.5',\n",
       "  'title': '',\n",
       "  'author': '',\n",
       "  'subject': '',\n",
       "  'keywords': '',\n",
       "  'creator': 'Adobe InDesign 14.0 (Macintosh)',\n",
       "  'producer': 'Adobe PDF Library 15.0',\n",
       "  'creationDate': \"D:20191121164258-05'00'\",\n",
       "  'modDate': \"D:20191121164422-05'00'\",\n",
       "  'trapped': '',\n",
       "  'encryption': None},\n",
       " 'source_id': '',\n",
       " 'contain_table_of_content': False,\n",
       " 'contain_links': False}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.loads(summ_json[100])['reports'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43e8156e-7565-479c-9c84-d04f8cea4140",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_json = read_normalized_file('/scratch/users/k21148846/' + 'normalization_text/5af136d35bf8ab9cc6116a3498f2a0a943d54c27' + '.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dict_counter: dict[str, int] = {}\n",
    "for report in report_list:\n",
    "    for word in report.title.split(' '):\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "        if word not in title_dict_counter:\n",
    "            title_dict_counter[word] = 1\n",
    "        else:\n",
    "            title_dict_counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d601e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.DataFrame(title_dict_counter.items(), columns=['word', 'count'])\n",
    "ds = ds.sort_values(by='count', ascending=False)\n",
    "words_to_remove = ['Report', 'and', 'Annual', 'Progress', 'Development', 'Review', 'Impact', 'Business', 'Activity', 'Summary', 'Statement', 'Data', 'Update', 'Supplement', 'on', 'Reporting']\n",
    "ds_filtered = ds[~ds['word'].isin(words_to_remove)].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd63fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_filtered['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_title_counter = 0\n",
    "not_found_reports_list = []\n",
    "is_found_word = False\n",
    "\n",
    "for report in report_list:\n",
    "    is_found_word = False\n",
    "    \n",
    "    for word in report.title.split(' '):\n",
    "        if ds_filtered[ds['word'] == word].shape[0] > 0:\n",
    "            in_title_counter += 1\n",
    "            is_found_word = True\n",
    "            break\n",
    "\n",
    "    if not is_found_word:\n",
    "        not_found_reports_list.append(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6429de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_found_reports_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d3ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "bert_cased_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d11c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForPreTraining\n",
    "config = BertConfig()\n",
    "model = BertForPreTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d3da2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saif8\\miniconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:362: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDatasetForNextSentencePrediction\n",
    "dataset = TextDatasetForNextSentencePrediction(\n",
    "    tokenizer=bert_cased_tokenizer,\n",
    "    file_path=\"./preprocessed.txt\",\n",
    "    block_size = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b331c873",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\nNo module named 'pandas.api'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1110\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\trainer.py:168\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available():\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\datasets\\__init__.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\datasets\\arrow_dataset.py:66\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\datasets\\arrow_writer.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Features, Image, Value\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     FeatureType,\n\u001b[0;32m     30\u001b[0m     _ArrayXDExtensionType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     to_pyarrow_listarray,\n\u001b[0;32m     37\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\datasets\\features\\__init__.py:18\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Array2D, Array3D, Array4D, Array5D, ClassLabel, Features, Sequence, Value\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\datasets\\features\\features.py:35\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExtensionArray \u001b[38;5;28;01mas\u001b[39;00m PandasExtensionArray\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExtensionDtype \u001b[38;5;28;01mas\u001b[39;00m PandasExtensionDtype\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas.api'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     15\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     16\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     17\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1100\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1100\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1112\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1113\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1114\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1115\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nNo module named 'pandas.api'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_gpu_train_batch_size= 16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "920c5f45",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.api'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas.api'"
     ]
    }
   ],
   "source": [
    "import pandas.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b7e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.array([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2a5a42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Reports\n",
    "report_list = Reports.get_all_reports('/scratch/users/k21148846/report_summary_extracted_data.jsonl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e79e05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "paragraph_list = []\n",
    "paragraph_info_list = []\n",
    "for report in report_list:\n",
    "    for page in report.pages_list:\n",
    "        sentence_list = []\n",
    "        for block in page.block_list:\n",
    "            # use text_str\n",
    "            # remove all numbers\n",
    "            # ignore if no full stop or comma\n",
    "            # split by full stop\n",
    "            # filtered_text = re.sub(r'\\d+', '', block.text_str)\n",
    "            sentence_list.append(block.text_str.strip())\n",
    "        \n",
    "        paragraph = ' '.join(sentence_list)\n",
    "        paragraph_length = len(paragraph.split())\n",
    "        if paragraph_length < 20:\n",
    "            continue\n",
    "        paragraph_list.append(paragraph)\n",
    "        paragraph_info_list.append((page.block_list[0].page_count, report.url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c4f0dc78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 609763\n",
      "100000 773000\n",
      "200000 942289\n",
      "300000 1111212\n",
      "400000 1278384\n",
      "500000 1443869\n",
      "600000 1610487\n",
      "Final length:  1208546\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "from copy import deepcopy\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 256\n",
    "indices_to_pop = []\n",
    "for i, p in enumerate(deepcopy(paragraph_list)):\n",
    "    paragraph = p.split()\n",
    "    length = len(paragraph)\n",
    "    if i % 100000 == 0:\n",
    "        print(i, len(paragraph_list))\n",
    "    if length <= MAX_SENTENCE_LENGTH:\n",
    "        continue\n",
    "    \n",
    "    n_splits = ceil(length/MAX_SENTENCE_LENGTH)\n",
    "    split_paragraphs = [' '.join(paragraph[s*MAX_SENTENCE_LENGTH:(s*MAX_SENTENCE_LENGTH)+MAX_SENTENCE_LENGTH]) for s in range(n_splits)]\n",
    "    split_paragraph_infos = [paragraph_info_list[i] for _ in range(n_splits)]\n",
    "        \n",
    "    indices_to_pop.append(i)\n",
    "    \n",
    "    paragraph_list.extend(split_paragraphs)\n",
    "    paragraph_info_list.extend(split_paragraph_infos)\n",
    "# need to reverse first so indices dont get messed up in the original array\n",
    "for i in indices_to_pop[::-1]:\n",
    "    paragraph_list.pop(i)\n",
    "    paragraph_info_list.pop(i)\n",
    "\n",
    "print('Final length: ', len(paragraph_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d10ebc17",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('/scratch/users/k21148846/docs_NO_ID.pkl', 'wb') as handle:\n",
    "    pickle.dump((paragraph_list, paragraph_info_list), handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "edc1e06c-106e-4a45-b638-d9d78aedd0c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1208546"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paragraph_info_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1ba80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(paragraph_list):\n",
    "    if p == '':\n",
    "        paragraph_list.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "41c773a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('../notebooks_testing/docs_NO_ID.pkl', 'rb') as fp:\n",
    "    docs = pickle.load(fp)\n",
    "\n",
    "with open('../notebooks_testing/results.pkl', 'rb') as handle:\n",
    "    results = pickle.load(handle)\n",
    "\n",
    "import pandas as pd\n",
    "result_df = pd.DataFrame(results)\n",
    "docs_df = pd.DataFrame({'docs': docs})\n",
    "df = pd.concat([docs_df, result_df], axis=1)\n",
    "df = df.sample(frac=1).reset_index(drop=True).loc[:,~df.columns.duplicated()].copy() # shuffle\n",
    "\n",
    "del docs, results, result_df, docs_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b5165df-41f0-42ad-9737-366d669dd49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[(df['score'] < 0.7) & (df['label'] == 'Governance'), 'label'] = 'None'\n",
    "df.loc[(df['score'] < 0.9) & (df['label'] == 'Social'), 'label'] = 'None' \n",
    "df.loc[(df['score'] < 0.9) & (df['label'] == 'Environmental'), 'label'] = 'None' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5836f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('docs_NO_ID.pkl', 'wb') as handle:\n",
    "    pickle.dump(paragraph_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b084849a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting terms and remove the quality value\n",
    "f = open('./AutoPhrase.txt', 'r', encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7bbcec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "printable = set(string.printable)\n",
    "\n",
    "terms = []\n",
    "for line in lines:\n",
    "    splitted_line = line.split('\\t')\n",
    "    terms.append(splitted_line[1])\n",
    "\n",
    "f = open('./terms.txt', 'w', encoding='utf-8')\n",
    "for i, term in enumerate(terms):\n",
    "    ascii_term = bytes(term, 'ascii', errors='ignore').decode()\n",
    "    filtered_term = ''.join((filter(lambda x: x in printable, ascii_term)))\n",
    "    f.write(filtered_term)\n",
    "    terms[i] = term[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "bdcbc5f8-2878-4587-9ae2-d130d9b25118",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "esg_topic = 'Governance'\n",
    "f = open(\"../dataset/terms_\" + esg_topic + \".txt\", 'r', encoding='utf-8')\n",
    "terms = [term[:-1] for term in f.readlines()]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ca4f5cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms_small = terms[:110000]\n",
    "terms_multiwords = []\n",
    "\n",
    "for term in terms_small:\n",
    "    split_term = term.split()\n",
    "    if len(split_term) > 1:\n",
    "        terms_multiwords.append(' '.join(split_term))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e92e8733",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_with_index(s, newstring, start_index, end_index):\n",
    "    return s[:start_index] + newstring + s[end_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "102c1368-6379-4cb0-9784-8b1f841c7c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terms = [' '.join(term.split('_')) for term in terms]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f0c7e473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "docs = df[df['label']==esg_topic]['docs'].to_list()\n",
    "\n",
    "matching_regex = '|'.join(terms_multiwords)\n",
    "# matching_regex = matching_regex.replace('||', '|')\n",
    "pattern = re.compile(matching_regex)\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    doc_lowered = doc.lower()\n",
    "    matched = pattern.match(doc_lowered)\n",
    "    while bool(matched):\n",
    "        start_index, end_index = matched.start(), matched.end()\n",
    "        word_with_underscore = matched.group(0).replace(' ', '_')\n",
    "\n",
    "        doc_lowered = replace_with_index(doc_lowered, word_with_underscore, start_index, end_index)\n",
    "\n",
    "        matched = pattern.match(doc_lowered)\n",
    "\n",
    "    docs[i] = doc_lowered\n",
    "    if i % 100000 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "85f9e370",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokenized_docs = []\n",
    "for i, doc in enumerate(docs):\n",
    "    tokenized_docs.append(' '.join(word_tokenize(bytes(doc, 'ascii', errors='ignore').decode())))\n",
    "    if i % 100000 == 0:\n",
    "        print(i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "50458ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "printable = set(string.printable)\n",
    "\n",
    "f = open(\"../dataset/terms_\" + esg_topic + \".txt\", 'w', encoding='utf-8')\n",
    "for i, term in enumerate(terms):\n",
    "    ascii_term = bytes(term, 'ascii', errors='ignore').decode()\n",
    "    filtered_term = ''.join((filter(lambda x: x in printable, ascii_term)))\n",
    "    f.write('_'.join(filtered_term.split()) + '\\n')\n",
    "    terms[i] = term[:-1]\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ac258209",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../dataset/docs_\" + esg_topic + \".txt\", 'w') as f:\n",
    "    for doc in tokenized_docs:\n",
    "        f.write(doc)\n",
    "        f.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "14c6a7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'external initiatives'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "backup.group(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f299d4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_regex.replace('||', '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4420aa43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'||' in matching_regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "883b597c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\ ebus|\\\\ sourcing tool|\\\\ seurujoki|\\\\ siniscola|\\\\ energieeffizienz|\\\\ bowles|\\\\ svetloye|\\\\ prefectureâ€™'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matching_regex[-100:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e8fefd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "printable = set(string.printable)\n",
    "\n",
    "f = open('./docs.txt', 'r', encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('./docs.txt', 'w', encoding='utf-8')\n",
    "for line in lines:\n",
    "    ascii_line = bytes(line, 'ascii', errors='ignore').decode()\n",
    "    filtered_line = ''.join((filter(lambda x: x in printable, ascii_line)))\n",
    "    f.write(filtered_line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d0a6119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Month</th>\n",
       "      <th>Price</th>\n",
       "      <th>Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>May-18</td>\n",
       "      <td>269.67</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jun-18</td>\n",
       "      <td>264.35</td>\n",
       "      <td>-1.97%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jul-18</td>\n",
       "      <td>266.88</td>\n",
       "      <td>0.96%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aug-18</td>\n",
       "      <td>261.04</td>\n",
       "      <td>-2.19%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sep-18</td>\n",
       "      <td>276.76</td>\n",
       "      <td>6.02%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Month   Price  Change\n",
       "0  May-18  269.67       -\n",
       "1  Jun-18  264.35  -1.97%\n",
       "2  Jul-18  266.88   0.96%\n",
       "3  Aug-18  261.04  -2.19%\n",
       "4  Sep-18  276.76   6.02%"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('crude-oil-60.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdec2b31-57a7-4969-af96-d1624e9a298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.156714744787369\n"
     ]
    }
   ],
   "source": [
    "# avg change from 44 to 59\n",
    "import matplotlib as plt\n",
    "\n",
    "print((302.83 / 267.62 - 1) * 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "59ffd9a4-fe3e-4ac8-a93c-34ed5dff7a01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6028323742620132\n"
     ]
    }
   ],
   "source": [
    "print((df['Price'].iloc[49] / df['Price'].iloc[43] - 1) * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1de8bdeb-2a34-4782-ad46-d026e53c0e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54.244824751513356\n"
     ]
    }
   ],
   "source": [
    "find_change_percent = lambda x, y: ((x/y) - 1) * 100\n",
    "print(find_change_percent(df['Price'].iloc[46], df['Price'].iloc[43]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a6ebe2-320c-4efd-bf18-4e88739cbe9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e907c9c662b38b6fe9e69d6881d7a36bc6e4ecfbfed88be2a66ef06ac6f6bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
