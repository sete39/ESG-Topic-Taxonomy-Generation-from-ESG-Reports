{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3044f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f388dced",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './normalization_text/f995667298e06d6ce2c81f0b30d0738445919c46.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m report_list \u001b[38;5;241m=\u001b[39m \u001b[43mReports\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_all_reports\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/topic_taxonomy_generation/ESG-Topic-Taxonomy-Generation-from-ESG-Reports/dataset/Reports.py:102\u001b[0m, in \u001b[0;36mget_all_reports\u001b[0;34m()\u001b[0m\n\u001b[1;32m    100\u001b[0m     summary_dict \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mloads(summary_json_str)\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m report_dict \u001b[38;5;129;01min\u001b[39;00m summary_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreports\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[0;32m--> 102\u001b[0m         report \u001b[38;5;241m=\u001b[39m \u001b[43mReport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_summary\u001b[49m\u001b[43m(\u001b[49m\u001b[43msummary_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreport_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m         report_list\u001b[38;5;241m.\u001b[39mappend(report)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m report_list\n",
      "File \u001b[0;32m~/topic_taxonomy_generation/ESG-Topic-Taxonomy-Generation-from-ESG-Reports/dataset/Reports.py:62\u001b[0m, in \u001b[0;36mReport.from_summary\u001b[0;34m(summary_dict, report_dict)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_summary\u001b[39m(summary_dict: \u001b[38;5;28mdict\u001b[39m, report_dict: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# summary_dict is the whole summary of a company\u001b[39;00m\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;66;03m# report_dict is one report from a company's summary\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     pages_list \u001b[38;5;241m=\u001b[39m \u001b[43mread_normalized_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreport_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mto_normalization_text_name\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Report(\n\u001b[1;32m     64\u001b[0m         pages_list\u001b[38;5;241m=\u001b[39mpages_list,\n\u001b[1;32m     65\u001b[0m         company_name\u001b[38;5;241m=\u001b[39msummary_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcompany_name\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     74\u001b[0m         keywords\u001b[38;5;241m=\u001b[39mreport_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeywords\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     75\u001b[0m     )\n",
      "File \u001b[0;32m~/topic_taxonomy_generation/ESG-Topic-Taxonomy-Generation-from-ESG-Reports/dataset/Reports.py:82\u001b[0m, in \u001b[0;36mread_normalized_file\u001b[0;34m(normalized_filename)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_normalized_file\u001b[39m(normalized_filename: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Page]:\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;66;03m# NOTE: each file represents an ESG report, and each line represents a page. \u001b[39;00m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;66;03m# In a page, each dict represents a block, which could be a text block or an image block.\u001b[39;00m\n\u001b[1;32m     81\u001b[0m     \u001b[38;5;66;03m# The images are all deleted. Each block tells the size, font, and location, of a text.\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./normalization_text/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mnormalized_filename\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m json_file:\n\u001b[1;32m     83\u001b[0m         json_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(json_file)\n\u001b[1;32m     85\u001b[0m     page_list: \u001b[38;5;28mlist\u001b[39m[Page] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './normalization_text/f995667298e06d6ce2c81f0b30d0738445919c46.jsonl'"
     ]
    }
   ],
   "source": [
    "report_list = Reports.get_all_reports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d9d0956",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10214"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(report_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a5de9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "title_dict_counter: dict[str, int] = {}\n",
    "for report in report_list:\n",
    "    for word in report.title.split(' '):\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "        if word not in title_dict_counter:\n",
    "            title_dict_counter[word] = 1\n",
    "        else:\n",
    "            title_dict_counter[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d601e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = pd.DataFrame(title_dict_counter.items(), columns=['word', 'count'])\n",
    "ds = ds.sort_values(by='count', ascending=False)\n",
    "words_to_remove = ['Report', 'and', 'Annual', 'Progress', 'Development', 'Review', 'Impact', 'Business', 'Activity', 'Summary', 'Statement', 'Data', 'Update', 'Supplement', 'on', 'Reporting']\n",
    "ds_filtered = ds[~ds['word'].isin(words_to_remove)].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ddd63fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_filtered['count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437f5207",
   "metadata": {},
   "outputs": [],
   "source": [
    "in_title_counter = 0\n",
    "not_found_reports_list = []\n",
    "is_found_word = False\n",
    "\n",
    "for report in report_list:\n",
    "    is_found_word = False\n",
    "    \n",
    "    for word in report.title.split(' '):\n",
    "        if ds_filtered[ds['word'] == word].shape[0] > 0:\n",
    "            in_title_counter += 1\n",
    "            is_found_word = True\n",
    "            break\n",
    "\n",
    "    if not is_found_word:\n",
    "        not_found_reports_list.append(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6429de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(not_found_reports_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b2d3ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "bert_cased_tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0d11c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertConfig, BertForPreTraining\n",
    "config = BertConfig()\n",
    "model = BertForPreTraining(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d3da2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saif8\\miniconda3\\lib\\site-packages\\transformers\\data\\datasets\\language_modeling.py:362: FutureWarning: This dataset will be removed from the library soon, preprocessing should be handled with the ðŸ¤— Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_mlm.py\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TextDatasetForNextSentencePrediction\n",
    "dataset = TextDatasetForNextSentencePrediction(\n",
    "    tokenizer=bert_cased_tokenizer,\n",
    "    file_path=\"./preprocessed.txt\",\n",
    "    block_size = 256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b331c873",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.trainer because of the following error (look up to see its traceback):\nNo module named 'pandas.api'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1110\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1109\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\importlib\\__init__.py:127\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    126\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 127\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1030\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1007\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:986\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:680\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:850\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:228\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\trainer.py:168\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_datasets_available():\n\u001b[1;32m--> 168\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdatasets\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_torch_tpu_available(check_device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\datasets\\__init__.py:43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[1;32m---> 43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\datasets\\arrow_dataset.py:66\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdownload_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DownloadConfig\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\datasets\\arrow_writer.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Features, Image, Value\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     29\u001b[0m     FeatureType,\n\u001b[0;32m     30\u001b[0m     _ArrayXDExtensionType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m     to_pyarrow_listarray,\n\u001b[0;32m     37\u001b[0m )\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\datasets\\features\\__init__.py:18\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Audio\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Array2D, Array3D, Array4D, Array5D, ClassLabel, Features, Sequence, Value\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\datasets\\features\\features.py:35\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpyarrow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExtensionArray \u001b[38;5;28;01mas\u001b[39;00m PandasExtensionArray\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextensions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExtensionDtype \u001b[38;5;28;01mas\u001b[39;00m PandasExtensionDtype\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas.api'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Trainer, TrainingArguments\n\u001b[0;32m      3\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      4\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./output\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      5\u001b[0m     overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     10\u001b[0m     prediction_loss_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     11\u001b[0m )\n\u001b[0;32m     13\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     14\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     15\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     16\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     17\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mdataset,\n\u001b[0;32m     18\u001b[0m )\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1055\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1100\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1099\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1100\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1101\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1102\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32m~\\miniconda3\\lib\\site-packages\\transformers\\utils\\import_utils.py:1112\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1111\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1113\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1114\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1115\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.trainer because of the following error (look up to see its traceback):\nNo module named 'pandas.api'"
     ]
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= \"./output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=2,\n",
    "    per_gpu_train_batch_size= 16,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"./model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "920c5f45",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas.api'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapi\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pandas.api'"
     ]
    }
   ],
   "source": [
    "import pandas.api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10b7e16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "arr = np.array([1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6b2a5a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import Reports\n",
    "report_list = Reports.get_all_reports()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf1e86ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report_list[0].pages_list[0].block_list[0].text_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b1e79e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "paragraph_list = []\n",
    "\n",
    "for report in report_list:\n",
    "    for page in report.pages_list:\n",
    "        sentence_list = []\n",
    "        for block in page.block_list:\n",
    "            # use text_str\n",
    "            # remove all numbers\n",
    "            # ignore if no full stop or comma\n",
    "            # split by full stop\n",
    "            filtered_text = re.sub(r'\\d+', '', block.text_str)\n",
    "            sentence_list.append(filtered_text.strip())\n",
    "        \n",
    "        paragraph = ' '.join(sentence_list)\n",
    "        paragraph_length = len(paragraph.split())\n",
    "        if paragraph_length < 20:\n",
    "            continue\n",
    "        paragraph_list.append(paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "c4f0dc78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 605185\n",
      "100000 647603\n",
      "200000 694126\n",
      "300000 739440\n",
      "400000 782741\n",
      "500000 827236\n",
      "600000 870881\n"
     ]
    }
   ],
   "source": [
    "from math import ceil\n",
    "from copy import deepcopy\n",
    "\n",
    "MAX_SENTENCE_LENGTH = 512\n",
    "indices_to_pop = []\n",
    "for i, p in enumerate(deepcopy(paragraph_list)):\n",
    "    paragraph = p.split()\n",
    "    length = len(paragraph)\n",
    "    if i % 100000 == 0:\n",
    "        print(i, len(paragraph_list))\n",
    "    if length <= MAX_SENTENCE_LENGTH:\n",
    "        continue\n",
    "    \n",
    "    n_splits = ceil(length/MAX_SENTENCE_LENGTH)\n",
    "    split_paragraphs = [' '.join(paragraph[s*MAX_SENTENCE_LENGTH:(s*MAX_SENTENCE_LENGTH)+MAX_SENTENCE_LENGTH]) for s in range(n_splits)]\n",
    "    indices_to_pop.append(i)\n",
    "\n",
    "    paragraph_list.extend(split_paragraphs)\n",
    "\n",
    "# need to reverse first so indices dont get messed up in the original array\n",
    "for i in indices_to_pop[::-1]:\n",
    "    paragraph_list.pop(i)\n",
    "\n",
    "len('Final length: 'paragraph_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "d10ebc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('docs_NO_ID.pkl', 'wb') as handle:\n",
    "    pickle.dump(paragraph_list, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "b1ba80d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, p in enumerate(paragraph_list):\n",
    "    if p == '':\n",
    "        paragraph_list.pop(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "41c773a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' OUR COMMITMENT Materiality Annually, we assess the Corporate Social Responsibility (CSR) topics that are of importance to our stakeholders and those where we can have the greatest economic, social and environmental impacts at local, regional or global levels. We are guided in this assessment by GRI standards. We examined changing external factors, including regulations and standards, social challenges people face around the world, our evolving business model and the environmental impact of our products, services, processes and operations. Our assessment included an array of fact-ï¬nding forums, including interviews and workshops with internal stakeholders and Xerox leaders. We reï¬‚ected on discussions with external stakeholders, as well as feedback from our employees and concluded: Managing operations responsibly across our value chain â€” from decreasing environmental impact and protecting customer privacy to promoting diversity and ensuring ethical behavior â€” remains a priority. Product, service and operations-related opportunities, such as improving energy efï¬ciency and expanding access to technology, represent the leading areas where we can create value for society and for our business. Enhancing health, safety and labor conditions in our global value chain is among the most important ways we can drive sustainable development. The following schematic illustrates the results of our materiality assessment: Corporate Materiality Matrix'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b084849a",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './AutoPhrase.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[65], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# getting terms and remove the quality value\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\u001b[39m'\u001b[39;49m\u001b[39m./AutoPhrase.txt\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mr\u001b[39;49m\u001b[39m'\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mutf-8\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m lines \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mreadlines()\n\u001b[0;32m      4\u001b[0m f\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python39\\site-packages\\IPython\\core\\interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m     )\n\u001b[1;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './AutoPhrase.txt'"
     ]
    }
   ],
   "source": [
    "# getting terms and remove the quality value\n",
    "f = open('./AutoPhrase.txt', 'r', encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6305bb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "printable = set(string.printable)\n",
    "\n",
    "terms = []\n",
    "for line in lines:\n",
    "    splitted_line = line.split('\\t')\n",
    "    terms.append(splitted_line[1])\n",
    "\n",
    "f = open('./terms.txt', 'w', encoding='utf-8')\n",
    "for term in terms:\n",
    "    ascii_term = bytes(term, 'ascii', errors='ignore').decode()\n",
    "    filtered_term = ''.join((filter(lambda x: x in printable, ascii_term)))\n",
    "    f.write(filtered_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fefd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "printable = set(string.printable)\n",
    "\n",
    "f = open('./preprocessed.txt', 'r', encoding='utf-8')\n",
    "lines = f.readlines()\n",
    "f.close()\n",
    "\n",
    "f = open('./preprocessed.txt', 'w', encoding='utf-8')\n",
    "for line in lines:\n",
    "    ascii_line = bytes(line, 'ascii', errors='ignore').decode()\n",
    "    filtered_line = ''.join((filter(lambda x: x in printable, ascii_line)))\n",
    "    f.write(filtered_line)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0a6119",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "vscode": {
   "interpreter": {
    "hash": "4e907c9c662b38b6fe9e69d6881d7a36bc6e4ecfbfed88be2a66ef06ac6f6bc4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
